Loading pretrained model... done
Testing... average test_loss = 7.1997453351198, average test_pred_err = 0.499
Iteration no. 1, lr = 2, attribute lr =0.02, average batch_loss = 8.0206241799606, Training Error = 0.48828125
Iteration no. 2, lr = 2, attribute lr =0.02, average batch_loss = 3.8511027182006, Training Error = 0.4921875
Iteration no. 3, lr = 2, attribute lr =0.02, average batch_loss = 1.6104330026476, Training Error = 0.5078125
Iteration no. 4, lr = 2, attribute lr =0.02, average batch_loss = 1.4692333424742, Training Error = 0.462890625
Iteration no. 5, lr = 2, attribute lr =0.02, average batch_loss = 1.6009583855164, Training Error = 0.5
Iteration no. 6, lr = 2, attribute lr =0.02, average batch_loss = 1.065855532193, Training Error = 0.447265625
Iteration no. 7, lr = 2, attribute lr =0.02, average batch_loss = 0.9786735076698, Training Error = 0.427734375
Iteration no. 8, lr = 2, attribute lr =0.02, average batch_loss = 1.0026356397938, Training Error = 0.494140625
Iteration no. 9, lr = 2, attribute lr =0.02, average batch_loss = 0.89592277827328, Training Error = 0.4375
Iteration no. 10, lr = 2, attribute lr =0.02, average batch_loss = 0.9160001769221, Training Error = 0.466796875
Iteration no. 11, lr = 2, attribute lr =0.02, average batch_loss = 1.0147040170731, Training Error = 0.48828125
Iteration no. 12, lr = 2, attribute lr =0.02, average batch_loss = 0.8227887888194, Training Error = 0.443359375
Iteration no. 13, lr = 2, attribute lr =0.02, average batch_loss = 0.87378398938183, Training Error = 0.46484375
Iteration no. 14, lr = 2, attribute lr =0.02, average batch_loss = 0.96157597951856, Training Error = 0.486328125
Iteration no. 15, lr = 2, attribute lr =0.02, average batch_loss = 1.1630202015659, Training Error = 0.4765625
Iteration no. 16, lr = 2, attribute lr =0.02, average batch_loss = 0.92288655901485, Training Error = 0.484375
Iteration no. 17, lr = 2, attribute lr =0.02, average batch_loss = 0.99907034049626, Training Error = 0.482421875
Iteration no. 18, lr = 2, attribute lr =0.02, average batch_loss = 0.77356981628273, Training Error = 0.435546875
Iteration no. 19, lr = 2, attribute lr =0.02, average batch_loss = 0.73817336630912, Training Error = 0.453125
Iteration no. 20, lr = 2, attribute lr =0.02, average batch_loss = 0.67441481580659, Training Error = 0.3984375
Iteration no. 21, lr = 2, attribute lr =0.02, average batch_loss = 0.70696695689069, Training Error = 0.431640625
Iteration no. 22, lr = 2, attribute lr =0.02, average batch_loss = 0.72585905608371, Training Error = 0.435546875
Iteration no. 23, lr = 2, attribute lr =0.02, average batch_loss = 0.7555818913571, Training Error = 0.4453125
Iteration no. 24, lr = 2, attribute lr =0.02, average batch_loss = 0.72065348098207, Training Error = 0.416015625
Iteration no. 25, lr = 2, attribute lr =0.02, average batch_loss = 0.7757900259726, Training Error = 0.435546875
Iteration no. 26, lr = 2, attribute lr =0.02, average batch_loss = 0.72808538613322, Training Error = 0.419921875
Iteration no. 27, lr = 2, attribute lr =0.02, average batch_loss = 0.71477009108217, Training Error = 0.427734375
Iteration no. 28, lr = 2, attribute lr =0.02, average batch_loss = 0.71846374176109, Training Error = 0.427734375
Iteration no. 29, lr = 2, attribute lr =0.02, average batch_loss = 0.73973772923606, Training Error = 0.44921875
Iteration no. 30, lr = 2, attribute lr =0.02, average batch_loss = 0.67353540796657, Training Error = 0.40625
Iteration no. 31, lr = 2, attribute lr =0.02, average batch_loss = 0.7184480516062, Training Error = 0.40625
Iteration no. 32, lr = 2, attribute lr =0.02, average batch_loss = 0.69443804610363, Training Error = 0.416015625
Iteration no. 33, lr = 2, attribute lr =0.02, average batch_loss = 0.7187693559387, Training Error = 0.404296875
Iteration no. 34, lr = 2, attribute lr =0.02, average batch_loss = 0.68580517858542, Training Error = 0.41015625
Iteration no. 35, lr = 2, attribute lr =0.02, average batch_loss = 0.721935651153, Training Error = 0.419921875
Iteration no. 36, lr = 2, attribute lr =0.02, average batch_loss = 0.68667820524777, Training Error = 0.416015625
Iteration no. 37, lr = 2, attribute lr =0.02, average batch_loss = 0.66354380113344, Training Error = 0.365234375
Iteration no. 38, lr = 2, attribute lr =0.02, average batch_loss = 0.67910910379558, Training Error = 0.400390625
Iteration no. 39, lr = 2, attribute lr =0.02, average batch_loss = 0.66959688959837, Training Error = 0.431640625
Iteration no. 40, lr = 2, attribute lr =0.02, average batch_loss = 0.65896595259671, Training Error = 0.376953125
Iteration no. 41, lr = 2, attribute lr =0.02, average batch_loss = 0.65879791252257, Training Error = 0.39453125
Iteration no. 42, lr = 2, attribute lr =0.02, average batch_loss = 0.65973417855079, Training Error = 0.392578125
Iteration no. 43, lr = 2, attribute lr =0.02, average batch_loss = 0.64726963599781, Training Error = 0.37890625
Iteration no. 44, lr = 2, attribute lr =0.02, average batch_loss = 0.62043553681066, Training Error = 0.33203125
Iteration no. 45, lr = 2, attribute lr =0.02, average batch_loss = 0.63581965375026, Training Error = 0.35546875
Iteration no. 46, lr = 2, attribute lr =0.02, average batch_loss = 0.6697265636418, Training Error = 0.4375
Iteration no. 47, lr = 2, attribute lr =0.02, average batch_loss = 0.63016461306221, Training Error = 0.34375
Iteration no. 48, lr = 2, attribute lr =0.02, average batch_loss = 0.65553440187005, Training Error = 0.38671875
Iteration no. 49, lr = 2, attribute lr =0.02, average batch_loss = 0.65258976087985, Training Error = 0.373046875
Iteration no. 50, lr = 2, attribute lr =0.02, average batch_loss = 0.64491885139241, Training Error = 0.3828125
Testing... average test_loss = 0.74119971738559, average test_pred_err = 0.508
Iteration no. 51, lr = 2, attribute lr =0.02, average batch_loss = 0.62562442419549, Training Error = 0.34765625
Iteration no. 52, lr = 2, attribute lr =0.02, average batch_loss = 0.63041508808766, Training Error = 0.376953125
Iteration no. 53, lr = 2, attribute lr =0.02, average batch_loss = 0.62303577427769, Training Error = 0.3515625
Iteration no. 54, lr = 2, attribute lr =0.02, average batch_loss = 0.62892459379832, Training Error = 0.369140625
Iteration no. 55, lr = 2, attribute lr =0.02, average batch_loss = 0.64426656401812, Training Error = 0.375
Iteration no. 56, lr = 2, attribute lr =0.02, average batch_loss = 0.61002808264348, Training Error = 0.337890625
Iteration no. 57, lr = 2, attribute lr =0.02, average batch_loss = 0.63212299548537, Training Error = 0.376953125
Iteration no. 58, lr = 2, attribute lr =0.02, average batch_loss = 0.64374345320677, Training Error = 0.37109375
Iteration no. 59, lr = 2, attribute lr =0.02, average batch_loss = 0.67191588205337, Training Error = 0.431640625
Iteration no. 60, lr = 2, attribute lr =0.02, average batch_loss = 0.65574189431965, Training Error = 0.376953125
Iteration no. 61, lr = 2, attribute lr =0.02, average batch_loss = 0.64801830010789, Training Error = 0.384765625
Iteration no. 62, lr = 2, attribute lr =0.02, average batch_loss = 0.63533907375114, Training Error = 0.361328125
Iteration no. 63, lr = 2, attribute lr =0.02, average batch_loss = 0.61288833287117, Training Error = 0.345703125
Iteration no. 64, lr = 2, attribute lr =0.02, average batch_loss = 0.61401153062699, Training Error = 0.33203125
Iteration no. 65, lr = 2, attribute lr =0.02, average batch_loss = 0.61880789785482, Training Error = 0.35546875
Iteration no. 66, lr = 2, attribute lr =0.02, average batch_loss = 0.62778798742617, Training Error = 0.361328125
Iteration no. 67, lr = 2, attribute lr =0.02, average batch_loss = 0.61986943080037, Training Error = 0.353515625
Iteration no. 68, lr = 2, attribute lr =0.02, average batch_loss = 0.63149840658143, Training Error = 0.373046875
Iteration no. 69, lr = 2, attribute lr =0.02, average batch_loss = 0.62459475677625, Training Error = 0.345703125
Iteration no. 70, lr = 2, attribute lr =0.02, average batch_loss = 0.61573392230541, Training Error = 0.353515625
Iteration no. 71, lr = 2, attribute lr =0.02, average batch_loss = 0.63480140325013, Training Error = 0.37109375
Iteration no. 72, lr = 2, attribute lr =0.02, average batch_loss = 0.63244350093859, Training Error = 0.390625
Iteration no. 73, lr = 2, attribute lr =0.02, average batch_loss = 0.62554494230595, Training Error = 0.361328125
Iteration no. 74, lr = 2, attribute lr =0.02, average batch_loss = 0.6054133108328, Training Error = 0.3359375
Iteration no. 75, lr = 2, attribute lr =0.02, average batch_loss = 0.63017694740691, Training Error = 0.375
Iteration no. 76, lr = 2, attribute lr =0.02, average batch_loss = 0.61651112414397, Training Error = 0.3359375
Iteration no. 77, lr = 2, attribute lr =0.02, average batch_loss = 0.5837209786613, Training Error = 0.322265625
Iteration no. 78, lr = 2, attribute lr =0.02, average batch_loss = 0.58515620160334, Training Error = 0.30078125
Iteration no. 79, lr = 2, attribute lr =0.02, average batch_loss = 0.61461214342411, Training Error = 0.337890625
Iteration no. 80, lr = 2, attribute lr =0.02, average batch_loss = 0.60130833403023, Training Error = 0.3125
Iteration no. 81, lr = 2, attribute lr =0.02, average batch_loss = 0.60122766392939, Training Error = 0.333984375
Iteration no. 82, lr = 2, attribute lr =0.02, average batch_loss = 0.59859843056709, Training Error = 0.337890625
Iteration no. 83, lr = 2, attribute lr =0.02, average batch_loss = 0.61468445770767, Training Error = 0.337890625
Iteration no. 84, lr = 2, attribute lr =0.02, average batch_loss = 0.60779290621409, Training Error = 0.33984375
Iteration no. 85, lr = 2, attribute lr =0.02, average batch_loss = 0.63228401819906, Training Error = 0.3515625
Iteration no. 86, lr = 2, attribute lr =0.02, average batch_loss = 0.62119355603406, Training Error = 0.37109375
Iteration no. 87, lr = 2, attribute lr =0.02, average batch_loss = 0.64476919676558, Training Error = 0.34765625
Iteration no. 88, lr = 2, attribute lr =0.02, average batch_loss = 0.61862872067758, Training Error = 0.345703125
Iteration no. 89, lr = 2, attribute lr =0.02, average batch_loss = 0.6107677938426, Training Error = 0.34375
Iteration no. 90, lr = 2, attribute lr =0.02, average batch_loss = 0.61267217896844, Training Error = 0.353515625
Iteration no. 91, lr = 2, attribute lr =0.02, average batch_loss = 0.60263943385795, Training Error = 0.345703125
Iteration no. 92, lr = 2, attribute lr =0.02, average batch_loss = 0.59672452640104, Training Error = 0.3359375
Iteration no. 93, lr = 2, attribute lr =0.02, average batch_loss = 0.63031542720349, Training Error = 0.3671875
Iteration no. 94, lr = 2, attribute lr =0.02, average batch_loss = 0.62537842237297, Training Error = 0.3515625
Iteration no. 95, lr = 2, attribute lr =0.02, average batch_loss = 0.61095580490634, Training Error = 0.361328125
Iteration no. 96, lr = 2, attribute lr =0.02, average batch_loss = 0.59246670555163, Training Error = 0.34765625
Iteration no. 97, lr = 2, attribute lr =0.02, average batch_loss = 0.63747683679943, Training Error = 0.38671875
Iteration no. 98, lr = 2, attribute lr =0.02, average batch_loss = 0.60322385012481, Training Error = 0.328125
Iteration no. 99, lr = 2, attribute lr =0.02, average batch_loss = 0.60683127739096, Training Error = 0.357421875
Iteration no. 100, lr = 2, attribute lr =0.02, average batch_loss = 0.59988841656498, Training Error = 0.330078125
Testing... average test_loss = 0.77013094712216, average test_pred_err = 0.484
Snapshotting C_model... done
Iteration no. 101, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57074933394859, Training Error = 0.2890625
Iteration no. 102, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61955616356665, Training Error = 0.361328125
Iteration no. 103, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59195015766433, Training Error = 0.310546875
Iteration no. 104, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59666378121483, Training Error = 0.3125
Iteration no. 105, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61848107973415, Training Error = 0.349609375
Iteration no. 106, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62104665224702, Training Error = 0.322265625
Iteration no. 107, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60798022175814, Training Error = 0.31640625
Iteration no. 108, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62634363696311, Training Error = 0.376953125
Iteration no. 109, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60019187492155, Training Error = 0.330078125
Iteration no. 110, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57387241139001, Training Error = 0.298828125
Iteration no. 111, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61241928957873, Training Error = 0.3515625
Iteration no. 112, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59716404162807, Training Error = 0.341796875
Iteration no. 113, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57824064016753, Training Error = 0.326171875
Iteration no. 114, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57310967067954, Training Error = 0.318359375
Iteration no. 115, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58275146012616, Training Error = 0.314453125
Iteration no. 116, lr = 1.4, attribute lr =0.014, average batch_loss = 0.65403935866191, Training Error = 0.380859375
Iteration no. 117, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58034371799361, Training Error = 0.314453125
Iteration no. 118, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59119312645308, Training Error = 0.3125
Iteration no. 119, lr = 1.4, attribute lr =0.014, average batch_loss = 0.6069877738452, Training Error = 0.33984375
Iteration no. 120, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57372432978602, Training Error = 0.29296875
Iteration no. 121, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58464341130525, Training Error = 0.330078125
Iteration no. 122, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57310111204006, Training Error = 0.322265625
Iteration no. 123, lr = 1.4, attribute lr =0.014, average batch_loss = 0.63313507643302, Training Error = 0.365234375
Iteration no. 124, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59739711735019, Training Error = 0.32421875
Iteration no. 125, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61102457605942, Training Error = 0.34765625
Iteration no. 126, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58622867526225, Training Error = 0.31640625
Iteration no. 127, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61329412598797, Training Error = 0.337890625
Iteration no. 128, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61729223434666, Training Error = 0.3515625
Iteration no. 129, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5776673776279, Training Error = 0.30078125
Iteration no. 130, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58297841314257, Training Error = 0.302734375
Iteration no. 131, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56106883288157, Training Error = 0.287109375
Iteration no. 132, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59216171301553, Training Error = 0.32421875
Iteration no. 133, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57666782159278, Training Error = 0.322265625
Iteration no. 134, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62110262905006, Training Error = 0.369140625
Iteration no. 135, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5545000274001, Training Error = 0.302734375
Iteration no. 136, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57246058188481, Training Error = 0.310546875
Iteration no. 137, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60300609183195, Training Error = 0.333984375
Iteration no. 138, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58014111728214, Training Error = 0.314453125
Iteration no. 139, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58728956676161, Training Error = 0.3359375
Iteration no. 140, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58402885323686, Training Error = 0.302734375
Iteration no. 141, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56830015285005, Training Error = 0.279296875
Iteration no. 142, lr = 1.4, attribute lr =0.014, average batch_loss = 0.63182313565882, Training Error = 0.361328125
Iteration no. 143, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58349548457972, Training Error = 0.30859375
Iteration no. 144, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57944472378809, Training Error = 0.310546875
Iteration no. 145, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57561681066076, Training Error = 0.298828125
Iteration no. 146, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57971826366351, Training Error = 0.31640625
Iteration no. 147, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58115649662312, Training Error = 0.3046875
Iteration no. 148, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58729017109155, Training Error = 0.298828125
Iteration no. 149, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57391470459615, Training Error = 0.296875
Iteration no. 150, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5866470103417, Training Error = 0.326171875
Testing... average test_loss = 0.81044480558306, average test_pred_err = 0.489
Iteration no. 151, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57285123767787, Training Error = 0.298828125
Iteration no. 152, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59218538550869, Training Error = 0.318359375
Iteration no. 153, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61256341129829, Training Error = 0.337890625
Iteration no. 154, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59541998513453, Training Error = 0.3359375
Iteration no. 155, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56982938732192, Training Error = 0.296875
Iteration no. 156, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58248027004226, Training Error = 0.326171875
Iteration no. 157, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5903286748781, Training Error = 0.3046875
Iteration no. 158, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56489620108822, Training Error = 0.287109375
Iteration no. 159, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59496213072655, Training Error = 0.3359375
Iteration no. 160, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57642243049544, Training Error = 0.31640625
Iteration no. 161, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57661599454824, Training Error = 0.287109375
Iteration no. 162, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56897015118983, Training Error = 0.310546875
Iteration no. 163, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57630543827937, Training Error = 0.3046875
Iteration no. 164, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57219016052726, Training Error = 0.294921875
Iteration no. 165, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56306314811127, Training Error = 0.294921875
Iteration no. 166, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5783532816319, Training Error = 0.296875
Iteration no. 167, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62737415229135, Training Error = 0.353515625
Iteration no. 168, lr = 1.4, attribute lr =0.014, average batch_loss = 0.53944299188691, Training Error = 0.2734375
Iteration no. 169, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55252865176313, Training Error = 0.294921875
Iteration no. 170, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5832547435648, Training Error = 0.29296875
Iteration no. 171, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60220973511423, Training Error = 0.33984375
Iteration no. 172, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56908317945306, Training Error = 0.306640625
Iteration no. 173, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60929347111655, Training Error = 0.34765625
Iteration no. 174, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58215495226595, Training Error = 0.318359375
Iteration no. 175, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5688324279438, Training Error = 0.3125
Iteration no. 176, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57555286640101, Training Error = 0.33203125
Iteration no. 177, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57677414099103, Training Error = 0.3046875
Iteration no. 178, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56935411326801, Training Error = 0.302734375
Iteration no. 179, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56484367452888, Training Error = 0.302734375
Iteration no. 180, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55059342035625, Training Error = 0.283203125
Iteration no. 181, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56845554461337, Training Error = 0.30859375
Iteration no. 182, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56246449283865, Training Error = 0.29296875
Iteration no. 183, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5913500185344, Training Error = 0.3203125
Iteration no. 184, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59355867021374, Training Error = 0.32421875
Iteration no. 185, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57547555378127, Training Error = 0.31640625
Iteration no. 186, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58613880635264, Training Error = 0.326171875
Iteration no. 187, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59485279929338, Training Error = 0.31640625
Iteration no. 188, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58002086153811, Training Error = 0.3046875
Iteration no. 189, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56093988491051, Training Error = 0.298828125
Iteration no. 190, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56158271683483, Training Error = 0.283203125
Iteration no. 191, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57705752428924, Training Error = 0.314453125
Iteration no. 192, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55864060258203, Training Error = 0.296875
Iteration no. 193, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57491195804244, Training Error = 0.3046875
Iteration no. 194, lr = 1.4, attribute lr =0.014, average batch_loss = 0.54431849229382, Training Error = 0.3046875
Iteration no. 195, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57747226194607, Training Error = 0.3125
Iteration no. 196, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55695459389174, Training Error = 0.298828125
Iteration no. 197, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57364596006206, Training Error = 0.306640625
Iteration no. 198, lr = 1.4, attribute lr =0.014, average batch_loss = 0.52458798015888, Training Error = 0.26953125
Iteration no. 199, lr = 1.4, attribute lr =0.014, average batch_loss = 0.53186822960127, Training Error = 0.265625
Iteration no. 200, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5260540811887, Training Error = 0.248046875
Testing... average test_loss = 0.85415347615712, average test_pred_err = 0.495
Snapshotting C_model... done
Iteration no. 201, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56932081033989, Training Error = 0.30859375
Iteration no. 202, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57303936881421, Training Error = 0.294921875
Iteration no. 203, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56217902677817, Training Error = 0.306640625
Iteration no. 204, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55707248346569, Training Error = 0.2890625
Iteration no. 205, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55539948064907, Training Error = 0.275390625
Iteration no. 206, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54467287563903, Training Error = 0.298828125
Iteration no. 207, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56669448070656, Training Error = 0.306640625
Iteration no. 208, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52719954625105, Training Error = 0.236328125
Iteration no. 209, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56823734033245, Training Error = 0.3046875
Iteration no. 210, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58101827629255, Training Error = 0.328125
Iteration no. 211, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55781667238769, Training Error = 0.294921875
Iteration no. 212, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56180612472112, Training Error = 0.322265625
Iteration no. 213, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54209439652706, Training Error = 0.2578125
Iteration no. 214, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5613433368436, Training Error = 0.29296875
Iteration no. 215, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57481668935173, Training Error = 0.30859375
Iteration no. 216, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57283162139644, Training Error = 0.30859375
Iteration no. 217, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55693738804659, Training Error = 0.29296875
Iteration no. 218, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57202056998869, Training Error = 0.3046875
Iteration no. 219, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.547262089679, Training Error = 0.291015625
Iteration no. 220, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55146949468072, Training Error = 0.279296875
Iteration no. 221, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54166613076256, Training Error = 0.275390625
Iteration no. 222, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56800368055962, Training Error = 0.294921875
Iteration no. 223, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58863917827562, Training Error = 0.314453125
Iteration no. 224, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56613771369117, Training Error = 0.28515625
Iteration no. 225, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.60438180833083, Training Error = 0.34375
Iteration no. 226, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57316891477372, Training Error = 0.306640625
Iteration no. 227, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52781902309305, Training Error = 0.271484375
Iteration no. 228, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57462627826237, Training Error = 0.3125
Iteration no. 229, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56230777010359, Training Error = 0.302734375
Iteration no. 230, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53109684386165, Training Error = 0.2578125
Iteration no. 231, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56645044962806, Training Error = 0.32421875
Iteration no. 232, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5417621669849, Training Error = 0.298828125
Iteration no. 233, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.61025666608346, Training Error = 0.328125
Iteration no. 234, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56935912678491, Training Error = 0.30859375
Iteration no. 235, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52669118407451, Training Error = 0.248046875
Iteration no. 236, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.60058088632382, Training Error = 0.32421875
Iteration no. 237, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58942550243675, Training Error = 0.31640625
Iteration no. 238, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56141868883587, Training Error = 0.283203125
Iteration no. 239, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53707719503353, Training Error = 0.27734375
Iteration no. 240, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57832127401432, Training Error = 0.298828125
Iteration no. 241, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58079163514655, Training Error = 0.3125
Iteration no. 242, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57035475566842, Training Error = 0.291015625
Iteration no. 243, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54699632412878, Training Error = 0.27734375
Iteration no. 244, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56381602143396, Training Error = 0.296875
Iteration no. 245, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53397979540266, Training Error = 0.263671875
Iteration no. 246, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54329552945718, Training Error = 0.28125
Iteration no. 247, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53970079477378, Training Error = 0.28125
Iteration no. 248, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56928970727657, Training Error = 0.296875
Iteration no. 249, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54821572386034, Training Error = 0.267578125
Iteration no. 250, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54880273931855, Training Error = 0.2734375
Testing... average test_loss = 0.80687537531845, average test_pred_err = 0.463
Iteration no. 251, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58397392007532, Training Error = 0.32421875
Iteration no. 252, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53066903280451, Training Error = 0.255859375
Iteration no. 253, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57958412168263, Training Error = 0.322265625
Iteration no. 254, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55268675479371, Training Error = 0.294921875
Iteration no. 255, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53310405960782, Training Error = 0.265625
Iteration no. 256, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54262960678438, Training Error = 0.259765625
Iteration no. 257, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5623821267014, Training Error = 0.29296875
Iteration no. 258, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54569024762758, Training Error = 0.28515625
Iteration no. 259, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55491848785754, Training Error = 0.296875
Iteration no. 260, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55452978131902, Training Error = 0.287109375
Iteration no. 261, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58367850476308, Training Error = 0.345703125
Iteration no. 262, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52364968540644, Training Error = 0.2734375
Iteration no. 263, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54498736926417, Training Error = 0.294921875
Iteration no. 264, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.50733545217739, Training Error = 0.251953125
Iteration no. 265, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53067553019089, Training Error = 0.251953125
Iteration no. 266, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55198983057795, Training Error = 0.30078125
Iteration no. 267, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54129361123671, Training Error = 0.29296875
Iteration no. 268, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54878739982651, Training Error = 0.28125
Iteration no. 269, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54944575646706, Training Error = 0.291015625
Iteration no. 270, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55255113492913, Training Error = 0.2734375
Iteration no. 271, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54760680719356, Training Error = 0.26171875
Iteration no. 272, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58420420586122, Training Error = 0.328125
Iteration no. 273, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53487627408959, Training Error = 0.2734375
Iteration no. 274, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55211824118582, Training Error = 0.279296875
Iteration no. 275, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53293403462947, Training Error = 0.26953125
Iteration no. 276, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53915601010426, Training Error = 0.265625
Iteration no. 277, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54368116201006, Training Error = 0.283203125
Iteration no. 278, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57060936074743, Training Error = 0.306640625
Iteration no. 279, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56771856662817, Training Error = 0.310546875
Iteration no. 280, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52244294735046, Training Error = 0.267578125
Iteration no. 281, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53544170711756, Training Error = 0.259765625
Iteration no. 282, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54658616276127, Training Error = 0.287109375
Iteration no. 283, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55052596449022, Training Error = 0.294921875
Iteration no. 284, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53589747334929, Training Error = 0.26953125
Iteration no. 285, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.50531831951227, Training Error = 0.2734375
Iteration no. 286, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54567839740232, Training Error = 0.287109375
Iteration no. 287, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53352958414711, Training Error = 0.26953125
Iteration no. 288, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53181389248111, Training Error = 0.287109375
Iteration no. 289, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5190352077473, Training Error = 0.251953125
Iteration no. 290, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55071978829334, Training Error = 0.298828125
Iteration no. 291, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5419054781088, Training Error = 0.279296875
Iteration no. 292, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.51538366342317, Training Error = 0.25
Iteration no. 293, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54257132117627, Training Error = 0.279296875
Iteration no. 294, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52183663706501, Training Error = 0.251953125
Iteration no. 295, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55588521559224, Training Error = 0.2890625
Iteration no. 296, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.50202202080995, Training Error = 0.2265625
Iteration no. 297, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52796708552012, Training Error = 0.271484375
Iteration no. 298, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54255677667421, Training Error = 0.28125
Iteration no. 299, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55323952188303, Training Error = 0.283203125
Iteration no. 300, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55759677597102, Training Error = 0.27734375
Testing... average test_loss = 0.855881005181, average test_pred_err = 0.459
Snapshotting C_model... done
Iteration no. 301, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54165902094859, Training Error = 0.26953125
Iteration no. 302, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55009405699071, Training Error = 0.2734375
Iteration no. 303, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54650698675142, Training Error = 0.283203125
Iteration no. 304, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5122275832385, Training Error = 0.259765625
Iteration no. 305, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54896064609749, Training Error = 0.29296875
Iteration no. 306, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55068796433653, Training Error = 0.306640625
Iteration no. 307, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54801106763926, Training Error = 0.28125
Iteration no. 308, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55578642711017, Training Error = 0.30078125
Iteration no. 309, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52929236853361, Training Error = 0.265625
Iteration no. 310, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55633905040556, Training Error = 0.294921875
Iteration no. 311, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52869797426186, Training Error = 0.267578125
Iteration no. 312, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54943407858376, Training Error = 0.271484375
Iteration no. 313, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51356519392937, Training Error = 0.2734375
Iteration no. 314, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50684649887241, Training Error = 0.26171875
Iteration no. 315, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54987364129445, Training Error = 0.28515625
Iteration no. 316, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54047342675872, Training Error = 0.28515625
Iteration no. 317, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5365485646814, Training Error = 0.28515625
Iteration no. 318, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5225133019838, Training Error = 0.248046875
Iteration no. 319, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52967119916495, Training Error = 0.28125
Iteration no. 320, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52261185673152, Training Error = 0.279296875
Iteration no. 321, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53083785206328, Training Error = 0.279296875
Iteration no. 322, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50438874972103, Training Error = 0.25
Iteration no. 323, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56011749204512, Training Error = 0.3046875
Iteration no. 324, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52931599640037, Training Error = 0.259765625
Iteration no. 325, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53751473652317, Training Error = 0.283203125
Iteration no. 326, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5355009697721, Training Error = 0.26953125
Iteration no. 327, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53119731282702, Training Error = 0.291015625
Iteration no. 328, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51559558014419, Training Error = 0.24609375
Iteration no. 329, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51807561220863, Training Error = 0.263671875
Iteration no. 330, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52657564735239, Training Error = 0.265625
Iteration no. 331, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53014014078781, Training Error = 0.28515625
Iteration no. 332, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55707962001916, Training Error = 0.30859375
Iteration no. 333, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56664896703123, Training Error = 0.296875
Iteration no. 334, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55034974109511, Training Error = 0.29296875
Iteration no. 335, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55320129321588, Training Error = 0.27734375
Iteration no. 336, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.549564240108, Training Error = 0.28515625
Iteration no. 337, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51277908553035, Training Error = 0.25390625
Iteration no. 338, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56355046432688, Training Error = 0.28515625
Iteration no. 339, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53004623788317, Training Error = 0.279296875
Iteration no. 340, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52825473238538, Training Error = 0.275390625
Iteration no. 341, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55270391353294, Training Error = 0.30078125
Iteration no. 342, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.549557958934, Training Error = 0.291015625
Iteration no. 343, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54819338310008, Training Error = 0.271484375
Iteration no. 344, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53883935484493, Training Error = 0.2890625
Iteration no. 345, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50749779783669, Training Error = 0.2578125
Iteration no. 346, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5379882871178, Training Error = 0.283203125
Iteration no. 347, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53228202394794, Training Error = 0.255859375
Iteration no. 348, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.49926901076137, Training Error = 0.263671875
Iteration no. 349, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52219742825474, Training Error = 0.291015625
Iteration no. 350, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53260811577841, Training Error = 0.28125
Testing... average test_loss = 0.89962086803857, average test_pred_err = 0.49
Iteration no. 351, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50069328366803, Training Error = 0.25
Iteration no. 352, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5427751721235, Training Error = 0.291015625
Iteration no. 353, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5465433109201, Training Error = 0.310546875
Iteration no. 354, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56033325457326, Training Error = 0.306640625
Iteration no. 355, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53988302597136, Training Error = 0.28515625
Iteration no. 356, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51893640221904, Training Error = 0.248046875
Iteration no. 357, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52920811780092, Training Error = 0.271484375
Iteration no. 358, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53851431550573, Training Error = 0.30078125
Iteration no. 359, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.57075842061714, Training Error = 0.3125
Iteration no. 360, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5309629472735, Training Error = 0.287109375
Iteration no. 361, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.49139436152141, Training Error = 0.240234375
Iteration no. 362, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.508132954056, Training Error = 0.267578125
Iteration no. 363, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53535356574102, Training Error = 0.291015625
Iteration no. 364, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50224078880874, Training Error = 0.25
Iteration no. 365, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51749648344919, Training Error = 0.283203125
Iteration no. 366, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55407883576739, Training Error = 0.29296875
Iteration no. 367, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53580362068019, Training Error = 0.263671875
Iteration no. 368, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56785544105525, Training Error = 0.275390625
Iteration no. 369, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50999343808531, Training Error = 0.259765625
Iteration no. 370, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55508993749338, Training Error = 0.28515625
Iteration no. 371, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52788594710141, Training Error = 0.2734375
Iteration no. 372, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5262430471066, Training Error = 0.267578125
Iteration no. 373, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54635678223491, Training Error = 0.28125
Iteration no. 374, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53284854654966, Training Error = 0.267578125
Iteration no. 375, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51197122023858, Training Error = 0.2578125
Iteration no. 376, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50778943969483, Training Error = 0.248046875
Iteration no. 377, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53501711594746, Training Error = 0.267578125
Iteration no. 378, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52041536006999, Training Error = 0.263671875
Iteration no. 379, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54716155333296, Training Error = 0.296875
Iteration no. 380, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50846185406403, Training Error = 0.255859375
Iteration no. 381, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5430230421896, Training Error = 0.291015625
Iteration no. 382, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50470624664872, Training Error = 0.234375
Iteration no. 383, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.522534277814, Training Error = 0.2734375
Iteration no. 384, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.49235193428539, Training Error = 0.232421875
Iteration no. 385, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52608562914534, Training Error = 0.263671875
Iteration no. 386, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5042850626773, Training Error = 0.26171875
Iteration no. 387, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51284337956289, Training Error = 0.251953125
Iteration no. 388, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55811130952104, Training Error = 0.287109375
Iteration no. 389, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5219636355379, Training Error = 0.259765625
Iteration no. 390, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52530607250076, Training Error = 0.25390625
Iteration no. 391, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54915954719932, Training Error = 0.28515625
Iteration no. 392, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54159854567497, Training Error = 0.271484375
Iteration no. 393, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50671006919193, Training Error = 0.267578125
Iteration no. 394, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54100447076801, Training Error = 0.287109375
Iteration no. 395, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.48766898580547, Training Error = 0.240234375
Iteration no. 396, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53329253473708, Training Error = 0.28125
Iteration no. 397, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55867397760959, Training Error = 0.314453125
Iteration no. 398, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51904026094619, Training Error = 0.271484375
Iteration no. 399, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54873895437983, Training Error = 0.29296875
Iteration no. 400, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54273894477239, Training Error = 0.294921875
Testing... average test_loss = 0.87145995865687, average test_pred_err = 0.482
Snapshotting C_model... done
Iteration no. 401, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48978500272765, Training Error = 0.251953125
Iteration no. 402, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53894177703173, Training Error = 0.2890625
Iteration no. 403, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52520534016966, Training Error = 0.279296875
Iteration no. 404, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55394711766905, Training Error = 0.291015625
Iteration no. 405, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56547014705262, Training Error = 0.27734375
Iteration no. 406, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56329370705005, Training Error = 0.291015625
Iteration no. 407, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48622566391593, Training Error = 0.234375
Iteration no. 408, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53821895227316, Training Error = 0.287109375
Iteration no. 409, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52693941489463, Training Error = 0.271484375
Iteration no. 410, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52521721223032, Training Error = 0.27734375
Iteration no. 411, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50756614343899, Training Error = 0.255859375
Iteration no. 412, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53963319828461, Training Error = 0.296875
Iteration no. 413, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51865016547647, Training Error = 0.251953125
Iteration no. 414, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54753215261592, Training Error = 0.28125
Iteration no. 415, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51933302215871, Training Error = 0.259765625
Iteration no. 416, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54893296421227, Training Error = 0.279296875
Iteration no. 417, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56485068906453, Training Error = 0.3046875
Iteration no. 418, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.4999215140045, Training Error = 0.2421875
Iteration no. 419, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50900537464604, Training Error = 0.265625
Iteration no. 420, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5409115614893, Training Error = 0.2890625
Iteration no. 421, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52374741066525, Training Error = 0.283203125
Iteration no. 422, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51812259967925, Training Error = 0.26171875
Iteration no. 423, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55603460689268, Training Error = 0.291015625
Iteration no. 424, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52011575637362, Training Error = 0.263671875
Iteration no. 425, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51540036383332, Training Error = 0.259765625
Iteration no. 426, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49242775512986, Training Error = 0.240234375
Iteration no. 427, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49566381403222, Training Error = 0.255859375
Iteration no. 428, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56125661257773, Training Error = 0.3046875
Iteration no. 429, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54101282951213, Training Error = 0.294921875
Iteration no. 430, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53771114066853, Training Error = 0.275390625
Iteration no. 431, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54426439133666, Training Error = 0.2734375
Iteration no. 432, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53319488423834, Training Error = 0.279296875
Iteration no. 433, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50415986606435, Training Error = 0.259765625
Iteration no. 434, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51598200071342, Training Error = 0.25
Iteration no. 435, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51099437781885, Training Error = 0.255859375
Iteration no. 436, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48756416266349, Training Error = 0.234375
Iteration no. 437, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55980025602919, Training Error = 0.3046875
Iteration no. 438, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52109654337761, Training Error = 0.259765625
Iteration no. 439, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52033913371857, Training Error = 0.2734375
Iteration no. 440, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50123900163946, Training Error = 0.2265625
Iteration no. 441, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55223052980861, Training Error = 0.294921875
Iteration no. 442, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56350975855733, Training Error = 0.310546875
Iteration no. 443, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53585923864101, Training Error = 0.279296875
Iteration no. 444, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54509114123619, Training Error = 0.28125
Iteration no. 445, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.57204701358102, Training Error = 0.294921875
Iteration no. 446, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54747528601833, Training Error = 0.294921875
Iteration no. 447, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5203481717381, Training Error = 0.26953125
Iteration no. 448, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50667734440655, Training Error = 0.251953125
Iteration no. 449, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54159580228344, Training Error = 0.287109375
Iteration no. 450, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53474579555507, Training Error = 0.28125
Testing... average test_loss = 0.89829965704142, average test_pred_err = 0.483
Iteration no. 451, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50008367001619, Training Error = 0.24609375
Iteration no. 452, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53623845316013, Training Error = 0.255859375
Iteration no. 453, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.4993047212097, Training Error = 0.22265625
Iteration no. 454, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52705368780385, Training Error = 0.259765625
Iteration no. 455, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51482812389949, Training Error = 0.265625
Iteration no. 456, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.4988379950472, Training Error = 0.248046875
Iteration no. 457, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51590159041837, Training Error = 0.27734375
Iteration no. 458, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51152733537269, Training Error = 0.234375
Iteration no. 459, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5265144014814, Training Error = 0.27734375
Iteration no. 460, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50509016879488, Training Error = 0.244140625
Iteration no. 461, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49906510712008, Training Error = 0.2421875
Iteration no. 462, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52956688267316, Training Error = 0.259765625
Iteration no. 463, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.47771311322175, Training Error = 0.251953125
Iteration no. 464, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49942022889837, Training Error = 0.228515625
Iteration no. 465, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54608408381385, Training Error = 0.29296875
Iteration no. 466, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53753980397775, Training Error = 0.283203125
Iteration no. 467, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49197278502263, Training Error = 0.240234375
Iteration no. 468, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53183000575479, Training Error = 0.294921875
Iteration no. 469, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51616948741932, Training Error = 0.2734375
Iteration no. 470, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52858104166079, Training Error = 0.2578125
Iteration no. 471, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52434199345452, Training Error = 0.259765625
Iteration no. 472, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51811388836333, Training Error = 0.275390625
Iteration no. 473, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53592463250015, Training Error = 0.26171875
Iteration no. 474, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52597496249439, Training Error = 0.267578125
Iteration no. 475, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50971493666395, Training Error = 0.26171875
Iteration no. 476, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54407813405217, Training Error = 0.2734375
Iteration no. 477, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49668239780168, Training Error = 0.263671875
Iteration no. 478, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53185697997816, Training Error = 0.28125
Iteration no. 479, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53490770186695, Training Error = 0.2734375
Iteration no. 480, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52874569109959, Training Error = 0.267578125
Iteration no. 481, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52779565216417, Training Error = 0.283203125
Iteration no. 482, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53335949908104, Training Error = 0.263671875
Iteration no. 483, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53657945714573, Training Error = 0.29296875
Iteration no. 484, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5275711412226, Training Error = 0.26171875
Iteration no. 485, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50844687936696, Training Error = 0.255859375
Iteration no. 486, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49289345437871, Training Error = 0.248046875
Iteration no. 487, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48958962987789, Training Error = 0.21875
Iteration no. 488, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55658711789007, Training Error = 0.275390625
Iteration no. 489, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55930588803528, Training Error = 0.2890625
Iteration no. 490, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50518781745676, Training Error = 0.267578125
Iteration no. 491, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51927024822839, Training Error = 0.25390625
Iteration no. 492, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50832773257196, Training Error = 0.26171875
Iteration no. 493, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51493901029333, Training Error = 0.267578125
Iteration no. 494, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48517443863674, Training Error = 0.236328125
Iteration no. 495, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51513123235796, Training Error = 0.271484375
Iteration no. 496, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52728556848401, Training Error = 0.25
Iteration no. 497, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53555293147449, Training Error = 0.26171875
Iteration no. 498, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52087186997242, Training Error = 0.271484375
Iteration no. 499, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49522268526424, Training Error = 0.23828125
Iteration no. 500, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51772673085505, Training Error = 0.2578125
Testing... average test_loss = 0.85602895731801, average test_pred_err = 0.474
Snapshotting C_model... done
Iteration no. 501, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50630763724052, Training Error = 0.24609375
Iteration no. 502, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53810452314095, Training Error = 0.263671875
Iteration no. 503, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48535589011036, Training Error = 0.232421875
Iteration no. 504, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51571413509611, Training Error = 0.265625
Iteration no. 505, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49796391999182, Training Error = 0.2578125
Iteration no. 506, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50812985748885, Training Error = 0.265625
Iteration no. 507, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51809027173409, Training Error = 0.263671875
Iteration no. 508, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50875228013837, Training Error = 0.25
Iteration no. 509, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52026828515383, Training Error = 0.28515625
Iteration no. 510, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50562471680668, Training Error = 0.259765625
Iteration no. 511, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54673983484016, Training Error = 0.28515625
Iteration no. 512, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54959229504619, Training Error = 0.2734375
Iteration no. 513, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55227490339225, Training Error = 0.275390625
Iteration no. 514, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52363996027191, Training Error = 0.26953125
Iteration no. 515, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.4826552966347, Training Error = 0.220703125
Iteration no. 516, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48015362932065, Training Error = 0.23046875
Iteration no. 517, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51459499118939, Training Error = 0.2734375
Iteration no. 518, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50025559420547, Training Error = 0.248046875
Iteration no. 519, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50896574496855, Training Error = 0.24609375
Iteration no. 520, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5201137874227, Training Error = 0.27734375
Iteration no. 521, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51907013149436, Training Error = 0.26171875
Iteration no. 522, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52192240357169, Training Error = 0.236328125
Iteration no. 523, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.47340654862765, Training Error = 0.2265625
Iteration no. 524, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50835568952078, Training Error = 0.279296875
Iteration no. 525, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49716502435879, Training Error = 0.228515625
Iteration no. 526, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54192292186221, Training Error = 0.2578125
Iteration no. 527, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55964607936938, Training Error = 0.291015625
Iteration no. 528, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48197430524074, Training Error = 0.224609375
Iteration no. 529, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54898033516166, Training Error = 0.30859375
Iteration no. 530, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48577569303186, Training Error = 0.25
Iteration no. 531, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51325001204542, Training Error = 0.251953125
Iteration no. 532, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49653579683787, Training Error = 0.240234375
Iteration no. 533, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51196085437027, Training Error = 0.26171875
Iteration no. 534, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5090139877064, Training Error = 0.25390625
Iteration no. 535, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51334377530757, Training Error = 0.2578125
Iteration no. 536, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48537457330672, Training Error = 0.234375
Iteration no. 537, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53085858503637, Training Error = 0.279296875
Iteration no. 538, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53101804830601, Training Error = 0.267578125
Iteration no. 539, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50814850021929, Training Error = 0.2578125
Iteration no. 540, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53720205607692, Training Error = 0.2734375
Iteration no. 541, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53863519590501, Training Error = 0.2734375
Iteration no. 542, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55721560526992, Training Error = 0.3046875
Iteration no. 543, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49519668967079, Training Error = 0.251953125
Iteration no. 544, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.47975885768505, Training Error = 0.228515625
Iteration no. 545, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50176664826572, Training Error = 0.251953125
Iteration no. 546, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53606275049859, Training Error = 0.265625
Iteration no. 547, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51701902225413, Training Error = 0.263671875
Iteration no. 548, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49409962890212, Training Error = 0.23046875
Iteration no. 549, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52561448260321, Training Error = 0.263671875
Iteration no. 550, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50498398843575, Training Error = 0.255859375
Testing... average test_loss = 0.91188062336522, average test_pred_err = 0.494
Iteration no. 551, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50516530041429, Training Error = 0.24609375
Iteration no. 552, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53377337279258, Training Error = 0.24609375
Iteration no. 553, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49539996220692, Training Error = 0.26171875
Iteration no. 554, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48524011908194, Training Error = 0.2421875
Iteration no. 555, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52636871839615, Training Error = 0.251953125
Iteration no. 556, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52759239687861, Training Error = 0.255859375
Iteration no. 557, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52694440886446, Training Error = 0.2578125
Iteration no. 558, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5126692469544, Training Error = 0.265625
Iteration no. 559, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.538024303186, Training Error = 0.28125
Iteration no. 560, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52298819734955, Training Error = 0.291015625
Iteration no. 561, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51467582913471, Training Error = 0.279296875
Iteration no. 562, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51474957250982, Training Error = 0.2578125
Iteration no. 563, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50768972020581, Training Error = 0.275390625
Iteration no. 564, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49756402936089, Training Error = 0.2578125
Iteration no. 565, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51003893602949, Training Error = 0.265625
Iteration no. 566, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49366821226148, Training Error = 0.23828125
Iteration no. 567, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49429473240241, Training Error = 0.24609375
Iteration no. 568, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51706232853366, Training Error = 0.25
Iteration no. 569, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52274135115337, Training Error = 0.259765625
Iteration no. 570, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52507463866244, Training Error = 0.279296875
Iteration no. 571, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49383529528051, Training Error = 0.24609375
Iteration no. 572, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5307577827464, Training Error = 0.275390625
Iteration no. 573, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49860915182397, Training Error = 0.23828125
Iteration no. 574, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50852454600488, Training Error = 0.265625
Iteration no. 575, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53378161782802, Training Error = 0.283203125
Iteration no. 576, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54858861058044, Training Error = 0.291015625
Iteration no. 577, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49637020633372, Training Error = 0.2265625
Iteration no. 578, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49204860816339, Training Error = 0.22265625
Iteration no. 579, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49567445134129, Training Error = 0.265625
Iteration no. 580, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53990562516354, Training Error = 0.279296875
Iteration no. 581, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51037291884526, Training Error = 0.267578125
Iteration no. 582, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51452405401466, Training Error = 0.26171875
Iteration no. 583, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51603849713526, Training Error = 0.27734375
Iteration no. 584, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50225139047586, Training Error = 0.267578125
Iteration no. 585, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5012310936371, Training Error = 0.248046875
Iteration no. 586, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49278332102008, Training Error = 0.240234375
Iteration no. 587, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49656077785435, Training Error = 0.244140625
Iteration no. 588, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54392885233889, Training Error = 0.287109375
Iteration no. 589, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55777064936624, Training Error = 0.287109375
Iteration no. 590, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51423253454941, Training Error = 0.25
Iteration no. 591, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52497190220638, Training Error = 0.271484375
Iteration no. 592, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49233053739595, Training Error = 0.234375
Iteration no. 593, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53590493904142, Training Error = 0.28515625
Iteration no. 594, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50614897256364, Training Error = 0.23046875
Iteration no. 595, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49457057090364, Training Error = 0.24609375
Iteration no. 596, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51803994295124, Training Error = 0.26171875
Iteration no. 597, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53842220414857, Training Error = 0.283203125
Iteration no. 598, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51899614196282, Training Error = 0.259765625
Iteration no. 599, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52175475659663, Training Error = 0.275390625
Iteration no. 600, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49806753222244, Training Error = 0.25
Testing... average test_loss = 0.8602294983306, average test_pred_err = 0.445
Snapshotting C_model... done
Iteration no. 601, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54369701520627, Training Error = 0.255859375
Iteration no. 602, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49048733263089, Training Error = 0.234375
Iteration no. 603, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49204038277669, Training Error = 0.263671875
Iteration no. 604, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48912779479461, Training Error = 0.236328125
Iteration no. 605, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.56167484631014, Training Error = 0.306640625
Iteration no. 606, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49461877139241, Training Error = 0.2421875
Iteration no. 607, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50267580699353, Training Error = 0.2734375
Iteration no. 608, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52298044391792, Training Error = 0.275390625
Iteration no. 609, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51587146906651, Training Error = 0.27734375
Iteration no. 610, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52390107493865, Training Error = 0.271484375
Iteration no. 611, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54463197437512, Training Error = 0.30078125
Iteration no. 612, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51421283038823, Training Error = 0.248046875
Iteration no. 613, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47217378199824, Training Error = 0.224609375
Iteration no. 614, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49173455944996, Training Error = 0.25390625
Iteration no. 615, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53303715638147, Training Error = 0.263671875
Iteration no. 616, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50863466811677, Training Error = 0.248046875
Iteration no. 617, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53429534768709, Training Error = 0.287109375
Iteration no. 618, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51808891921966, Training Error = 0.26171875
Iteration no. 619, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.4918486908994, Training Error = 0.2578125
Iteration no. 620, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49295045567991, Training Error = 0.26171875
Iteration no. 621, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53022229890852, Training Error = 0.279296875
Iteration no. 622, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5051735590642, Training Error = 0.275390625
Iteration no. 623, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48883629401229, Training Error = 0.224609375
Iteration no. 624, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51618696900227, Training Error = 0.2421875
Iteration no. 625, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51669252742725, Training Error = 0.271484375
Iteration no. 626, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49902927410229, Training Error = 0.236328125
Iteration no. 627, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52038752886932, Training Error = 0.248046875
Iteration no. 628, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50210994610712, Training Error = 0.2421875
Iteration no. 629, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49301364829196, Training Error = 0.263671875
Iteration no. 630, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50494805996923, Training Error = 0.2421875
Iteration no. 631, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5200959196507, Training Error = 0.2734375
Iteration no. 632, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52250645517827, Training Error = 0.2734375
Iteration no. 633, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54536966806834, Training Error = 0.279296875
Iteration no. 634, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52150213007666, Training Error = 0.28125
Iteration no. 635, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49154361122389, Training Error = 0.2421875
Iteration no. 636, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50395809201316, Training Error = 0.240234375
Iteration no. 637, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52220797119367, Training Error = 0.263671875
Iteration no. 638, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52655296542084, Training Error = 0.265625
Iteration no. 639, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52386228429056, Training Error = 0.271484375
Iteration no. 640, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5181742000286, Training Error = 0.26171875
Iteration no. 641, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50019135356597, Training Error = 0.25390625
Iteration no. 642, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.4607194162887, Training Error = 0.21875
Iteration no. 643, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50462079958904, Training Error = 0.244140625
Iteration no. 644, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48651195249887, Training Error = 0.24609375
Iteration no. 645, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53316823651804, Training Error = 0.271484375
Iteration no. 646, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48720227095187, Training Error = 0.248046875
Iteration no. 647, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.46171829001422, Training Error = 0.232421875
Iteration no. 648, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49678922228908, Training Error = 0.2421875
Iteration no. 649, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49151610927661, Training Error = 0.234375
Iteration no. 650, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47755463727164, Training Error = 0.224609375
Testing... average test_loss = 0.87665404353958, average test_pred_err = 0.451
Iteration no. 651, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50008196008761, Training Error = 0.259765625
Iteration no. 652, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50798059678891, Training Error = 0.259765625
Iteration no. 653, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48911095636436, Training Error = 0.25390625
Iteration no. 654, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52261338207637, Training Error = 0.25
Iteration no. 655, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49969431380492, Training Error = 0.24609375
Iteration no. 656, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51379686064275, Training Error = 0.263671875
Iteration no. 657, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52948271253965, Training Error = 0.2734375
Iteration no. 658, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50109814664221, Training Error = 0.255859375
Iteration no. 659, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5102099943349, Training Error = 0.26953125
Iteration no. 660, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52219091737375, Training Error = 0.28125
Iteration no. 661, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50891645224873, Training Error = 0.240234375
Iteration no. 662, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48766316274863, Training Error = 0.23828125
Iteration no. 663, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51968708987573, Training Error = 0.2734375
Iteration no. 664, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.4962623961868, Training Error = 0.234375
Iteration no. 665, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51848104178162, Training Error = 0.275390625
Iteration no. 666, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50244999861426, Training Error = 0.248046875
Iteration no. 667, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53953946660906, Training Error = 0.279296875
Iteration no. 668, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49595507595151, Training Error = 0.25390625
Iteration no. 669, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49664992653611, Training Error = 0.248046875
Iteration no. 670, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48884925959388, Training Error = 0.2421875
Iteration no. 671, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49562678225061, Training Error = 0.23828125
Iteration no. 672, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48946112393162, Training Error = 0.234375
Iteration no. 673, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47925874206438, Training Error = 0.220703125
Iteration no. 674, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50542511908231, Training Error = 0.271484375
Iteration no. 675, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51871629292976, Training Error = 0.244140625
Iteration no. 676, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54410717339037, Training Error = 0.28515625
Iteration no. 677, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51534196281664, Training Error = 0.248046875
Iteration no. 678, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49592227597551, Training Error = 0.2578125
Iteration no. 679, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49984518101052, Training Error = 0.265625
Iteration no. 680, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48531098516964, Training Error = 0.259765625
Iteration no. 681, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52161594149873, Training Error = 0.271484375
Iteration no. 682, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49382315911419, Training Error = 0.228515625
Iteration no. 683, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53804162971315, Training Error = 0.27734375
Iteration no. 684, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54467751359696, Training Error = 0.283203125
Iteration no. 685, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53146914603968, Training Error = 0.279296875
Iteration no. 686, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49746648103995, Training Error = 0.234375
Iteration no. 687, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50643103851145, Training Error = 0.232421875
Iteration no. 688, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52087368869524, Training Error = 0.26953125
Iteration no. 689, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49962182401108, Training Error = 0.23046875
Iteration no. 690, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5038799499389, Training Error = 0.255859375
Iteration no. 691, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49890983950168, Training Error = 0.25
Iteration no. 692, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52190715093748, Training Error = 0.244140625
Iteration no. 693, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49473274785316, Training Error = 0.251953125
Iteration no. 694, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53872676027152, Training Error = 0.291015625
Iteration no. 695, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50423696947559, Training Error = 0.25390625
Iteration no. 696, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51101198193454, Training Error = 0.24609375
Iteration no. 697, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47400100421957, Training Error = 0.228515625
Iteration no. 698, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50516923394828, Training Error = 0.259765625
Iteration no. 699, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52068695782794, Training Error = 0.26171875
Iteration no. 700, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49643546799862, Training Error = 0.25
Testing... average test_loss = 0.89229713558973, average test_pred_err = 0.448
Snapshotting C_model... done
Iteration no. 701, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50846255584293, Training Error = 0.244140625
Iteration no. 702, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50294187672966, Training Error = 0.267578125
Iteration no. 703, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51338872176419, Training Error = 0.265625
Iteration no. 704, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47080355370651, Training Error = 0.22265625
Iteration no. 705, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.500966002809, Training Error = 0.2578125
Iteration no. 706, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52557787040636, Training Error = 0.26171875
Iteration no. 707, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53026494431335, Training Error = 0.279296875
Iteration no. 708, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.54486731047903, Training Error = 0.2734375
Iteration no. 709, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50008462280234, Training Error = 0.23828125
Iteration no. 710, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50160920225009, Training Error = 0.26171875
Iteration no. 711, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49755972859106, Training Error = 0.263671875
Iteration no. 712, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53910276819801, Training Error = 0.26953125
Iteration no. 713, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48998193845002, Training Error = 0.251953125
Iteration no. 714, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51295148492793, Training Error = 0.26171875
Iteration no. 715, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49436332033717, Training Error = 0.23046875
Iteration no. 716, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51743608134532, Training Error = 0.26953125
Iteration no. 717, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4856973154676, Training Error = 0.244140625
Iteration no. 718, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52101601297372, Training Error = 0.25390625
Iteration no. 719, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50388434319502, Training Error = 0.2421875
Iteration no. 720, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51493051971481, Training Error = 0.24609375
Iteration no. 721, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50304377521324, Training Error = 0.267578125
Iteration no. 722, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48326369578974, Training Error = 0.23828125
Iteration no. 723, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48667443836084, Training Error = 0.24609375
Iteration no. 724, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.46613521866913, Training Error = 0.2265625
Iteration no. 725, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52922253565689, Training Error = 0.279296875
Iteration no. 726, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.46769458175355, Training Error = 0.21875
Iteration no. 727, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4852258361635, Training Error = 0.23828125
Iteration no. 728, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48900908228836, Training Error = 0.240234375
Iteration no. 729, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50353440782385, Training Error = 0.255859375
Iteration no. 730, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51201369779776, Training Error = 0.244140625
Iteration no. 731, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50407668361553, Training Error = 0.2578125
Iteration no. 732, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50104411360701, Training Error = 0.259765625
Iteration no. 733, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51653124727202, Training Error = 0.251953125
Iteration no. 734, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.55699239352636, Training Error = 0.296875
Iteration no. 735, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48868584883156, Training Error = 0.23046875
Iteration no. 736, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49382012522083, Training Error = 0.236328125
Iteration no. 737, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52676046938826, Training Error = 0.263671875
Iteration no. 738, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51110938531225, Training Error = 0.263671875
Iteration no. 739, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48542285736295, Training Error = 0.255859375
Iteration no. 740, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49802195192857, Training Error = 0.2421875
Iteration no. 741, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48384506933145, Training Error = 0.236328125
Iteration no. 742, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49894543585586, Training Error = 0.236328125
Iteration no. 743, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50464256336683, Training Error = 0.25390625
Iteration no. 744, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.45206561668873, Training Error = 0.208984375
Iteration no. 745, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49613262315897, Training Error = 0.25390625
Iteration no. 746, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.54675160153512, Training Error = 0.28125
Iteration no. 747, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.45945239176787, Training Error = 0.20703125
Iteration no. 748, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48362877416866, Training Error = 0.259765625
Iteration no. 749, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47668719224092, Training Error = 0.251953125
Iteration no. 750, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51600266155721, Training Error = 0.248046875
Testing... average test_loss = 0.92018127409741, average test_pred_err = 0.456
Iteration no. 751, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51131839200627, Training Error = 0.259765625
Iteration no. 752, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51764440231792, Training Error = 0.27734375
Iteration no. 753, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53113898489798, Training Error = 0.283203125
Iteration no. 754, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.5081940576024, Training Error = 0.2578125
Iteration no. 755, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52480633430746, Training Error = 0.271484375
Iteration no. 756, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53359143142905, Training Error = 0.310546875
Iteration no. 757, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52296358666656, Training Error = 0.267578125
Iteration no. 758, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51413958034727, Training Error = 0.283203125
Iteration no. 759, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53489663490033, Training Error = 0.2734375
Iteration no. 760, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53434430724442, Training Error = 0.275390625
Iteration no. 761, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.5192138493858, Training Error = 0.2578125
Iteration no. 762, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.55471300571686, Training Error = 0.27734375
Iteration no. 763, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52175834111962, Training Error = 0.25390625
Iteration no. 764, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52515026334181, Training Error = 0.275390625
Iteration no. 765, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51806271633904, Training Error = 0.271484375
Iteration no. 766, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49840739424573, Training Error = 0.25390625
Iteration no. 767, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.5268752004506, Training Error = 0.2578125
Iteration no. 768, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51941041231681, Training Error = 0.26953125
Iteration no. 769, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50021921768922, Training Error = 0.25390625
Iteration no. 770, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4905992775102, Training Error = 0.248046875
Iteration no. 771, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51864846086796, Training Error = 0.26953125
Iteration no. 772, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50883922413892, Training Error = 0.25390625
Iteration no. 773, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49093181936965, Training Error = 0.25390625
Iteration no. 774, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50736141897333, Training Error = 0.2578125
Iteration no. 775, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50315142202934, Training Error = 0.25
Iteration no. 776, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51185272035589, Training Error = 0.287109375
Iteration no. 777, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47787964848209, Training Error = 0.234375
Iteration no. 778, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51046367427903, Training Error = 0.27734375
Iteration no. 779, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.46334163692868, Training Error = 0.234375
Iteration no. 780, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51900346251395, Training Error = 0.279296875
Iteration no. 781, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49260176134147, Training Error = 0.2421875
Iteration no. 782, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51875180628876, Training Error = 0.24609375
Iteration no. 783, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51814665711445, Training Error = 0.279296875
Iteration no. 784, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51805805041463, Training Error = 0.25390625
Iteration no. 785, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50618518182181, Training Error = 0.251953125
Iteration no. 786, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49491796156397, Training Error = 0.2421875
Iteration no. 787, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50117897240064, Training Error = 0.251953125
Iteration no. 788, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52901826662962, Training Error = 0.267578125
Iteration no. 789, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53380559239016, Training Error = 0.2734375
Iteration no. 790, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53089921826256, Training Error = 0.279296875
Iteration no. 791, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47409948403792, Training Error = 0.240234375
Iteration no. 792, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49086847243742, Training Error = 0.251953125
Iteration no. 793, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50426884715161, Training Error = 0.24609375
Iteration no. 794, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47498912118568, Training Error = 0.232421875
Iteration no. 795, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48592645439359, Training Error = 0.240234375
Iteration no. 796, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52501919724599, Training Error = 0.283203125
Iteration no. 797, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49254338359768, Training Error = 0.244140625
Iteration no. 798, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51790058494296, Training Error = 0.248046875
Iteration no. 799, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4921577610491, Training Error = 0.23828125
Iteration no. 800, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48293845936184, Training Error = 0.240234375
Testing... average test_loss = 0.91701670100134, average test_pred_err = 0.462
Snapshotting C_model... done
Iteration no. 801, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48493077752156, Training Error = 0.248046875
Iteration no. 802, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51304112115064, Training Error = 0.251953125
Iteration no. 803, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51583499411243, Training Error = 0.2734375
Iteration no. 804, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53113953617504, Training Error = 0.28125
Iteration no. 805, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48977474263768, Training Error = 0.240234375
Iteration no. 806, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52038963884103, Training Error = 0.2734375
Iteration no. 807, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53362398776675, Training Error = 0.2578125
Iteration no. 808, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.54265381037704, Training Error = 0.28515625
Iteration no. 809, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52980892071795, Training Error = 0.267578125
Iteration no. 810, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50446750321673, Training Error = 0.2578125
Iteration no. 811, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51628358792373, Training Error = 0.255859375
Iteration no. 812, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51389240769483, Training Error = 0.25390625
Iteration no. 813, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49447829489476, Training Error = 0.25
Iteration no. 814, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52952976598157, Training Error = 0.275390625
Iteration no. 815, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4874950823217, Training Error = 0.20703125
Iteration no. 816, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4695293602337, Training Error = 0.220703125
Iteration no. 817, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49072848607561, Training Error = 0.232421875
Iteration no. 818, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52992796213481, Training Error = 0.296875
Iteration no. 819, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5451750681446, Training Error = 0.275390625
Iteration no. 820, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50546704138115, Training Error = 0.24609375
Iteration no. 821, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52921563506978, Training Error = 0.279296875
Iteration no. 822, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51364028982586, Training Error = 0.25
Iteration no. 823, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52308968014251, Training Error = 0.283203125
Iteration no. 824, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51484888628012, Training Error = 0.265625
Iteration no. 825, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49197783547129, Training Error = 0.224609375
Iteration no. 826, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52258872807301, Training Error = 0.267578125
Iteration no. 827, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4762256256513, Training Error = 0.205078125
Iteration no. 828, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46623861078322, Training Error = 0.2265625
Iteration no. 829, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51004859951157, Training Error = 0.275390625
Iteration no. 830, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51736481934497, Training Error = 0.26171875
Iteration no. 831, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46768594112694, Training Error = 0.220703125
Iteration no. 832, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5091413373055, Training Error = 0.2578125
Iteration no. 833, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48237818254912, Training Error = 0.23828125
Iteration no. 834, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52640402906143, Training Error = 0.25
Iteration no. 835, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50265630939911, Training Error = 0.265625
Iteration no. 836, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49579352944567, Training Error = 0.25
Iteration no. 837, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49607922764955, Training Error = 0.251953125
Iteration no. 838, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5106518260663, Training Error = 0.251953125
Iteration no. 839, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51798493908794, Training Error = 0.26953125
Iteration no. 840, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4918382361651, Training Error = 0.234375
Iteration no. 841, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.55035359933998, Training Error = 0.2734375
Iteration no. 842, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53173282634139, Training Error = 0.2578125
Iteration no. 843, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.54371694775121, Training Error = 0.3046875
Iteration no. 844, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52899603202947, Training Error = 0.27734375
Iteration no. 845, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50808201631632, Training Error = 0.25
Iteration no. 846, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52043563960659, Training Error = 0.2734375
Iteration no. 847, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4905882628885, Training Error = 0.224609375
Iteration no. 848, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5203912052344, Training Error = 0.265625
Iteration no. 849, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47564132693, Training Error = 0.25
Iteration no. 850, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52475993816357, Training Error = 0.26953125
Testing... average test_loss = 0.9440602351324, average test_pred_err = 0.483
Iteration no. 851, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49766739317845, Training Error = 0.2421875
Iteration no. 852, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50128010592782, Training Error = 0.265625
Iteration no. 853, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46523476619941, Training Error = 0.21875
Iteration no. 854, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49884996285513, Training Error = 0.25
Iteration no. 855, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51965737864319, Training Error = 0.26171875
Iteration no. 856, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53461227816336, Training Error = 0.2890625
Iteration no. 857, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53288086459182, Training Error = 0.2734375
Iteration no. 858, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51694331625428, Training Error = 0.2578125
Iteration no. 859, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47910575207402, Training Error = 0.224609375
Iteration no. 860, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4816596050343, Training Error = 0.228515625
Iteration no. 861, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48618178229985, Training Error = 0.228515625
Iteration no. 862, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52191026027188, Training Error = 0.2578125
Iteration no. 863, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53007273397861, Training Error = 0.265625
Iteration no. 864, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51653254538383, Training Error = 0.248046875
Iteration no. 865, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47460713892953, Training Error = 0.234375
Iteration no. 866, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48292216181327, Training Error = 0.24609375
Iteration no. 867, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49915790888645, Training Error = 0.244140625
Iteration no. 868, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47903505373591, Training Error = 0.22265625
Iteration no. 869, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50901851836856, Training Error = 0.27734375
Iteration no. 870, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52138479805189, Training Error = 0.2578125
Iteration no. 871, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47283451753732, Training Error = 0.244140625
Iteration no. 872, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47975907732464, Training Error = 0.234375
Iteration no. 873, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5238077767868, Training Error = 0.2734375
Iteration no. 874, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50459121575834, Training Error = 0.244140625
Iteration no. 875, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49741944363054, Training Error = 0.240234375
Iteration no. 876, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5101291249786, Training Error = 0.234375
Iteration no. 877, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53143720017342, Training Error = 0.255859375
Iteration no. 878, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51047993954124, Training Error = 0.2734375
Iteration no. 879, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49381483437501, Training Error = 0.255859375
Iteration no. 880, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5028888247579, Training Error = 0.240234375
Iteration no. 881, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4730315932681, Training Error = 0.248046875
Iteration no. 882, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51195010546896, Training Error = 0.2578125
Iteration no. 883, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49021995293677, Training Error = 0.244140625
Iteration no. 884, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50145617501848, Training Error = 0.2578125
Iteration no. 885, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49977935096001, Training Error = 0.26171875
Iteration no. 886, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48767371213449, Training Error = 0.2421875
Iteration no. 887, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46808030867021, Training Error = 0.23046875
Iteration no. 888, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47151367014403, Training Error = 0.21875
Iteration no. 889, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50772043257004, Training Error = 0.255859375
Iteration no. 890, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5126097194092, Training Error = 0.255859375
Iteration no. 891, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47253176392913, Training Error = 0.2109375
Iteration no. 892, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51795669386853, Training Error = 0.2578125
Iteration no. 893, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52956245112024, Training Error = 0.27734375
Iteration no. 894, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51300811713152, Training Error = 0.25
Iteration no. 895, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50552023366791, Training Error = 0.25390625
Iteration no. 896, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47212029904265, Training Error = 0.21875
Iteration no. 897, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51571913321537, Training Error = 0.248046875
Iteration no. 898, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50900804605875, Training Error = 0.2734375
Iteration no. 899, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48261156073771, Training Error = 0.23828125
Iteration no. 900, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50863263235663, Training Error = 0.255859375
Testing... average test_loss = 0.92267680089698, average test_pred_err = 0.438
Snapshotting C_model... done
Iteration no. 901, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54116411204936, Training Error = 0.291015625
Iteration no. 902, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51049780336617, Training Error = 0.263671875
Iteration no. 903, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47446761513274, Training Error = 0.240234375
Iteration no. 904, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4806065467095, Training Error = 0.2265625
Iteration no. 905, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50109708252123, Training Error = 0.26953125
Iteration no. 906, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53248901797007, Training Error = 0.283203125
Iteration no. 907, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49152363197199, Training Error = 0.22265625
Iteration no. 908, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48964755594989, Training Error = 0.255859375
Iteration no. 909, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54530792984422, Training Error = 0.28515625
Iteration no. 910, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47727766457989, Training Error = 0.23046875
Iteration no. 911, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50439134050857, Training Error = 0.2578125
Iteration no. 912, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50003614772561, Training Error = 0.259765625
Iteration no. 913, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51727023623405, Training Error = 0.259765625
Iteration no. 914, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51837138581919, Training Error = 0.2734375
Iteration no. 915, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53930374117871, Training Error = 0.265625
Iteration no. 916, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.46876645144741, Training Error = 0.23046875
Iteration no. 917, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49855552840132, Training Error = 0.267578125
Iteration no. 918, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48505123378969, Training Error = 0.251953125
Iteration no. 919, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5011874174971, Training Error = 0.26171875
Iteration no. 920, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51428515454052, Training Error = 0.26953125
Iteration no. 921, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49958124866684, Training Error = 0.251953125
Iteration no. 922, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53260221519327, Training Error = 0.283203125
Iteration no. 923, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49218734876624, Training Error = 0.248046875
Iteration no. 924, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49322161026614, Training Error = 0.24609375
Iteration no. 925, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5177924427434, Training Error = 0.259765625
Iteration no. 926, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54953812401194, Training Error = 0.287109375
Iteration no. 927, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50771894982061, Training Error = 0.259765625
Iteration no. 928, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47790434916371, Training Error = 0.240234375
Iteration no. 929, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54732125096969, Training Error = 0.28125
Iteration no. 930, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4992789427532, Training Error = 0.26171875
Iteration no. 931, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49174762954971, Training Error = 0.2578125
Iteration no. 932, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51739212940946, Training Error = 0.26171875
Iteration no. 933, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51665116831017, Training Error = 0.259765625
Iteration no. 934, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50622253907823, Training Error = 0.25
Iteration no. 935, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5263432485706, Training Error = 0.26171875
Iteration no. 936, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51835765924698, Training Error = 0.26171875
Iteration no. 937, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.45855219812562, Training Error = 0.208984375
Iteration no. 938, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5185016265249, Training Error = 0.240234375
Iteration no. 939, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51566590252731, Training Error = 0.265625
Iteration no. 940, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49982423619554, Training Error = 0.2578125
Iteration no. 941, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.46471703922308, Training Error = 0.2109375
Iteration no. 942, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51306018149782, Training Error = 0.26171875
Iteration no. 943, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50909081517999, Training Error = 0.255859375
Iteration no. 944, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52690641593004, Training Error = 0.265625
Iteration no. 945, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53405163526727, Training Error = 0.28125
Iteration no. 946, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49079823755984, Training Error = 0.265625
Iteration no. 947, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50715540941229, Training Error = 0.25
Iteration no. 948, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4936444167927, Training Error = 0.2265625
Iteration no. 949, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48492191692137, Training Error = 0.25
Iteration no. 950, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47445184910323, Training Error = 0.251953125
Testing... average test_loss = 0.96785263959461, average test_pred_err = 0.495
Iteration no. 951, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51695175432798, Training Error = 0.244140625
Iteration no. 952, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52362441716383, Training Error = 0.27734375
Iteration no. 953, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53505655346885, Training Error = 0.283203125
Iteration no. 954, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50622486135108, Training Error = 0.25
Iteration no. 955, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51330357620326, Training Error = 0.2265625
Iteration no. 956, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52187351776885, Training Error = 0.265625
Iteration no. 957, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50828380913092, Training Error = 0.2734375
Iteration no. 958, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51810665924826, Training Error = 0.275390625
Iteration no. 959, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47520958049208, Training Error = 0.2265625
Iteration no. 960, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48913108977353, Training Error = 0.2578125
Iteration no. 961, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49458917232163, Training Error = 0.234375
Iteration no. 962, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47968449964639, Training Error = 0.236328125
Iteration no. 963, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52299211548691, Training Error = 0.279296875
Iteration no. 964, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52452294798458, Training Error = 0.25
Iteration no. 965, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50751645022689, Training Error = 0.25
Iteration no. 966, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51618097119512, Training Error = 0.255859375
Iteration no. 967, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50984310015289, Training Error = 0.271484375
Iteration no. 968, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48954007688285, Training Error = 0.240234375
Iteration no. 969, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5149169706309, Training Error = 0.23828125
Iteration no. 970, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47780637851722, Training Error = 0.234375
Iteration no. 971, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50875208032631, Training Error = 0.275390625
Iteration no. 972, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49940663261979, Training Error = 0.23828125
Iteration no. 973, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51916528090776, Training Error = 0.2578125
Iteration no. 974, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5056033818659, Training Error = 0.265625
Iteration no. 975, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49750301224827, Training Error = 0.25390625
Iteration no. 976, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48391631722885, Training Error = 0.236328125
Iteration no. 977, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50233040409739, Training Error = 0.24609375
Iteration no. 978, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51214512543951, Training Error = 0.27734375
Iteration no. 979, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50481948091825, Training Error = 0.255859375
Iteration no. 980, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53998200173649, Training Error = 0.27734375
Iteration no. 981, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49738150018118, Training Error = 0.2578125
Iteration no. 982, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4930979157053, Training Error = 0.2578125
Iteration no. 983, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5307333992034, Training Error = 0.275390625
Iteration no. 984, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.46607483076226, Training Error = 0.236328125
Iteration no. 985, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49593308062223, Training Error = 0.263671875
Iteration no. 986, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49286448006854, Training Error = 0.2265625
Iteration no. 987, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48652163478306, Training Error = 0.232421875
Iteration no. 988, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49117315403254, Training Error = 0.2421875
Iteration no. 989, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47491708457767, Training Error = 0.2265625
Iteration no. 990, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54513620865136, Training Error = 0.291015625
Iteration no. 991, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47266954139684, Training Error = 0.23828125
Iteration no. 992, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51628848662501, Training Error = 0.263671875
Iteration no. 993, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49204480943138, Training Error = 0.248046875
Iteration no. 994, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50085613163836, Training Error = 0.236328125
Iteration no. 995, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51792155815298, Training Error = 0.26953125
Iteration no. 996, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50417078842241, Training Error = 0.23828125
Iteration no. 997, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48753354216798, Training Error = 0.23828125
Iteration no. 998, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51546576438154, Training Error = 0.27734375
Iteration no. 999, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48463514027865, Training Error = 0.244140625
Iteration no. 1000, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.45610181848215, Training Error = 0.2109375
Testing... average test_loss = 0.92005245455858, average test_pred_err = 0.45
Snapshotting C_model... done
Iteration no. 1001, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52276137271715, Training Error = 0.28125
Iteration no. 1002, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50166829521614, Training Error = 0.28125
Iteration no. 1003, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.53423042933796, Training Error = 0.2578125
Iteration no. 1004, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48765396875483, Training Error = 0.2265625
Iteration no. 1005, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52237307346627, Training Error = 0.265625
Iteration no. 1006, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49397605499816, Training Error = 0.24609375
Iteration no. 1007, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51640473354979, Training Error = 0.26953125
Iteration no. 1008, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47774721729428, Training Error = 0.228515625
Iteration no. 1009, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49977941814046, Training Error = 0.25
Iteration no. 1010, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46981899648039, Training Error = 0.236328125
Iteration no. 1011, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51195740039882, Training Error = 0.255859375
Iteration no. 1012, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51188580831371, Training Error = 0.259765625
Iteration no. 1013, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46058659460352, Training Error = 0.22265625
Iteration no. 1014, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.55298314035917, Training Error = 0.27734375
Iteration no. 1015, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46362107458413, Training Error = 0.224609375
Iteration no. 1016, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50828673254133, Training Error = 0.267578125
Iteration no. 1017, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51400418439648, Training Error = 0.248046875
Iteration no. 1018, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49628879216829, Training Error = 0.263671875
Iteration no. 1019, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52337393250563, Training Error = 0.279296875
Iteration no. 1020, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52415940509052, Training Error = 0.26171875
Iteration no. 1021, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51206651554103, Training Error = 0.2578125
Iteration no. 1022, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47592746020764, Training Error = 0.236328125
Iteration no. 1023, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47600655991602, Training Error = 0.224609375
Iteration no. 1024, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.54174763230915, Training Error = 0.275390625
Iteration no. 1025, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52588858573926, Training Error = 0.26171875
Iteration no. 1026, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51869548893431, Training Error = 0.259765625
Iteration no. 1027, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51853377598746, Training Error = 0.244140625
Iteration no. 1028, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49996111094315, Training Error = 0.24609375
Iteration no. 1029, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4674121454726, Training Error = 0.228515625
Iteration no. 1030, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51118829322443, Training Error = 0.244140625
Iteration no. 1031, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47464871697007, Training Error = 0.240234375
Iteration no. 1032, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51914363558823, Training Error = 0.2734375
Iteration no. 1033, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47462789354834, Training Error = 0.216796875
Iteration no. 1034, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51863563278498, Training Error = 0.263671875
Iteration no. 1035, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5009994427807, Training Error = 0.248046875
Iteration no. 1036, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49158828873099, Training Error = 0.244140625
Iteration no. 1037, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51196664607699, Training Error = 0.248046875
Iteration no. 1038, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50396578630635, Training Error = 0.2578125
Iteration no. 1039, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48239824162373, Training Error = 0.255859375
Iteration no. 1040, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.53427009572095, Training Error = 0.294921875
Iteration no. 1041, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51425410081255, Training Error = 0.2734375
Iteration no. 1042, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52631771779654, Training Error = 0.291015625
Iteration no. 1043, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49938166063971, Training Error = 0.24609375
Iteration no. 1044, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5134549013102, Training Error = 0.248046875
Iteration no. 1045, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50988845170316, Training Error = 0.2578125
Iteration no. 1046, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52314010022455, Training Error = 0.265625
Iteration no. 1047, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50152959212323, Training Error = 0.248046875
Iteration no. 1048, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51719589539339, Training Error = 0.2578125
Iteration no. 1049, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50102918251301, Training Error = 0.232421875
Iteration no. 1050, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.54685698532454, Training Error = 0.3046875
Testing... average test_loss = 0.947501360425, average test_pred_err = 0.468
Iteration no. 1051, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51731588011861, Training Error = 0.283203125
Iteration no. 1052, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50818701356259, Training Error = 0.263671875
Iteration no. 1053, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49259265155335, Training Error = 0.24609375
Iteration no. 1054, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48812384127494, Training Error = 0.23828125
Iteration no. 1055, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5166797552808, Training Error = 0.26171875
Iteration no. 1056, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49723820998302, Training Error = 0.240234375
Iteration no. 1057, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48984935429127, Training Error = 0.244140625
Iteration no. 1058, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47996392466614, Training Error = 0.224609375
Iteration no. 1059, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50209860688468, Training Error = 0.263671875
Iteration no. 1060, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49911236645007, Training Error = 0.24609375
Iteration no. 1061, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49922765510444, Training Error = 0.236328125
Iteration no. 1062, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4826328572252, Training Error = 0.248046875
Iteration no. 1063, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50316230429426, Training Error = 0.23828125
Iteration no. 1064, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48118692219632, Training Error = 0.23046875
Iteration no. 1065, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49403187168687, Training Error = 0.248046875
Iteration no. 1066, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5030543672141, Training Error = 0.25390625
Iteration no. 1067, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50784729501506, Training Error = 0.25390625
Iteration no. 1068, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50516449980517, Training Error = 0.236328125
Iteration no. 1069, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50297956823792, Training Error = 0.267578125
Iteration no. 1070, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51310512884669, Training Error = 0.2734375
Iteration no. 1071, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50705311507459, Training Error = 0.263671875
Iteration no. 1072, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48004616091415, Training Error = 0.2265625
Iteration no. 1073, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51221184659796, Training Error = 0.25390625
Iteration no. 1074, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4734301397447, Training Error = 0.216796875
Iteration no. 1075, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51166548245868, Training Error = 0.259765625
Iteration no. 1076, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48362151036351, Training Error = 0.255859375
Iteration no. 1077, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51433286596234, Training Error = 0.26953125
Iteration no. 1078, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52939820010242, Training Error = 0.2578125
Iteration no. 1079, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46240253793963, Training Error = 0.212890625
Iteration no. 1080, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48577880635671, Training Error = 0.232421875
Iteration no. 1081, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49667391618815, Training Error = 0.26171875
Iteration no. 1082, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51992445207983, Training Error = 0.263671875
Iteration no. 1083, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49001477522628, Training Error = 0.232421875
Iteration no. 1084, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48264805930371, Training Error = 0.23828125
Iteration no. 1085, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51615039129057, Training Error = 0.271484375
Iteration no. 1086, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51454444216023, Training Error = 0.27734375
Iteration no. 1087, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48158347186578, Training Error = 0.2265625
Iteration no. 1088, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48090522711188, Training Error = 0.23046875
Iteration no. 1089, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.54213432601799, Training Error = 0.298828125
Iteration no. 1090, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51800207733852, Training Error = 0.255859375
Iteration no. 1091, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51971757118808, Training Error = 0.240234375
Iteration no. 1092, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5390021290867, Training Error = 0.306640625
Iteration no. 1093, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49399202000341, Training Error = 0.234375
Iteration no. 1094, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5014847234888, Training Error = 0.234375
Iteration no. 1095, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.53229204424822, Training Error = 0.2578125
Iteration no. 1096, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48339685974883, Training Error = 0.236328125
Iteration no. 1097, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47928756883293, Training Error = 0.244140625
Iteration no. 1098, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46819491372915, Training Error = 0.21875
Iteration no. 1099, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5419756899365, Training Error = 0.2734375
Iteration no. 1100, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4856839104002, Training Error = 0.23046875
Testing... average test_loss = 0.98660208325224, average test_pred_err = 0.49
Snapshotting C_model... done
Iteration no. 1101, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48605113109782, Training Error = 0.255859375
Iteration no. 1102, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48535189932549, Training Error = 0.220703125
Iteration no. 1103, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54573923502011, Training Error = 0.267578125
Iteration no. 1104, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48863178410745, Training Error = 0.255859375
Iteration no. 1105, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48659148585155, Training Error = 0.2265625
Iteration no. 1106, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5026891693641, Training Error = 0.248046875
Iteration no. 1107, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51576223139306, Training Error = 0.279296875
Iteration no. 1108, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49589861365072, Training Error = 0.25
Iteration no. 1109, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53711634548405, Training Error = 0.267578125
Iteration no. 1110, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50841092124892, Training Error = 0.255859375
Iteration no. 1111, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47649998449548, Training Error = 0.234375
Iteration no. 1112, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49481697378549, Training Error = 0.236328125
Iteration no. 1113, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47987329804635, Training Error = 0.236328125
Iteration no. 1114, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54288455733767, Training Error = 0.283203125
Iteration no. 1115, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49902594586646, Training Error = 0.2265625
Iteration no. 1116, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47233519108637, Training Error = 0.251953125
Iteration no. 1117, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47332149295666, Training Error = 0.24609375
Iteration no. 1118, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50211776264346, Training Error = 0.259765625
Iteration no. 1119, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50091040981971, Training Error = 0.255859375
Iteration no. 1120, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48624130616454, Training Error = 0.244140625
Iteration no. 1121, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.55138912344086, Training Error = 0.275390625
Iteration no. 1122, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47078095349946, Training Error = 0.21484375
Iteration no. 1123, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48258231294412, Training Error = 0.248046875
Iteration no. 1124, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48874026553339, Training Error = 0.26171875
Iteration no. 1125, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51057756252313, Training Error = 0.23828125
Iteration no. 1126, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48501279459169, Training Error = 0.23828125
Iteration no. 1127, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51044271347902, Training Error = 0.263671875
Iteration no. 1128, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.52514429700279, Training Error = 0.2578125
Iteration no. 1129, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50013570965696, Training Error = 0.240234375
Iteration no. 1130, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48698617906559, Training Error = 0.25
Iteration no. 1131, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49703070151667, Training Error = 0.2578125
Iteration no. 1132, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50951000488877, Training Error = 0.263671875
Iteration no. 1133, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48788884651248, Training Error = 0.244140625
Iteration no. 1134, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50814868846757, Training Error = 0.2578125
Iteration no. 1135, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49852472491009, Training Error = 0.2421875
Iteration no. 1136, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54235311923255, Training Error = 0.30078125
Iteration no. 1137, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48691380493113, Training Error = 0.25390625
Iteration no. 1138, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49653795847936, Training Error = 0.220703125
Iteration no. 1139, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54801257891104, Training Error = 0.279296875
Iteration no. 1140, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50353655957801, Training Error = 0.263671875
Iteration no. 1141, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50201532766799, Training Error = 0.244140625
Iteration no. 1142, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51514963862588, Training Error = 0.2578125
Iteration no. 1143, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51369469101958, Training Error = 0.265625
Iteration no. 1144, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51229277770525, Training Error = 0.259765625
Iteration no. 1145, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.52231752388038, Training Error = 0.259765625
Iteration no. 1146, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5377118565022, Training Error = 0.28515625
Iteration no. 1147, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48740050246461, Training Error = 0.2421875
Iteration no. 1148, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49972973299165, Training Error = 0.23828125
Iteration no. 1149, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51575256696784, Training Error = 0.265625
Iteration no. 1150, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51340169069388, Training Error = 0.267578125
Testing... average test_loss = 0.92590819707315, average test_pred_err = 0.465
Iteration no. 1151, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49759171498059, Training Error = 0.232421875
Iteration no. 1152, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47758506776223, Training Error = 0.224609375
Iteration no. 1153, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49205037176077, Training Error = 0.2421875
Iteration no. 1154, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47699997377164, Training Error = 0.224609375
Iteration no. 1155, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50508541029691, Training Error = 0.244140625
Iteration no. 1156, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50215213707085, Training Error = 0.248046875
Iteration no. 1157, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51562369647274, Training Error = 0.26171875
Iteration no. 1158, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49153129725613, Training Error = 0.271484375
Iteration no. 1159, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53642162256531, Training Error = 0.279296875
Iteration no. 1160, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5123886284221, Training Error = 0.24609375
Iteration no. 1161, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53137305258711, Training Error = 0.2578125
Iteration no. 1162, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49081811894184, Training Error = 0.240234375
Iteration no. 1163, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50505695840824, Training Error = 0.251953125
Iteration no. 1164, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47614015493154, Training Error = 0.212890625
Iteration no. 1165, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48170160944491, Training Error = 0.23046875
Iteration no. 1166, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51314491367825, Training Error = 0.248046875
Iteration no. 1167, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50274616045885, Training Error = 0.24609375
Iteration no. 1168, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48840319030781, Training Error = 0.23046875
Iteration no. 1169, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53764554935021, Training Error = 0.267578125
Iteration no. 1170, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51606052875406, Training Error = 0.27734375
Iteration no. 1171, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47533910606381, Training Error = 0.228515625
Iteration no. 1172, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49388534473734, Training Error = 0.2578125
Iteration no. 1173, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51215366956989, Training Error = 0.271484375
Iteration no. 1174, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50423200996218, Training Error = 0.25390625
Iteration no. 1175, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47868182711173, Training Error = 0.25
Iteration no. 1176, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.46772874494761, Training Error = 0.208984375
Iteration no. 1177, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51182979284086, Training Error = 0.25
Iteration no. 1178, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.4842254884044, Training Error = 0.24609375
Iteration no. 1179, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.4993044447771, Training Error = 0.2265625
Iteration no. 1180, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49200237203953, Training Error = 0.255859375
Iteration no. 1181, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5124294914567, Training Error = 0.265625
Iteration no. 1182, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50534044460332, Training Error = 0.26953125
Iteration no. 1183, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53563428810175, Training Error = 0.259765625
Iteration no. 1184, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47497565734024, Training Error = 0.251953125
Iteration no. 1185, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50070125139759, Training Error = 0.240234375
Iteration no. 1186, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47058812493287, Training Error = 0.2109375
Iteration no. 1187, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.52910935051839, Training Error = 0.259765625
Iteration no. 1188, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47270809386774, Training Error = 0.224609375
Iteration no. 1189, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50749200598513, Training Error = 0.2421875
Iteration no. 1190, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49978621646948, Training Error = 0.2578125
Iteration no. 1191, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49411706079915, Training Error = 0.259765625
Iteration no. 1192, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50085435275598, Training Error = 0.267578125
Iteration no. 1193, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53754237185025, Training Error = 0.283203125
Iteration no. 1194, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49987755800919, Training Error = 0.244140625
Iteration no. 1195, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51479011437659, Training Error = 0.263671875
Iteration no. 1196, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47059927540895, Training Error = 0.224609375
Iteration no. 1197, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51509417401473, Training Error = 0.265625
Iteration no. 1198, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48346287721141, Training Error = 0.240234375
Iteration no. 1199, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.52999158111157, Training Error = 0.275390625
Iteration no. 1200, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51995807280152, Training Error = 0.251953125
Testing... average test_loss = 0.9074898555484, average test_pred_err = 0.457
Snapshotting C_model... done
Iteration no. 1201, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48235561793679, Training Error = 0.21484375
Iteration no. 1202, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5450952002152, Training Error = 0.275390625
Iteration no. 1203, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48579192077403, Training Error = 0.240234375
Iteration no. 1204, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49764317311109, Training Error = 0.25
Iteration no. 1205, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47709171887649, Training Error = 0.232421875
Iteration no. 1206, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52098746514506, Training Error = 0.26953125
Iteration no. 1207, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51887275596382, Training Error = 0.25
Iteration no. 1208, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48692280199974, Training Error = 0.236328125
Iteration no. 1209, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52406385691515, Training Error = 0.267578125
Iteration no. 1210, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48513060390255, Training Error = 0.23828125
Iteration no. 1211, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47992499019934, Training Error = 0.234375
Iteration no. 1212, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50153662270888, Training Error = 0.263671875
Iteration no. 1213, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48724008027125, Training Error = 0.255859375
Iteration no. 1214, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51419701205487, Training Error = 0.26953125
Iteration no. 1215, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49147204776407, Training Error = 0.263671875
Iteration no. 1216, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48275978069844, Training Error = 0.23046875
Iteration no. 1217, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53066409686574, Training Error = 0.291015625
Iteration no. 1218, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48006263364455, Training Error = 0.220703125
Iteration no. 1219, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49766810035699, Training Error = 0.2578125
Iteration no. 1220, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50979097290226, Training Error = 0.259765625
Iteration no. 1221, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5398429555692, Training Error = 0.265625
Iteration no. 1222, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48644574228466, Training Error = 0.2421875
Iteration no. 1223, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47353261078911, Training Error = 0.21875
Iteration no. 1224, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47136063759746, Training Error = 0.236328125
Iteration no. 1225, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53551165631246, Training Error = 0.275390625
Iteration no. 1226, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50311362640478, Training Error = 0.248046875
Iteration no. 1227, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50893409515867, Training Error = 0.26171875
Iteration no. 1228, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48804405039513, Training Error = 0.2421875
Iteration no. 1229, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48883439672016, Training Error = 0.2421875
Iteration no. 1230, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53646993036502, Training Error = 0.28125
Iteration no. 1231, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51136299504407, Training Error = 0.2421875
Iteration no. 1232, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5090290461501, Training Error = 0.244140625
Iteration no. 1233, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49311768800732, Training Error = 0.2421875
Iteration no. 1234, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49646079973647, Training Error = 0.236328125
Iteration no. 1235, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5062540561697, Training Error = 0.251953125
Iteration no. 1236, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47695632478283, Training Error = 0.234375
Iteration no. 1237, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.46563518821656, Training Error = 0.216796875
Iteration no. 1238, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47535720183395, Training Error = 0.25
Iteration no. 1239, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49198826190581, Training Error = 0.24609375
Iteration no. 1240, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52126402102429, Training Error = 0.265625
Iteration no. 1241, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52185115492429, Training Error = 0.26953125
Iteration no. 1242, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5133095443447, Training Error = 0.26171875
Iteration no. 1243, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52371056016923, Training Error = 0.294921875
Iteration no. 1244, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49992494081908, Training Error = 0.24609375
Iteration no. 1245, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53018092278968, Training Error = 0.28515625
Iteration no. 1246, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50141959589312, Training Error = 0.24609375
Iteration no. 1247, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50670639628746, Training Error = 0.26953125
Iteration no. 1248, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50496824489528, Training Error = 0.25
Iteration no. 1249, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49072088567293, Training Error = 0.259765625
Iteration no. 1250, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53001103759721, Training Error = 0.2890625
Testing... average test_loss = 0.9170913880896, average test_pred_err = 0.454
Iteration no. 1251, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50214646994486, Training Error = 0.240234375
Iteration no. 1252, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50049329747324, Training Error = 0.259765625
Iteration no. 1253, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47601101764703, Training Error = 0.240234375
Iteration no. 1254, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49695755184428, Training Error = 0.22265625
Iteration no. 1255, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52252483907463, Training Error = 0.279296875
Iteration no. 1256, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52008850896032, Training Error = 0.265625
Iteration no. 1257, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.477173415266, Training Error = 0.232421875
Iteration no. 1258, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51358327109259, Training Error = 0.25390625
Iteration no. 1259, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49207766733986, Training Error = 0.255859375
Iteration no. 1260, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51546983571738, Training Error = 0.26953125
Iteration no. 1261, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48917247073106, Training Error = 0.267578125
Iteration no. 1262, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50193652510652, Training Error = 0.2421875
Iteration no. 1263, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51160639160568, Training Error = 0.244140625
Iteration no. 1264, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.46582670146332, Training Error = 0.205078125
Iteration no. 1265, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48356838043784, Training Error = 0.26171875
Iteration no. 1266, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53787947356519, Training Error = 0.291015625
Iteration no. 1267, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50442837914528, Training Error = 0.255859375
Iteration no. 1268, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50580757018692, Training Error = 0.25390625
Iteration no. 1269, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50376938376104, Training Error = 0.25390625
Iteration no. 1270, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53199935060117, Training Error = 0.27734375
Iteration no. 1271, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48681841303527, Training Error = 0.234375
Iteration no. 1272, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48422495499314, Training Error = 0.23046875
Iteration no. 1273, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51734114001068, Training Error = 0.27734375
Iteration no. 1274, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51151694659758, Training Error = 0.263671875
Iteration no. 1275, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.53939941129724, Training Error = 0.2578125
Iteration no. 1276, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51346803934123, Training Error = 0.26953125
Iteration no. 1277, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48665739767669, Training Error = 0.236328125
Iteration no. 1278, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50683327946697, Training Error = 0.248046875
Iteration no. 1279, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5356725601188, Training Error = 0.27734375
Iteration no. 1280, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.4839052122987, Training Error = 0.25390625
Iteration no. 1281, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47228633318491, Training Error = 0.224609375
Iteration no. 1282, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51153192573744, Training Error = 0.248046875
Iteration no. 1283, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5047022942366, Training Error = 0.271484375
Iteration no. 1284, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50738850083413, Training Error = 0.267578125
Iteration no. 1285, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50705646526672, Training Error = 0.251953125
Iteration no. 1286, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51296211617499, Training Error = 0.26171875
Iteration no. 1287, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.49936964236854, Training Error = 0.26953125
Iteration no. 1288, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.46331991393771, Training Error = 0.220703125
Iteration no. 1289, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51425677314039, Training Error = 0.26171875
Iteration no. 1290, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52889722037567, Training Error = 0.287109375
Iteration no. 1291, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.5001914789803, Training Error = 0.244140625
Iteration no. 1292, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.46542197067783, Training Error = 0.224609375
Iteration no. 1293, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50736069493216, Training Error = 0.2421875
Iteration no. 1294, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.52283343644866, Training Error = 0.251953125
Iteration no. 1295, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.50368878543955, Training Error = 0.255859375
Iteration no. 1296, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.48062884492229, Training Error = 0.22265625
Iteration no. 1297, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51335872382115, Training Error = 0.25390625
Iteration no. 1298, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51624168496172, Training Error = 0.2734375
Iteration no. 1299, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.47385159733341, Training Error = 0.228515625
Iteration no. 1300, lr = 0.027682574402, attribute lr =0.00027682574402, average batch_loss = 0.51974105783054, Training Error = 0.265625
Testing... average test_loss = 0.98205615642587, average test_pred_err = 0.489
Snapshotting C_model... done
Iteration no. 1301, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50082399157805, Training Error = 0.263671875
Iteration no. 1302, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49933673434966, Training Error = 0.25
Iteration no. 1303, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51170243165609, Training Error = 0.26171875
Iteration no. 1304, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50397779887627, Training Error = 0.240234375
Iteration no. 1305, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50507436514364, Training Error = 0.2734375
Iteration no. 1306, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52351982348551, Training Error = 0.2734375
Iteration no. 1307, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50503322755077, Training Error = 0.2578125
Iteration no. 1308, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49633454928194, Training Error = 0.244140625
Iteration no. 1309, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51504996009853, Training Error = 0.263671875
Iteration no. 1310, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.47629673723865, Training Error = 0.234375
Iteration no. 1311, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48240407515309, Training Error = 0.248046875
Iteration no. 1312, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50403570008296, Training Error = 0.255859375
Iteration no. 1313, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52455137558544, Training Error = 0.26953125
Iteration no. 1314, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48445738403066, Training Error = 0.228515625
Iteration no. 1315, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48608918914673, Training Error = 0.234375
Iteration no. 1316, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50888701440721, Training Error = 0.248046875
Iteration no. 1317, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48136562782158, Training Error = 0.236328125
Iteration no. 1318, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51646428496665, Training Error = 0.26171875
Iteration no. 1319, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50734613933337, Training Error = 0.240234375
Iteration no. 1320, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50344297546742, Training Error = 0.251953125
Iteration no. 1321, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46990927987319, Training Error = 0.240234375
Iteration no. 1322, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46452520169435, Training Error = 0.19921875
Iteration no. 1323, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49619250047983, Training Error = 0.24609375
Iteration no. 1324, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.54588480728973, Training Error = 0.306640625
Iteration no. 1325, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48951553650485, Training Error = 0.244140625
Iteration no. 1326, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.47682756410729, Training Error = 0.23046875
Iteration no. 1327, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52359997061185, Training Error = 0.26171875
Iteration no. 1328, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48929640527457, Training Error = 0.22265625
Iteration no. 1329, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46952039633657, Training Error = 0.22265625
Iteration no. 1330, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50807640321346, Training Error = 0.251953125
Iteration no. 1331, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49813827497138, Training Error = 0.244140625
Iteration no. 1332, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49600508705526, Training Error = 0.2421875
Iteration no. 1333, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50410352077325, Training Error = 0.26953125
Iteration no. 1334, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.53694060191339, Training Error = 0.26953125
Iteration no. 1335, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49610639003613, Training Error = 0.234375
Iteration no. 1336, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52955275896066, Training Error = 0.275390625
Iteration no. 1337, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48937791289406, Training Error = 0.232421875
Iteration no. 1338, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51352815479047, Training Error = 0.259765625
Iteration no. 1339, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48217513852598, Training Error = 0.220703125
Iteration no. 1340, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50732092435055, Training Error = 0.267578125
Iteration no. 1341, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51362004192762, Training Error = 0.259765625
Iteration no. 1342, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50941091652407, Training Error = 0.251953125
Iteration no. 1343, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49819616885126, Training Error = 0.263671875
Iteration no. 1344, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.4928424089372, Training Error = 0.23046875
Iteration no. 1345, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.47762211587216, Training Error = 0.22265625
Iteration no. 1346, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51858585584261, Training Error = 0.267578125
Iteration no. 1347, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51469944543444, Training Error = 0.255859375
Iteration no. 1348, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46959438345213, Training Error = 0.232421875
Iteration no. 1349, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50207971635809, Training Error = 0.244140625
Iteration no. 1350, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50143761694806, Training Error = 0.244140625
Testing... average test_loss = 0.93149484531295, average test_pred_err = 0.461
Iteration no. 1351, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46823318951483, Training Error = 0.2421875
Iteration no. 1352, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46865931627109, Training Error = 0.224609375
Iteration no. 1353, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.47617675005497, Training Error = 0.23046875
Iteration no. 1354, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49714228231829, Training Error = 0.23828125
Iteration no. 1355, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50215061104635, Training Error = 0.23828125
Iteration no. 1356, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49549781155798, Training Error = 0.234375
Iteration no. 1357, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50449197778969, Training Error = 0.232421875
Iteration no. 1358, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50762092884653, Training Error = 0.263671875
Iteration no. 1359, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51759919918, Training Error = 0.26171875
Iteration no. 1360, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.47214810994445, Training Error = 0.228515625
Iteration no. 1361, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49503125692521, Training Error = 0.2421875
Iteration no. 1362, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.47643416287263, Training Error = 0.24609375
Iteration no. 1363, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49529687890105, Training Error = 0.26171875
Iteration no. 1364, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49741429124417, Training Error = 0.251953125
Iteration no. 1365, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52230662246026, Training Error = 0.26953125
Iteration no. 1366, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49032814635206, Training Error = 0.240234375
Iteration no. 1367, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.54099111189857, Training Error = 0.26953125
Iteration no. 1368, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.46712617604965, Training Error = 0.236328125
Iteration no. 1369, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52154984280982, Training Error = 0.2734375
Iteration no. 1370, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52331527910586, Training Error = 0.275390625
Iteration no. 1371, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.52171962987835, Training Error = 0.251953125
Iteration no. 1372, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49564141602602, Training Error = 0.23046875
Iteration no. 1373, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48321631397371, Training Error = 0.234375
Iteration no. 1374, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.5283414390179, Training Error = 0.265625
Iteration no. 1375, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49334351065522, Training Error = 0.248046875
Iteration no. 1376, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.53692792262797, Training Error = 0.26171875
Iteration no. 1377, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.4677580042579, Training Error = 0.232421875
Iteration no. 1378, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51089358205933, Training Error = 0.25
Iteration no. 1379, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49665717361072, Training Error = 0.259765625
Iteration no. 1380, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.4800311420487, Training Error = 0.2421875
Iteration no. 1381, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50162898328018, Training Error = 0.255859375
Iteration no. 1382, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.48235145133986, Training Error = 0.24609375
Iteration no. 1383, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50997584659072, Training Error = 0.248046875
Iteration no. 1384, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49153998644506, Training Error = 0.228515625
Iteration no. 1385, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.5168676841764, Training Error = 0.267578125
Iteration no. 1386, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50684090669596, Training Error = 0.23828125
Iteration no. 1387, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50585564328344, Training Error = 0.23828125
Iteration no. 1388, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.4707185803181, Training Error = 0.205078125
Iteration no. 1389, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51683915970757, Training Error = 0.279296875
Iteration no. 1390, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51009926921172, Training Error = 0.255859375
Iteration no. 1391, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51583723144005, Training Error = 0.263671875
Iteration no. 1392, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.51314279720091, Training Error = 0.2578125
Iteration no. 1393, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49313750262045, Training Error = 0.263671875
Iteration no. 1394, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.4913505122431, Training Error = 0.220703125
Iteration no. 1395, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.53242326505743, Training Error = 0.271484375
Iteration no. 1396, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50261045151966, Training Error = 0.236328125
Iteration no. 1397, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49886260011646, Training Error = 0.23828125
Iteration no. 1398, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.49365531116349, Training Error = 0.240234375
Iteration no. 1399, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.53904826081459, Training Error = 0.27734375
Iteration no. 1400, lr = 0.0193778020814, attribute lr =0.000193778020814, average batch_loss = 0.50057573367853, Training Error = 0.25
Testing... average test_loss = 0.93367748336407, average test_pred_err = 0.474
Snapshotting C_model... done
Iteration no. 1401, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51846384843119, Training Error = 0.263671875
Iteration no. 1402, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48455945545734, Training Error = 0.234375
Iteration no. 1403, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49528835624078, Training Error = 0.265625
Iteration no. 1404, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50357243595685, Training Error = 0.259765625
Iteration no. 1405, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51289480622431, Training Error = 0.271484375
Iteration no. 1406, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48329502481146, Training Error = 0.232421875
Iteration no. 1407, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.46299190662247, Training Error = 0.232421875
Iteration no. 1408, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48660188414347, Training Error = 0.25390625
Iteration no. 1409, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52205012015714, Training Error = 0.255859375
Iteration no. 1410, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.46542915192698, Training Error = 0.20703125
Iteration no. 1411, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.46917294982661, Training Error = 0.220703125
Iteration no. 1412, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48497133663996, Training Error = 0.23828125
Iteration no. 1413, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48698384006644, Training Error = 0.24609375
Iteration no. 1414, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51114418947927, Training Error = 0.251953125
Iteration no. 1415, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49636746356379, Training Error = 0.2421875
Iteration no. 1416, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47960037148526, Training Error = 0.228515625
Iteration no. 1417, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52970440861136, Training Error = 0.28515625
Iteration no. 1418, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.54005393016989, Training Error = 0.294921875
Iteration no. 1419, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50432968989617, Training Error = 0.248046875
Iteration no. 1420, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.522307928784, Training Error = 0.275390625
Iteration no. 1421, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51285218881412, Training Error = 0.263671875
Iteration no. 1422, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51998645785643, Training Error = 0.265625
Iteration no. 1423, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5621742853654, Training Error = 0.30859375
Iteration no. 1424, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49549498509638, Training Error = 0.240234375
Iteration no. 1425, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48295848019644, Training Error = 0.234375
Iteration no. 1426, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.46551853267661, Training Error = 0.228515625
Iteration no. 1427, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.44513104685464, Training Error = 0.193359375
Iteration no. 1428, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47407357345778, Training Error = 0.224609375
Iteration no. 1429, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47966665768748, Training Error = 0.244140625
Iteration no. 1430, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.4534550026923, Training Error = 0.21875
Iteration no. 1431, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5370719036961, Training Error = 0.294921875
Iteration no. 1432, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51035329388305, Training Error = 0.25
Iteration no. 1433, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.4716513133354, Training Error = 0.220703125
Iteration no. 1434, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49736462671689, Training Error = 0.23046875
Iteration no. 1435, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5124106709898, Training Error = 0.255859375
Iteration no. 1436, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.46747570298791, Training Error = 0.220703125
Iteration no. 1437, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.4855099329425, Training Error = 0.255859375
Iteration no. 1438, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48358318259688, Training Error = 0.25
Iteration no. 1439, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49425119871853, Training Error = 0.24609375
Iteration no. 1440, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48215808591359, Training Error = 0.232421875
Iteration no. 1441, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49186075102774, Training Error = 0.2421875
Iteration no. 1442, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48559626147424, Training Error = 0.251953125
Iteration no. 1443, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50921625806747, Training Error = 0.263671875
Iteration no. 1444, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52750093845233, Training Error = 0.291015625
Iteration no. 1445, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.53796247412305, Training Error = 0.255859375
Iteration no. 1446, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49880531114041, Training Error = 0.248046875
Iteration no. 1447, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52951379790736, Training Error = 0.267578125
Iteration no. 1448, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52957352568373, Training Error = 0.287109375
Iteration no. 1449, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47449707649919, Training Error = 0.25
Iteration no. 1450, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5029051182784, Training Error = 0.23828125
Testing... average test_loss = 0.94079703361933, average test_pred_err = 0.486
Iteration no. 1451, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5053984243266, Training Error = 0.255859375
Iteration no. 1452, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48375309504502, Training Error = 0.224609375
Iteration no. 1453, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51540293822538, Training Error = 0.240234375
Iteration no. 1454, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48997425848736, Training Error = 0.236328125
Iteration no. 1455, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47117639765857, Training Error = 0.216796875
Iteration no. 1456, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49545395812159, Training Error = 0.24609375
Iteration no. 1457, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5084276400051, Training Error = 0.255859375
Iteration no. 1458, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47518698011012, Training Error = 0.232421875
Iteration no. 1459, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51252681379739, Training Error = 0.24609375
Iteration no. 1460, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5075550420578, Training Error = 0.263671875
Iteration no. 1461, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52021462205926, Training Error = 0.275390625
Iteration no. 1462, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.45496872090597, Training Error = 0.201171875
Iteration no. 1463, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.54151161576717, Training Error = 0.3046875
Iteration no. 1464, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51197709379465, Training Error = 0.2578125
Iteration no. 1465, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5343116557366, Training Error = 0.26171875
Iteration no. 1466, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52430427804296, Training Error = 0.27734375
Iteration no. 1467, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.47448517959524, Training Error = 0.23046875
Iteration no. 1468, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49361765041208, Training Error = 0.2578125
Iteration no. 1469, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51434973892918, Training Error = 0.28515625
Iteration no. 1470, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51191952825894, Training Error = 0.26171875
Iteration no. 1471, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.58011299794328, Training Error = 0.3046875
Iteration no. 1472, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51853947159354, Training Error = 0.26953125
Iteration no. 1473, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51673068062638, Training Error = 0.28125
Iteration no. 1474, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52179159760088, Training Error = 0.26171875
Iteration no. 1475, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.45889306765491, Training Error = 0.2265625
Iteration no. 1476, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51045166352512, Training Error = 0.25
Iteration no. 1477, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.53791165763781, Training Error = 0.30078125
Iteration no. 1478, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50876030338095, Training Error = 0.244140625
Iteration no. 1479, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50759942839613, Training Error = 0.267578125
Iteration no. 1480, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48851613327372, Training Error = 0.244140625
Iteration no. 1481, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52394467029399, Training Error = 0.28125
Iteration no. 1482, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49118780438583, Training Error = 0.25390625
Iteration no. 1483, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.55154693799389, Training Error = 0.2890625
Iteration no. 1484, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50510595393509, Training Error = 0.25390625
Iteration no. 1485, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49698650037883, Training Error = 0.244140625
Iteration no. 1486, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.50811771989556, Training Error = 0.26171875
Iteration no. 1487, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49891058871344, Training Error = 0.234375
Iteration no. 1488, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.52611157465696, Training Error = 0.27734375
Iteration no. 1489, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49842334922478, Training Error = 0.24609375
Iteration no. 1490, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51413182619845, Training Error = 0.25390625
Iteration no. 1491, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48589635966156, Training Error = 0.25
Iteration no. 1492, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48418127639826, Training Error = 0.2265625
Iteration no. 1493, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.499722534269, Training Error = 0.265625
Iteration no. 1494, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51842386965922, Training Error = 0.244140625
Iteration no. 1495, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.49977991415336, Training Error = 0.23046875
Iteration no. 1496, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.5557059727151, Training Error = 0.294921875
Iteration no. 1497, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48922198514316, Training Error = 0.244140625
Iteration no. 1498, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48859121639285, Training Error = 0.234375
Iteration no. 1499, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.51467383831285, Training Error = 0.259765625
Iteration no. 1500, lr = 0.01356446145698, attribute lr =0.0001356446145698, average batch_loss = 0.48355077959833, Training Error = 0.244140625
Testing... average test_loss = 0.92868811568279, average test_pred_err = 0.453
Snapshotting C_model... done
Iteration no. 1501, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48259946652849, Training Error = 0.240234375
Iteration no. 1502, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.4907168908193, Training Error = 0.23046875
Iteration no. 1503, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48873608941557, Training Error = 0.236328125
Iteration no. 1504, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52232002741553, Training Error = 0.2578125
Iteration no. 1505, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.4889924115918, Training Error = 0.23828125
Iteration no. 1506, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47410182181798, Training Error = 0.2421875
Iteration no. 1507, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50494911438596, Training Error = 0.25390625
Iteration no. 1508, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49067293641448, Training Error = 0.240234375
Iteration no. 1509, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50570082706253, Training Error = 0.26171875
Iteration no. 1510, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52787858095656, Training Error = 0.2734375
Iteration no. 1511, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48441136513794, Training Error = 0.2421875
Iteration no. 1512, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.45577883668194, Training Error = 0.212890625
Iteration no. 1513, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51754127650708, Training Error = 0.26953125
Iteration no. 1514, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48509095682637, Training Error = 0.236328125
Iteration no. 1515, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50628409291465, Training Error = 0.251953125
Iteration no. 1516, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52792671580879, Training Error = 0.271484375
Iteration no. 1517, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48619319015172, Training Error = 0.236328125
Iteration no. 1518, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50011796736883, Training Error = 0.2578125
Iteration no. 1519, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47748467460908, Training Error = 0.25390625
Iteration no. 1520, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50039482042326, Training Error = 0.2421875
Iteration no. 1521, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50600584969903, Training Error = 0.279296875
Iteration no. 1522, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.5222424077005, Training Error = 0.2890625
Iteration no. 1523, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49760826390008, Training Error = 0.26171875
Iteration no. 1524, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50747844806303, Training Error = 0.267578125
Iteration no. 1525, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49282403846975, Training Error = 0.2421875
Iteration no. 1526, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49763795554, Training Error = 0.25390625
Iteration no. 1527, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.46909933702688, Training Error = 0.22265625
Iteration no. 1528, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.54667643562565, Training Error = 0.28125
Iteration no. 1529, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48659935017859, Training Error = 0.23828125
Iteration no. 1530, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50138583207933, Training Error = 0.259765625
Iteration no. 1531, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.46424570520669, Training Error = 0.228515625
Iteration no. 1532, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47185496978241, Training Error = 0.23046875
Iteration no. 1533, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47547465462762, Training Error = 0.240234375
Iteration no. 1534, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51833120421431, Training Error = 0.2578125
Iteration no. 1535, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49255508428957, Training Error = 0.25
Iteration no. 1536, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.53870346503932, Training Error = 0.28515625
Iteration no. 1537, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48191496958312, Training Error = 0.23828125
Iteration no. 1538, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51593410710114, Training Error = 0.28515625
Iteration no. 1539, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.45549924880543, Training Error = 0.21875
Iteration no. 1540, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.4794990400093, Training Error = 0.228515625
Iteration no. 1541, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48699970630395, Training Error = 0.240234375
Iteration no. 1542, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49378998383098, Training Error = 0.2421875
Iteration no. 1543, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47102108051729, Training Error = 0.240234375
Iteration no. 1544, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.4846070878054, Training Error = 0.244140625
Iteration no. 1545, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50304627346206, Training Error = 0.22265625
Iteration no. 1546, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52317566749425, Training Error = 0.244140625
Iteration no. 1547, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.4940825273985, Training Error = 0.2578125
Iteration no. 1548, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49288215337168, Training Error = 0.2421875
Iteration no. 1549, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50662850739174, Training Error = 0.26171875
Iteration no. 1550, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51749247222252, Training Error = 0.248046875
Testing... average test_loss = 0.93651486323177, average test_pred_err = 0.485
Iteration no. 1551, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50308188273466, Training Error = 0.265625
Iteration no. 1552, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52746424979694, Training Error = 0.25
Iteration no. 1553, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.4873682286308, Training Error = 0.244140625
Iteration no. 1554, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52783662694077, Training Error = 0.28125
Iteration no. 1555, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49152181423236, Training Error = 0.240234375
Iteration no. 1556, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51204577397183, Training Error = 0.26171875
Iteration no. 1557, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48490705358581, Training Error = 0.21875
Iteration no. 1558, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51264863280397, Training Error = 0.2578125
Iteration no. 1559, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47682875166655, Training Error = 0.234375
Iteration no. 1560, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49554595761916, Training Error = 0.2734375
Iteration no. 1561, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50447509248726, Training Error = 0.259765625
Iteration no. 1562, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47180793968572, Training Error = 0.2265625
Iteration no. 1563, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52371541263144, Training Error = 0.28515625
Iteration no. 1564, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.45733757316157, Training Error = 0.216796875
Iteration no. 1565, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48826964022474, Training Error = 0.240234375
Iteration no. 1566, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50589560343355, Training Error = 0.26171875
Iteration no. 1567, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51062087427202, Training Error = 0.259765625
Iteration no. 1568, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49446460673038, Training Error = 0.25
Iteration no. 1569, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50690791611779, Training Error = 0.2578125
Iteration no. 1570, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50793679116683, Training Error = 0.265625
Iteration no. 1571, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49815177394983, Training Error = 0.25
Iteration no. 1572, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49689205326663, Training Error = 0.2421875
Iteration no. 1573, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.46632651077993, Training Error = 0.23828125
Iteration no. 1574, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48847368059481, Training Error = 0.251953125
Iteration no. 1575, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47502402282241, Training Error = 0.205078125
Iteration no. 1576, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48112220642601, Training Error = 0.216796875
Iteration no. 1577, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.53399363475308, Training Error = 0.267578125
Iteration no. 1578, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52683290909927, Training Error = 0.24609375
Iteration no. 1579, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52564670497546, Training Error = 0.26171875
Iteration no. 1580, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.5014256980641, Training Error = 0.259765625
Iteration no. 1581, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49489767677488, Training Error = 0.251953125
Iteration no. 1582, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50003011166437, Training Error = 0.251953125
Iteration no. 1583, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49644197718923, Training Error = 0.251953125
Iteration no. 1584, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.49625173225415, Training Error = 0.23046875
Iteration no. 1585, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48902880647128, Training Error = 0.22265625
Iteration no. 1586, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51279767985264, Training Error = 0.25390625
Iteration no. 1587, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47279507266574, Training Error = 0.2265625
Iteration no. 1588, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.45322906495343, Training Error = 0.205078125
Iteration no. 1589, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.5156976785175, Training Error = 0.287109375
Iteration no. 1590, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.48049948632736, Training Error = 0.263671875
Iteration no. 1591, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50088875761643, Training Error = 0.255859375
Iteration no. 1592, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.52953247262527, Training Error = 0.26953125
Iteration no. 1593, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.53120664569922, Training Error = 0.259765625
Iteration no. 1594, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50260979446336, Training Error = 0.265625
Iteration no. 1595, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.47974905324365, Training Error = 0.2421875
Iteration no. 1596, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50663216310483, Training Error = 0.255859375
Iteration no. 1597, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51423970973005, Training Error = 0.244140625
Iteration no. 1598, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.50761744360371, Training Error = 0.255859375
Iteration no. 1599, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51861585684474, Training Error = 0.259765625
Iteration no. 1600, lr = 0.009495123019886, attribute lr =9.495123019886e-05, average batch_loss = 0.51035292762501, Training Error = 0.275390625
Testing... average test_loss = 0.98744609821062, average test_pred_err = 0.491
Snapshotting C_model... done
Iteration no. 1601, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51607943403695, Training Error = 0.267578125
Iteration no. 1602, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51661467676422, Training Error = 0.283203125
Iteration no. 1603, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51744649215593, Training Error = 0.265625
Iteration no. 1604, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.5065518176742, Training Error = 0.240234375
Iteration no. 1605, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49842404568497, Training Error = 0.248046875
Iteration no. 1606, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47387490568261, Training Error = 0.220703125
Iteration no. 1607, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47086116264695, Training Error = 0.224609375
Iteration no. 1608, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52255089678439, Training Error = 0.26953125
Iteration no. 1609, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51594536497965, Training Error = 0.2734375
Iteration no. 1610, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.4972176715385, Training Error = 0.24609375
Iteration no. 1611, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.4978459738733, Training Error = 0.248046875
Iteration no. 1612, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47764474979566, Training Error = 0.224609375
Iteration no. 1613, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52023989339104, Training Error = 0.2734375
Iteration no. 1614, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.5178498549269, Training Error = 0.263671875
Iteration no. 1615, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52269513008192, Training Error = 0.28515625
Iteration no. 1616, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50113033835734, Training Error = 0.2421875
Iteration no. 1617, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51171536308625, Training Error = 0.255859375
Iteration no. 1618, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.46440755802116, Training Error = 0.23046875
Iteration no. 1619, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47020872656236, Training Error = 0.2265625
Iteration no. 1620, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.53293638908842, Training Error = 0.279296875
Iteration no. 1621, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.45600032201962, Training Error = 0.212890625
Iteration no. 1622, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50251180213892, Training Error = 0.2578125
Iteration no. 1623, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50757794191124, Training Error = 0.248046875
Iteration no. 1624, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.4888975561668, Training Error = 0.232421875
Iteration no. 1625, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50225550468697, Training Error = 0.234375
Iteration no. 1626, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49539355445177, Training Error = 0.248046875
Iteration no. 1627, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52448856984781, Training Error = 0.267578125
Iteration no. 1628, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47603279085953, Training Error = 0.2265625
Iteration no. 1629, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.46404747207638, Training Error = 0.208984375
Iteration no. 1630, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48987188165552, Training Error = 0.251953125
Iteration no. 1631, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49792871022597, Training Error = 0.25390625
Iteration no. 1632, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51102386450354, Training Error = 0.232421875
Iteration no. 1633, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.53200871913994, Training Error = 0.26953125
Iteration no. 1634, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52085856585703, Training Error = 0.283203125
Iteration no. 1635, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51485553420494, Training Error = 0.271484375
Iteration no. 1636, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.5295341538836, Training Error = 0.2734375
Iteration no. 1637, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52438283414026, Training Error = 0.28125
Iteration no. 1638, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51360538503415, Training Error = 0.275390625
Iteration no. 1639, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52977350402896, Training Error = 0.28515625
Iteration no. 1640, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52335350924764, Training Error = 0.267578125
Iteration no. 1641, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49078175101734, Training Error = 0.263671875
Iteration no. 1642, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50979028227853, Training Error = 0.259765625
Iteration no. 1643, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47972823979041, Training Error = 0.240234375
Iteration no. 1644, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47591568703561, Training Error = 0.236328125
Iteration no. 1645, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.46687520461352, Training Error = 0.212890625
Iteration no. 1646, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51735068424317, Training Error = 0.267578125
Iteration no. 1647, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50711949038024, Training Error = 0.27734375
Iteration no. 1648, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49820353292805, Training Error = 0.248046875
Iteration no. 1649, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49377875747607, Training Error = 0.232421875
Iteration no. 1650, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52654218472921, Training Error = 0.25
Testing... average test_loss = 0.95329709076263, average test_pred_err = 0.48
Iteration no. 1651, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50288089738106, Training Error = 0.251953125
Iteration no. 1652, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47037625796344, Training Error = 0.224609375
Iteration no. 1653, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49357204500372, Training Error = 0.244140625
Iteration no. 1654, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49072617828702, Training Error = 0.25
Iteration no. 1655, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51850297288611, Training Error = 0.265625
Iteration no. 1656, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.46068277782011, Training Error = 0.19921875
Iteration no. 1657, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48080684944078, Training Error = 0.234375
Iteration no. 1658, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52356218635795, Training Error = 0.2578125
Iteration no. 1659, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49526189385657, Training Error = 0.255859375
Iteration no. 1660, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.4876740887322, Training Error = 0.234375
Iteration no. 1661, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49737262382319, Training Error = 0.2421875
Iteration no. 1662, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51327851279697, Training Error = 0.26953125
Iteration no. 1663, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50959082208209, Training Error = 0.263671875
Iteration no. 1664, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49918479394883, Training Error = 0.244140625
Iteration no. 1665, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51048621618947, Training Error = 0.25
Iteration no. 1666, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50670318111408, Training Error = 0.2578125
Iteration no. 1667, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52754534753756, Training Error = 0.265625
Iteration no. 1668, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49944825609392, Training Error = 0.25
Iteration no. 1669, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48166650281729, Training Error = 0.2265625
Iteration no. 1670, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48783062793545, Training Error = 0.232421875
Iteration no. 1671, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51338413586597, Training Error = 0.25
Iteration no. 1672, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49742762962332, Training Error = 0.236328125
Iteration no. 1673, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49049252977988, Training Error = 0.232421875
Iteration no. 1674, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.54512687445184, Training Error = 0.30078125
Iteration no. 1675, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.53909400655666, Training Error = 0.287109375
Iteration no. 1676, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51592282484143, Training Error = 0.265625
Iteration no. 1677, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49451947542957, Training Error = 0.236328125
Iteration no. 1678, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.45664700996128, Training Error = 0.2109375
Iteration no. 1679, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52437140985447, Training Error = 0.27734375
Iteration no. 1680, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.52467184538605, Training Error = 0.263671875
Iteration no. 1681, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49625793443354, Training Error = 0.240234375
Iteration no. 1682, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48382021775301, Training Error = 0.22265625
Iteration no. 1683, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49313909338457, Training Error = 0.25390625
Iteration no. 1684, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.4738758308533, Training Error = 0.21875
Iteration no. 1685, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49414049494148, Training Error = 0.26953125
Iteration no. 1686, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49917543906408, Training Error = 0.265625
Iteration no. 1687, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.5173225494592, Training Error = 0.27734375
Iteration no. 1688, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51992069590624, Training Error = 0.255859375
Iteration no. 1689, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.46829998481612, Training Error = 0.203125
Iteration no. 1690, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48879751503382, Training Error = 0.22265625
Iteration no. 1691, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.53550249292093, Training Error = 0.27734375
Iteration no. 1692, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.48315895326541, Training Error = 0.228515625
Iteration no. 1693, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.51741625515568, Training Error = 0.271484375
Iteration no. 1694, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47342021571749, Training Error = 0.236328125
Iteration no. 1695, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50058096564651, Training Error = 0.25390625
Iteration no. 1696, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.50846181210561, Training Error = 0.265625
Iteration no. 1697, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47587713005804, Training Error = 0.22265625
Iteration no. 1698, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.49864789339601, Training Error = 0.240234375
Iteration no. 1699, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.47498033770939, Training Error = 0.23828125
Iteration no. 1700, lr = 0.0066465861139202, attribute lr =6.6465861139202e-05, average batch_loss = 0.5159009524236, Training Error = 0.255859375
Testing... average test_loss = 0.8828377571905, average test_pred_err = 0.461
Snapshotting C_model... done
Iteration no. 1701, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.45686444085735, Training Error = 0.21484375
Iteration no. 1702, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48126230886407, Training Error = 0.234375
Iteration no. 1703, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50364651080131, Training Error = 0.25
Iteration no. 1704, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48227483771004, Training Error = 0.24609375
Iteration no. 1705, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47605923858217, Training Error = 0.2265625
Iteration no. 1706, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50228038410955, Training Error = 0.23828125
Iteration no. 1707, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.55934343470319, Training Error = 0.298828125
Iteration no. 1708, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48408624799465, Training Error = 0.234375
Iteration no. 1709, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48747931037202, Training Error = 0.236328125
Iteration no. 1710, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50690001775292, Training Error = 0.232421875
Iteration no. 1711, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50386754249288, Training Error = 0.2421875
Iteration no. 1712, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52804724924877, Training Error = 0.27734375
Iteration no. 1713, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47752241885521, Training Error = 0.2109375
Iteration no. 1714, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50618860790041, Training Error = 0.275390625
Iteration no. 1715, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51759465245268, Training Error = 0.279296875
Iteration no. 1716, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.54578612686294, Training Error = 0.291015625
Iteration no. 1717, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50312763824551, Training Error = 0.240234375
Iteration no. 1718, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5261647159065, Training Error = 0.25
Iteration no. 1719, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48819309236966, Training Error = 0.248046875
Iteration no. 1720, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.53753288483061, Training Error = 0.275390625
Iteration no. 1721, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48358524420782, Training Error = 0.2421875
Iteration no. 1722, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49702988547074, Training Error = 0.232421875
Iteration no. 1723, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.46722965550763, Training Error = 0.228515625
Iteration no. 1724, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52070716286171, Training Error = 0.26171875
Iteration no. 1725, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52193041314057, Training Error = 0.255859375
Iteration no. 1726, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51207135170727, Training Error = 0.25390625
Iteration no. 1727, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.46905228813612, Training Error = 0.232421875
Iteration no. 1728, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4868902679337, Training Error = 0.23046875
Iteration no. 1729, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48650785537712, Training Error = 0.2421875
Iteration no. 1730, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.45288093901349, Training Error = 0.220703125
Iteration no. 1731, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5187623799103, Training Error = 0.267578125
Iteration no. 1732, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49781784710141, Training Error = 0.244140625
Iteration no. 1733, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49289365727997, Training Error = 0.255859375
Iteration no. 1734, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47642888899843, Training Error = 0.224609375
Iteration no. 1735, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.53304665116689, Training Error = 0.26171875
Iteration no. 1736, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50383984520641, Training Error = 0.265625
Iteration no. 1737, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5213097013561, Training Error = 0.271484375
Iteration no. 1738, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49114576151561, Training Error = 0.244140625
Iteration no. 1739, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47236395651, Training Error = 0.22265625
Iteration no. 1740, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.46168115475145, Training Error = 0.22265625
Iteration no. 1741, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50856024535581, Training Error = 0.248046875
Iteration no. 1742, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.46619260814771, Training Error = 0.212890625
Iteration no. 1743, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52695462236505, Training Error = 0.291015625
Iteration no. 1744, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48249123923549, Training Error = 0.244140625
Iteration no. 1745, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49593072980771, Training Error = 0.228515625
Iteration no. 1746, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4931546726286, Training Error = 0.23828125
Iteration no. 1747, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50032609684762, Training Error = 0.25
Iteration no. 1748, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50203429999154, Training Error = 0.248046875
Iteration no. 1749, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47727696779423, Training Error = 0.2421875
Iteration no. 1750, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.54317332380451, Training Error = 0.29296875
Testing... average test_loss = 0.98902211674785, average test_pred_err = 0.499
Iteration no. 1751, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49132886375837, Training Error = 0.25390625
Iteration no. 1752, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5032516962614, Training Error = 0.25390625
Iteration no. 1753, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52045202672938, Training Error = 0.283203125
Iteration no. 1754, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52589124909359, Training Error = 0.263671875
Iteration no. 1755, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.55023704891001, Training Error = 0.271484375
Iteration no. 1756, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4988655109504, Training Error = 0.24609375
Iteration no. 1757, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50103469466484, Training Error = 0.24609375
Iteration no. 1758, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48143529028841, Training Error = 0.23828125
Iteration no. 1759, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50956816824399, Training Error = 0.283203125
Iteration no. 1760, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.53590932573947, Training Error = 0.28125
Iteration no. 1761, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4845026055188, Training Error = 0.23046875
Iteration no. 1762, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50460320111308, Training Error = 0.244140625
Iteration no. 1763, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5165865950594, Training Error = 0.26953125
Iteration no. 1764, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49981356662846, Training Error = 0.25390625
Iteration no. 1765, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48589591423191, Training Error = 0.255859375
Iteration no. 1766, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49466710559263, Training Error = 0.26171875
Iteration no. 1767, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5240443618082, Training Error = 0.255859375
Iteration no. 1768, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51612307528381, Training Error = 0.27734375
Iteration no. 1769, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49854273133702, Training Error = 0.255859375
Iteration no. 1770, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48291587300245, Training Error = 0.21875
Iteration no. 1771, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47650820838008, Training Error = 0.224609375
Iteration no. 1772, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4893560287158, Training Error = 0.240234375
Iteration no. 1773, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50751258675775, Training Error = 0.267578125
Iteration no. 1774, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.46272522634115, Training Error = 0.2265625
Iteration no. 1775, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47105125607703, Training Error = 0.2265625
Iteration no. 1776, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52440580957154, Training Error = 0.2734375
Iteration no. 1777, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52043412804807, Training Error = 0.29296875
Iteration no. 1778, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51290615324148, Training Error = 0.27734375
Iteration no. 1779, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4776303852338, Training Error = 0.255859375
Iteration no. 1780, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.53407093128635, Training Error = 0.267578125
Iteration no. 1781, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50353888322549, Training Error = 0.236328125
Iteration no. 1782, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.45281131287055, Training Error = 0.201171875
Iteration no. 1783, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51028544161464, Training Error = 0.2421875
Iteration no. 1784, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51539587461006, Training Error = 0.267578125
Iteration no. 1785, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52722879157218, Training Error = 0.28125
Iteration no. 1786, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.48503746011673, Training Error = 0.23828125
Iteration no. 1787, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.53058248982278, Training Error = 0.255859375
Iteration no. 1788, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.52450519005724, Training Error = 0.267578125
Iteration no. 1789, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.51078053601896, Training Error = 0.255859375
Iteration no. 1790, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50328680551718, Training Error = 0.2421875
Iteration no. 1791, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49094593028442, Training Error = 0.22265625
Iteration no. 1792, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.5119216911273, Training Error = 0.26953125
Iteration no. 1793, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.4865388103474, Training Error = 0.24609375
Iteration no. 1794, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49247464411266, Training Error = 0.2265625
Iteration no. 1795, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50286043127071, Training Error = 0.2578125
Iteration no. 1796, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.49865548048209, Training Error = 0.24609375
Iteration no. 1797, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.50161147256475, Training Error = 0.240234375
Iteration no. 1798, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.53695998885317, Training Error = 0.26953125
Iteration no. 1799, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.47602788473186, Training Error = 0.232421875
Iteration no. 1800, lr = 0.0046526102797441, attribute lr =4.6526102797441e-05, average batch_loss = 0.54423650652024, Training Error = 0.28515625
Testing... average test_loss = 0.96866810556795, average test_pred_err = 0.483
Snapshotting C_model... done
Iteration no. 1801, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51015420782662, Training Error = 0.25
Iteration no. 1802, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49376303343615, Training Error = 0.244140625
Iteration no. 1803, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52939503474022, Training Error = 0.265625
Iteration no. 1804, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52273489464636, Training Error = 0.265625
Iteration no. 1805, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50195224562998, Training Error = 0.263671875
Iteration no. 1806, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51946882593001, Training Error = 0.275390625
Iteration no. 1807, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50823185444982, Training Error = 0.23828125
Iteration no. 1808, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.5084029760152, Training Error = 0.25390625
Iteration no. 1809, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48477246488201, Training Error = 0.2421875
Iteration no. 1810, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48538047034782, Training Error = 0.25
Iteration no. 1811, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50548803016616, Training Error = 0.263671875
Iteration no. 1812, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49700989522536, Training Error = 0.236328125
Iteration no. 1813, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48096337420573, Training Error = 0.224609375
Iteration no. 1814, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48774546722336, Training Error = 0.2265625
Iteration no. 1815, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49979254985194, Training Error = 0.24609375
Iteration no. 1816, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52739868622256, Training Error = 0.26171875
Iteration no. 1817, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50208881617451, Training Error = 0.2578125
Iteration no. 1818, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.484066917357, Training Error = 0.2265625
Iteration no. 1819, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48517402713015, Training Error = 0.25
Iteration no. 1820, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51827337339463, Training Error = 0.27734375
Iteration no. 1821, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48768169940121, Training Error = 0.25390625
Iteration no. 1822, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.54020392200308, Training Error = 0.265625
Iteration no. 1823, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48183867736444, Training Error = 0.244140625
Iteration no. 1824, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51527637974454, Training Error = 0.283203125
Iteration no. 1825, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49756836083463, Training Error = 0.2578125
Iteration no. 1826, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52057575514669, Training Error = 0.279296875
Iteration no. 1827, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49114576834343, Training Error = 0.236328125
Iteration no. 1828, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52685394122629, Training Error = 0.28515625
Iteration no. 1829, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52433920438442, Training Error = 0.265625
Iteration no. 1830, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.46726403914484, Training Error = 0.234375
Iteration no. 1831, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50335594835252, Training Error = 0.267578125
Iteration no. 1832, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48616276575714, Training Error = 0.248046875
Iteration no. 1833, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51033526783387, Training Error = 0.26171875
Iteration no. 1834, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50798978101408, Training Error = 0.26171875
Iteration no. 1835, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.4819017255452, Training Error = 0.248046875
Iteration no. 1836, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47722833266243, Training Error = 0.234375
Iteration no. 1837, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48813688199632, Training Error = 0.2421875
Iteration no. 1838, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.45570835759407, Training Error = 0.21484375
Iteration no. 1839, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50362567014444, Training Error = 0.248046875
Iteration no. 1840, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50123432151131, Training Error = 0.236328125
Iteration no. 1841, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47794347355391, Training Error = 0.25
Iteration no. 1842, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.537261987346, Training Error = 0.2734375
Iteration no. 1843, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48543834861319, Training Error = 0.232421875
Iteration no. 1844, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49729807160337, Training Error = 0.240234375
Iteration no. 1845, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50468423478673, Training Error = 0.26953125
Iteration no. 1846, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.45060159145082, Training Error = 0.205078125
Iteration no. 1847, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.46903038917931, Training Error = 0.220703125
Iteration no. 1848, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.55446385378293, Training Error = 0.283203125
Iteration no. 1849, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52533174629771, Training Error = 0.26171875
Iteration no. 1850, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.5133796292598, Training Error = 0.275390625
Testing... average test_loss = 0.96556399324387, average test_pred_err = 0.487
Iteration no. 1851, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51609172646126, Training Error = 0.265625
Iteration no. 1852, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.5101429042954, Training Error = 0.263671875
Iteration no. 1853, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48581398403229, Training Error = 0.220703125
Iteration no. 1854, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51393837758204, Training Error = 0.27734375
Iteration no. 1855, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47351389394162, Training Error = 0.248046875
Iteration no. 1856, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48959404343046, Training Error = 0.251953125
Iteration no. 1857, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.46718896657064, Training Error = 0.212890625
Iteration no. 1858, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48083611199098, Training Error = 0.232421875
Iteration no. 1859, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49460726802749, Training Error = 0.232421875
Iteration no. 1860, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51844275975966, Training Error = 0.265625
Iteration no. 1861, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47906547406598, Training Error = 0.2421875
Iteration no. 1862, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.4966174449331, Training Error = 0.265625
Iteration no. 1863, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49711081263726, Training Error = 0.234375
Iteration no. 1864, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49918364760232, Training Error = 0.26953125
Iteration no. 1865, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52586213672964, Training Error = 0.265625
Iteration no. 1866, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50590104618686, Training Error = 0.23828125
Iteration no. 1867, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48122071401909, Training Error = 0.232421875
Iteration no. 1868, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49799438252962, Training Error = 0.25390625
Iteration no. 1869, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48673629132234, Training Error = 0.25
Iteration no. 1870, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49197259241179, Training Error = 0.248046875
Iteration no. 1871, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50759402771957, Training Error = 0.255859375
Iteration no. 1872, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47887400307652, Training Error = 0.234375
Iteration no. 1873, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47784364458725, Training Error = 0.240234375
Iteration no. 1874, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50173560072209, Training Error = 0.236328125
Iteration no. 1875, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50121351683307, Training Error = 0.265625
Iteration no. 1876, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.50180196036748, Training Error = 0.287109375
Iteration no. 1877, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47638309196016, Training Error = 0.236328125
Iteration no. 1878, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52581390816539, Training Error = 0.251953125
Iteration no. 1879, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.51358660325931, Training Error = 0.263671875
Iteration no. 1880, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.54523563667279, Training Error = 0.302734375
Iteration no. 1881, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52120464028756, Training Error = 0.259765625
Iteration no. 1882, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52946793704783, Training Error = 0.259765625
Iteration no. 1883, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.54089761044636, Training Error = 0.26953125
Iteration no. 1884, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49859835928757, Training Error = 0.271484375
Iteration no. 1885, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47861465504932, Training Error = 0.220703125
Iteration no. 1886, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.52745889318847, Training Error = 0.248046875
Iteration no. 1887, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49556656696084, Training Error = 0.2421875
Iteration no. 1888, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.5041648523761, Training Error = 0.25390625
Iteration no. 1889, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.4931636674656, Training Error = 0.240234375
Iteration no. 1890, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48937881894459, Training Error = 0.25
Iteration no. 1891, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.45382186985696, Training Error = 0.212890625
Iteration no. 1892, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49902985449654, Training Error = 0.26171875
Iteration no. 1893, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.49305183318015, Training Error = 0.244140625
Iteration no. 1894, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.4849611408508, Training Error = 0.23828125
Iteration no. 1895, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47254156643768, Training Error = 0.224609375
Iteration no. 1896, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.5076020190955, Training Error = 0.2734375
Iteration no. 1897, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48507791397827, Training Error = 0.23046875
Iteration no. 1898, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.46812744878367, Training Error = 0.234375
Iteration no. 1899, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.48716607726052, Training Error = 0.240234375
Iteration no. 1900, lr = 0.0032568271958209, attribute lr =3.2568271958209e-05, average batch_loss = 0.47906014279739, Training Error = 0.21484375
Testing... average test_loss = 0.93020279804138, average test_pred_err = 0.455
Snapshotting C_model... done
Iteration no. 1901, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.53797121729146, Training Error = 0.275390625
Iteration no. 1902, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49745019986687, Training Error = 0.26171875
Iteration no. 1903, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48580944228835, Training Error = 0.23828125
Iteration no. 1904, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4900838713824, Training Error = 0.248046875
Iteration no. 1905, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49061742998561, Training Error = 0.244140625
Iteration no. 1906, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50597494185012, Training Error = 0.267578125
Iteration no. 1907, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47669954889437, Training Error = 0.23828125
Iteration no. 1908, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52314222781628, Training Error = 0.259765625
Iteration no. 1909, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51970919276629, Training Error = 0.283203125
Iteration no. 1910, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48042317395704, Training Error = 0.220703125
Iteration no. 1911, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49584284539948, Training Error = 0.23046875
Iteration no. 1912, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49052906946731, Training Error = 0.232421875
Iteration no. 1913, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.46087830966117, Training Error = 0.23046875
Iteration no. 1914, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51914581106864, Training Error = 0.244140625
Iteration no. 1915, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49654016850774, Training Error = 0.2421875
Iteration no. 1916, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48973549237107, Training Error = 0.255859375
Iteration no. 1917, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49916910395355, Training Error = 0.259765625
Iteration no. 1918, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48719272788975, Training Error = 0.2265625
Iteration no. 1919, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50033338458824, Training Error = 0.24609375
Iteration no. 1920, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52369526148842, Training Error = 0.271484375
Iteration no. 1921, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49988988181467, Training Error = 0.265625
Iteration no. 1922, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.46215109905873, Training Error = 0.22265625
Iteration no. 1923, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.46306026723254, Training Error = 0.22265625
Iteration no. 1924, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52902346738597, Training Error = 0.251953125
Iteration no. 1925, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49847747083238, Training Error = 0.244140625
Iteration no. 1926, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49206657528764, Training Error = 0.23046875
Iteration no. 1927, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49475209311448, Training Error = 0.2421875
Iteration no. 1928, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51500989772508, Training Error = 0.265625
Iteration no. 1929, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.5331031607029, Training Error = 0.27734375
Iteration no. 1930, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4750056834994, Training Error = 0.255859375
Iteration no. 1931, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49151890718634, Training Error = 0.232421875
Iteration no. 1932, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.45831467247077, Training Error = 0.21875
Iteration no. 1933, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.53677388879622, Training Error = 0.29296875
Iteration no. 1934, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48757587660535, Training Error = 0.25
Iteration no. 1935, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52639811343838, Training Error = 0.2890625
Iteration no. 1936, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48300344920104, Training Error = 0.236328125
Iteration no. 1937, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.5033354353126, Training Error = 0.240234375
Iteration no. 1938, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47732290062574, Training Error = 0.234375
Iteration no. 1939, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50579999702752, Training Error = 0.23828125
Iteration no. 1940, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47979281625969, Training Error = 0.240234375
Iteration no. 1941, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49074510531137, Training Error = 0.248046875
Iteration no. 1942, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47343281494974, Training Error = 0.220703125
Iteration no. 1943, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48244522428458, Training Error = 0.24609375
Iteration no. 1944, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52068775392084, Training Error = 0.28125
Iteration no. 1945, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4970555744655, Training Error = 0.220703125
Iteration no. 1946, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51286846988708, Training Error = 0.26953125
Iteration no. 1947, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50190685262167, Training Error = 0.25390625
Iteration no. 1948, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51103962551077, Training Error = 0.2578125
Iteration no. 1949, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48175098855029, Training Error = 0.24609375
Iteration no. 1950, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48922137562219, Training Error = 0.26171875
Testing... average test_loss = 0.96282101952471, average test_pred_err = 0.468
Iteration no. 1951, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.5316268104378, Training Error = 0.271484375
Iteration no. 1952, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50998615480926, Training Error = 0.25
Iteration no. 1953, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4998349613448, Training Error = 0.2421875
Iteration no. 1954, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49892568526798, Training Error = 0.23046875
Iteration no. 1955, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50227468333837, Training Error = 0.251953125
Iteration no. 1956, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52524517487458, Training Error = 0.265625
Iteration no. 1957, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47409870141307, Training Error = 0.22265625
Iteration no. 1958, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.5000734133062, Training Error = 0.25390625
Iteration no. 1959, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49184173786355, Training Error = 0.23046875
Iteration no. 1960, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47789327359668, Training Error = 0.248046875
Iteration no. 1961, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47524894623096, Training Error = 0.24609375
Iteration no. 1962, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51435739639529, Training Error = 0.2578125
Iteration no. 1963, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4757032987885, Training Error = 0.236328125
Iteration no. 1964, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47807986188627, Training Error = 0.240234375
Iteration no. 1965, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48312643229366, Training Error = 0.2421875
Iteration no. 1966, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4887133882465, Training Error = 0.267578125
Iteration no. 1967, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49447517344431, Training Error = 0.23828125
Iteration no. 1968, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4716781393053, Training Error = 0.23046875
Iteration no. 1969, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4709571201318, Training Error = 0.22265625
Iteration no. 1970, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.501991567026, Training Error = 0.2421875
Iteration no. 1971, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49305749520922, Training Error = 0.2421875
Iteration no. 1972, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51803437090618, Training Error = 0.26953125
Iteration no. 1973, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52210126735469, Training Error = 0.2578125
Iteration no. 1974, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48822909385643, Training Error = 0.25390625
Iteration no. 1975, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52180381924676, Training Error = 0.267578125
Iteration no. 1976, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4880651331846, Training Error = 0.234375
Iteration no. 1977, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.51629369788248, Training Error = 0.271484375
Iteration no. 1978, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.488057571719, Training Error = 0.2265625
Iteration no. 1979, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48460059661041, Training Error = 0.216796875
Iteration no. 1980, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52670036501282, Training Error = 0.2734375
Iteration no. 1981, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49498034137277, Training Error = 0.248046875
Iteration no. 1982, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49470540368647, Training Error = 0.244140625
Iteration no. 1983, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50822507887169, Training Error = 0.2734375
Iteration no. 1984, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49157016593527, Training Error = 0.232421875
Iteration no. 1985, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49564017264702, Training Error = 0.236328125
Iteration no. 1986, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49705933758125, Training Error = 0.25
Iteration no. 1987, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.46607833138545, Training Error = 0.2109375
Iteration no. 1988, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.50510584230408, Training Error = 0.24609375
Iteration no. 1989, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49266337862652, Training Error = 0.259765625
Iteration no. 1990, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52072983210048, Training Error = 0.265625
Iteration no. 1991, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.5377200841568, Training Error = 0.28125
Iteration no. 1992, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.54701903818091, Training Error = 0.29296875
Iteration no. 1993, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47778976146612, Training Error = 0.232421875
Iteration no. 1994, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.4701926724146, Training Error = 0.220703125
Iteration no. 1995, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.49791442961188, Training Error = 0.251953125
Iteration no. 1996, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47308303549833, Training Error = 0.216796875
Iteration no. 1997, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.53552171802983, Training Error = 0.28125
Iteration no. 1998, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.52447652254737, Training Error = 0.28125
Iteration no. 1999, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.48018439267416, Training Error = 0.2421875
Iteration no. 2000, lr = 0.0022797790370746, attribute lr =2.2797790370746e-05, average batch_loss = 0.47160781648928, Training Error = 0.22265625
Testing... average test_loss = 0.94978392657567, average test_pred_err = 0.487
Snapshotting C_model... done
Iteration no. 2001, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51509852743596, Training Error = 0.25
Iteration no. 2002, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48642866617592, Training Error = 0.23828125
Iteration no. 2003, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.54691397557078, Training Error = 0.298828125
Iteration no. 2004, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48183507564562, Training Error = 0.23828125
Iteration no. 2005, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48786397773244, Training Error = 0.23828125
Iteration no. 2006, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48839251050346, Training Error = 0.240234375
Iteration no. 2007, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51923499244527, Training Error = 0.279296875
Iteration no. 2008, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47831615494597, Training Error = 0.23828125
Iteration no. 2009, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49036613119205, Training Error = 0.2421875
Iteration no. 2010, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50984395957382, Training Error = 0.26171875
Iteration no. 2011, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50927340484385, Training Error = 0.2578125
Iteration no. 2012, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52880945404926, Training Error = 0.255859375
Iteration no. 2013, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49541706503422, Training Error = 0.234375
Iteration no. 2014, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51863355595397, Training Error = 0.26171875
Iteration no. 2015, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48425663951601, Training Error = 0.25
Iteration no. 2016, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.4687833607874, Training Error = 0.244140625
Iteration no. 2017, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47737045940922, Training Error = 0.216796875
Iteration no. 2018, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49212910346638, Training Error = 0.25
Iteration no. 2019, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47413097301712, Training Error = 0.23046875
Iteration no. 2020, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52665876939092, Training Error = 0.251953125
Iteration no. 2021, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51229862832426, Training Error = 0.2734375
Iteration no. 2022, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.4820370536971, Training Error = 0.2421875
Iteration no. 2023, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51673437644664, Training Error = 0.251953125
Iteration no. 2024, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52348160191629, Training Error = 0.251953125
Iteration no. 2025, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49291166898996, Training Error = 0.2578125
Iteration no. 2026, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.46323980900929, Training Error = 0.228515625
Iteration no. 2027, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.46438454039243, Training Error = 0.216796875
Iteration no. 2028, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.46709663703113, Training Error = 0.224609375
Iteration no. 2029, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48482359619453, Training Error = 0.244140625
Iteration no. 2030, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.5375727021353, Training Error = 0.271484375
Iteration no. 2031, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49641200690249, Training Error = 0.25
Iteration no. 2032, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48137542528423, Training Error = 0.216796875
Iteration no. 2033, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51942028356448, Training Error = 0.287109375
Iteration no. 2034, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48070134758788, Training Error = 0.2265625
Iteration no. 2035, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50199300939906, Training Error = 0.24609375
Iteration no. 2036, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51329496176287, Training Error = 0.259765625
Iteration no. 2037, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52853808774764, Training Error = 0.279296875
Iteration no. 2038, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51622377796439, Training Error = 0.2578125
Iteration no. 2039, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.45180825350132, Training Error = 0.24609375
Iteration no. 2040, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50962642878979, Training Error = 0.255859375
Iteration no. 2041, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.4734049534598, Training Error = 0.220703125
Iteration no. 2042, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50678745594455, Training Error = 0.25
Iteration no. 2043, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52694215883575, Training Error = 0.27734375
Iteration no. 2044, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49973111734037, Training Error = 0.26953125
Iteration no. 2045, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50256688870872, Training Error = 0.248046875
Iteration no. 2046, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51199556387108, Training Error = 0.23828125
Iteration no. 2047, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48374813446285, Training Error = 0.255859375
Iteration no. 2048, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48593579569534, Training Error = 0.23046875
Iteration no. 2049, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49993593119335, Training Error = 0.263671875
Iteration no. 2050, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50110279288606, Training Error = 0.26171875
Testing... average test_loss = 0.90633238914608, average test_pred_err = 0.483
Iteration no. 2051, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48745862983573, Training Error = 0.255859375
Iteration no. 2052, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.4927446096166, Training Error = 0.25
Iteration no. 2053, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50638354035343, Training Error = 0.25390625
Iteration no. 2054, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.54460143304431, Training Error = 0.283203125
Iteration no. 2055, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.53574433089513, Training Error = 0.29296875
Iteration no. 2056, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51110676481564, Training Error = 0.255859375
Iteration no. 2057, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48870304873776, Training Error = 0.2421875
Iteration no. 2058, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47512538450509, Training Error = 0.244140625
Iteration no. 2059, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47817123668708, Training Error = 0.236328125
Iteration no. 2060, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.5149233662661, Training Error = 0.26171875
Iteration no. 2061, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47561514670257, Training Error = 0.228515625
Iteration no. 2062, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.53882624952197, Training Error = 0.28125
Iteration no. 2063, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48712672055332, Training Error = 0.25
Iteration no. 2064, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48970265033618, Training Error = 0.236328125
Iteration no. 2065, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.4826760224995, Training Error = 0.232421875
Iteration no. 2066, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48695834221149, Training Error = 0.234375
Iteration no. 2067, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49094801996053, Training Error = 0.240234375
Iteration no. 2068, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49264573519852, Training Error = 0.234375
Iteration no. 2069, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.54718595793577, Training Error = 0.306640625
Iteration no. 2070, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48166695723302, Training Error = 0.240234375
Iteration no. 2071, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.46949732048092, Training Error = 0.25
Iteration no. 2072, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52376933235141, Training Error = 0.2734375
Iteration no. 2073, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.46020686374507, Training Error = 0.21875
Iteration no. 2074, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51868611631451, Training Error = 0.275390625
Iteration no. 2075, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51009991780632, Training Error = 0.2578125
Iteration no. 2076, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51289958730995, Training Error = 0.25
Iteration no. 2077, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48668734149087, Training Error = 0.248046875
Iteration no. 2078, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50017344797276, Training Error = 0.248046875
Iteration no. 2079, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49778694573412, Training Error = 0.271484375
Iteration no. 2080, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47476795515844, Training Error = 0.23046875
Iteration no. 2081, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49234517893481, Training Error = 0.23828125
Iteration no. 2082, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50713389919669, Training Error = 0.267578125
Iteration no. 2083, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52304323719508, Training Error = 0.25390625
Iteration no. 2084, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47087357758185, Training Error = 0.234375
Iteration no. 2085, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.49607202064707, Training Error = 0.248046875
Iteration no. 2086, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48932815325278, Training Error = 0.244140625
Iteration no. 2087, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.53058462165864, Training Error = 0.28125
Iteration no. 2088, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.5101808025689, Training Error = 0.275390625
Iteration no. 2089, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.47189019666607, Training Error = 0.216796875
Iteration no. 2090, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.51772284975348, Training Error = 0.255859375
Iteration no. 2091, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.54708511089591, Training Error = 0.291015625
Iteration no. 2092, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48588880416087, Training Error = 0.248046875
Iteration no. 2093, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48302847745258, Training Error = 0.232421875
Iteration no. 2094, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.4927559372981, Training Error = 0.251953125
Iteration no. 2095, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.52473584605888, Training Error = 0.279296875
Iteration no. 2096, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.48662133754321, Training Error = 0.248046875
Iteration no. 2097, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.46175320950706, Training Error = 0.205078125
Iteration no. 2098, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50141772482162, Training Error = 0.251953125
Iteration no. 2099, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50302558860189, Training Error = 0.26953125
Iteration no. 2100, lr = 0.0015958453259522, attribute lr =1.5958453259522e-05, average batch_loss = 0.50962089012015, Training Error = 0.259765625
Testing... average test_loss = 0.95067614814309, average test_pred_err = 0.478
Snapshotting C_model... done
Iteration no. 2101, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48936644396413, Training Error = 0.232421875
Iteration no. 2102, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47674210192739, Training Error = 0.244140625
Iteration no. 2103, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50126405144418, Training Error = 0.248046875
Iteration no. 2104, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51510630077672, Training Error = 0.271484375
Iteration no. 2105, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5121426575478, Training Error = 0.24609375
Iteration no. 2106, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49989787263949, Training Error = 0.244140625
Iteration no. 2107, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.44384769825838, Training Error = 0.224609375
Iteration no. 2108, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.52752453387342, Training Error = 0.2734375
Iteration no. 2109, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48772012135039, Training Error = 0.220703125
Iteration no. 2110, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.52102581854058, Training Error = 0.265625
Iteration no. 2111, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50929038527802, Training Error = 0.2421875
Iteration no. 2112, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48918111231344, Training Error = 0.240234375
Iteration no. 2113, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5070391553966, Training Error = 0.248046875
Iteration no. 2114, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.52745542351717, Training Error = 0.279296875
Iteration no. 2115, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48302921477249, Training Error = 0.240234375
Iteration no. 2116, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48910280807562, Training Error = 0.234375
Iteration no. 2117, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49108060406737, Training Error = 0.240234375
Iteration no. 2118, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50168116077627, Training Error = 0.25
Iteration no. 2119, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5196644647291, Training Error = 0.275390625
Iteration no. 2120, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49119552240061, Training Error = 0.25
Iteration no. 2121, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51487760240027, Training Error = 0.2734375
Iteration no. 2122, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49742413562493, Training Error = 0.251953125
Iteration no. 2123, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50474547982433, Training Error = 0.244140625
Iteration no. 2124, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.53224234199966, Training Error = 0.279296875
Iteration no. 2125, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5002657032329, Training Error = 0.255859375
Iteration no. 2126, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48103294585606, Training Error = 0.228515625
Iteration no. 2127, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50301657347233, Training Error = 0.248046875
Iteration no. 2128, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47111975385561, Training Error = 0.208984375
Iteration no. 2129, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47952890960572, Training Error = 0.220703125
Iteration no. 2130, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49575800384192, Training Error = 0.24609375
Iteration no. 2131, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49712231860598, Training Error = 0.248046875
Iteration no. 2132, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50255744056005, Training Error = 0.240234375
Iteration no. 2133, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47550976734582, Training Error = 0.236328125
Iteration no. 2134, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48263594739683, Training Error = 0.21875
Iteration no. 2135, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49109995382732, Training Error = 0.244140625
Iteration no. 2136, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5081620418835, Training Error = 0.236328125
Iteration no. 2137, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51614429156308, Training Error = 0.248046875
Iteration no. 2138, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49092660925833, Training Error = 0.248046875
Iteration no. 2139, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48953643194233, Training Error = 0.240234375
Iteration no. 2140, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47499987713018, Training Error = 0.244140625
Iteration no. 2141, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51890364785174, Training Error = 0.265625
Iteration no. 2142, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48007375084595, Training Error = 0.234375
Iteration no. 2143, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47655182817944, Training Error = 0.232421875
Iteration no. 2144, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.53524451216647, Training Error = 0.298828125
Iteration no. 2145, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49085383189594, Training Error = 0.232421875
Iteration no. 2146, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49962573363271, Training Error = 0.248046875
Iteration no. 2147, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51578301086738, Training Error = 0.2578125
Iteration no. 2148, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.55139081690567, Training Error = 0.306640625
Iteration no. 2149, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51864372941361, Training Error = 0.26953125
Iteration no. 2150, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51937390162588, Training Error = 0.26171875
Testing... average test_loss = 0.97765931690655, average test_pred_err = 0.487
Iteration no. 2151, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47216002998164, Training Error = 0.228515625
Iteration no. 2152, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49496376470856, Training Error = 0.26953125
Iteration no. 2153, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47089703959687, Training Error = 0.228515625
Iteration no. 2154, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49414070499136, Training Error = 0.23046875
Iteration no. 2155, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.54985173182409, Training Error = 0.30078125
Iteration no. 2156, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47470651213252, Training Error = 0.23046875
Iteration no. 2157, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.55611749276291, Training Error = 0.296875
Iteration no. 2158, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50265269240604, Training Error = 0.265625
Iteration no. 2159, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49541971990558, Training Error = 0.2421875
Iteration no. 2160, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.46565263536613, Training Error = 0.21875
Iteration no. 2161, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49664257104932, Training Error = 0.25390625
Iteration no. 2162, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51661442863533, Training Error = 0.265625
Iteration no. 2163, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50977685824352, Training Error = 0.263671875
Iteration no. 2164, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49051642852308, Training Error = 0.234375
Iteration no. 2165, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50795361337921, Training Error = 0.236328125
Iteration no. 2166, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49747019153798, Training Error = 0.24609375
Iteration no. 2167, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.53445389722472, Training Error = 0.28125
Iteration no. 2168, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50315124611699, Training Error = 0.251953125
Iteration no. 2169, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50578996576339, Training Error = 0.240234375
Iteration no. 2170, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51319715458407, Training Error = 0.25
Iteration no. 2171, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50373288803605, Training Error = 0.2578125
Iteration no. 2172, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47982180988146, Training Error = 0.220703125
Iteration no. 2173, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.46467099207266, Training Error = 0.208984375
Iteration no. 2174, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50011042084655, Training Error = 0.244140625
Iteration no. 2175, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49171106847524, Training Error = 0.251953125
Iteration no. 2176, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.53962350477339, Training Error = 0.275390625
Iteration no. 2177, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48436324288142, Training Error = 0.25
Iteration no. 2178, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5088161327557, Training Error = 0.23828125
Iteration no. 2179, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49308299628275, Training Error = 0.263671875
Iteration no. 2180, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48538889029993, Training Error = 0.23046875
Iteration no. 2181, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48290295835556, Training Error = 0.240234375
Iteration no. 2182, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50509640034984, Training Error = 0.25390625
Iteration no. 2183, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48614857909245, Training Error = 0.2265625
Iteration no. 2184, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48395293895793, Training Error = 0.2265625
Iteration no. 2185, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49766978921846, Training Error = 0.2578125
Iteration no. 2186, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49483442935071, Training Error = 0.259765625
Iteration no. 2187, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50536399033844, Training Error = 0.263671875
Iteration no. 2188, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49350575508056, Training Error = 0.25
Iteration no. 2189, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.49663922504966, Training Error = 0.251953125
Iteration no. 2190, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51917671216104, Training Error = 0.26171875
Iteration no. 2191, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.46163304164301, Training Error = 0.21875
Iteration no. 2192, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51119982381573, Training Error = 0.267578125
Iteration no. 2193, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.51191500937162, Training Error = 0.267578125
Iteration no. 2194, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.45691490136179, Training Error = 0.19921875
Iteration no. 2195, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.47248194311741, Training Error = 0.25
Iteration no. 2196, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.4722640843963, Training Error = 0.228515625
Iteration no. 2197, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.50061486755926, Training Error = 0.244140625
Iteration no. 2198, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.46772746578416, Training Error = 0.23046875
Iteration no. 2199, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.5149120595584, Training Error = 0.26953125
Iteration no. 2200, lr = 0.0011170917281666, attribute lr =1.1170917281666e-05, average batch_loss = 0.48689882681596, Training Error = 0.244140625
Testing... average test_loss = 0.91483001084442, average test_pred_err = 0.462
Snapshotting C_model... done
Iteration no. 2201, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47551012414887, Training Error = 0.21875
Iteration no. 2202, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52553498128176, Training Error = 0.271484375
Iteration no. 2203, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48461143314363, Training Error = 0.23828125
Iteration no. 2204, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.53750701935302, Training Error = 0.287109375
Iteration no. 2205, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50263065724106, Training Error = 0.259765625
Iteration no. 2206, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49124059884961, Training Error = 0.244140625
Iteration no. 2207, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49143255494205, Training Error = 0.25390625
Iteration no. 2208, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50373270614397, Training Error = 0.251953125
Iteration no. 2209, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.45779132097859, Training Error = 0.216796875
Iteration no. 2210, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52216464280206, Training Error = 0.27734375
Iteration no. 2211, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.4808463321149, Training Error = 0.23828125
Iteration no. 2212, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.46898910868568, Training Error = 0.212890625
Iteration no. 2213, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47169179970333, Training Error = 0.21484375
Iteration no. 2214, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.4784291112268, Training Error = 0.228515625
Iteration no. 2215, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.53638713324085, Training Error = 0.267578125
Iteration no. 2216, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50854878847352, Training Error = 0.2734375
Iteration no. 2217, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51798556333432, Training Error = 0.271484375
Iteration no. 2218, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.46344171189075, Training Error = 0.212890625
Iteration no. 2219, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52245617309838, Training Error = 0.26953125
Iteration no. 2220, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52554074734349, Training Error = 0.287109375
Iteration no. 2221, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51929104239956, Training Error = 0.279296875
Iteration no. 2222, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48002264700762, Training Error = 0.234375
Iteration no. 2223, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50753766397667, Training Error = 0.259765625
Iteration no. 2224, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.5000733588029, Training Error = 0.26171875
Iteration no. 2225, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.53817297927372, Training Error = 0.28125
Iteration no. 2226, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52636522068171, Training Error = 0.248046875
Iteration no. 2227, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51734297281591, Training Error = 0.275390625
Iteration no. 2228, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48312698207721, Training Error = 0.234375
Iteration no. 2229, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47217214168259, Training Error = 0.212890625
Iteration no. 2230, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.46406910409401, Training Error = 0.232421875
Iteration no. 2231, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51147260275128, Training Error = 0.2734375
Iteration no. 2232, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49482253392289, Training Error = 0.244140625
Iteration no. 2233, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.53815627969375, Training Error = 0.271484375
Iteration no. 2234, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52428898670262, Training Error = 0.2734375
Iteration no. 2235, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.53511062398305, Training Error = 0.263671875
Iteration no. 2236, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.4959233551965, Training Error = 0.236328125
Iteration no. 2237, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.46533004050034, Training Error = 0.216796875
Iteration no. 2238, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50747990778801, Training Error = 0.236328125
Iteration no. 2239, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49520006167552, Training Error = 0.2421875
Iteration no. 2240, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.53846716443963, Training Error = 0.283203125
Iteration no. 2241, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48429822958439, Training Error = 0.248046875
Iteration no. 2242, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.46629328960636, Training Error = 0.22265625
Iteration no. 2243, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49964694566444, Training Error = 0.244140625
Iteration no. 2244, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50017229320631, Training Error = 0.25
Iteration no. 2245, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50120743234588, Training Error = 0.25390625
Iteration no. 2246, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48696000487931, Training Error = 0.251953125
Iteration no. 2247, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48579658054907, Training Error = 0.25390625
Iteration no. 2248, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.46860708902109, Training Error = 0.2109375
Iteration no. 2249, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49468997549504, Training Error = 0.232421875
Iteration no. 2250, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51423431561377, Training Error = 0.265625
Testing... average test_loss = 0.94523476510376, average test_pred_err = 0.473
Iteration no. 2251, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48264263326179, Training Error = 0.234375
Iteration no. 2252, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49526888616898, Training Error = 0.201171875
Iteration no. 2253, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50479518947364, Training Error = 0.265625
Iteration no. 2254, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.5311637263394, Training Error = 0.279296875
Iteration no. 2255, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.4899049696772, Training Error = 0.240234375
Iteration no. 2256, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.5182177048782, Training Error = 0.263671875
Iteration no. 2257, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47053240197166, Training Error = 0.236328125
Iteration no. 2258, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.4727605506389, Training Error = 0.23828125
Iteration no. 2259, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49179545961629, Training Error = 0.255859375
Iteration no. 2260, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49746383451485, Training Error = 0.234375
Iteration no. 2261, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.45228890221982, Training Error = 0.205078125
Iteration no. 2262, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49712596452199, Training Error = 0.2578125
Iteration no. 2263, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49357293773181, Training Error = 0.2109375
Iteration no. 2264, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51001496056616, Training Error = 0.26171875
Iteration no. 2265, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52882546114558, Training Error = 0.26953125
Iteration no. 2266, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52397322923917, Training Error = 0.259765625
Iteration no. 2267, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51060302454388, Training Error = 0.2734375
Iteration no. 2268, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50720027097635, Training Error = 0.23046875
Iteration no. 2269, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49234129308123, Training Error = 0.2421875
Iteration no. 2270, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50185646984855, Training Error = 0.2578125
Iteration no. 2271, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51217173313121, Training Error = 0.25
Iteration no. 2272, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47342479216709, Training Error = 0.23828125
Iteration no. 2273, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47361479460946, Training Error = 0.234375
Iteration no. 2274, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49123388642654, Training Error = 0.22265625
Iteration no. 2275, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50054958751562, Training Error = 0.251953125
Iteration no. 2276, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50496302864914, Training Error = 0.267578125
Iteration no. 2277, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52138281785057, Training Error = 0.248046875
Iteration no. 2278, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.4777496112872, Training Error = 0.23828125
Iteration no. 2279, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49322070950409, Training Error = 0.2578125
Iteration no. 2280, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49809966464533, Training Error = 0.234375
Iteration no. 2281, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.54049484076964, Training Error = 0.28125
Iteration no. 2282, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47829301170258, Training Error = 0.232421875
Iteration no. 2283, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.47762512023915, Training Error = 0.224609375
Iteration no. 2284, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48548035609532, Training Error = 0.234375
Iteration no. 2285, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52152838715215, Training Error = 0.251953125
Iteration no. 2286, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51316935882749, Training Error = 0.283203125
Iteration no. 2287, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50019543377684, Training Error = 0.263671875
Iteration no. 2288, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50418587407371, Training Error = 0.244140625
Iteration no. 2289, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52545502834152, Training Error = 0.25390625
Iteration no. 2290, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48630671055503, Training Error = 0.216796875
Iteration no. 2291, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48464251589865, Training Error = 0.2265625
Iteration no. 2292, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48539736051505, Training Error = 0.251953125
Iteration no. 2293, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.54531419146422, Training Error = 0.287109375
Iteration no. 2294, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51107255714007, Training Error = 0.27734375
Iteration no. 2295, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.50101388146724, Training Error = 0.267578125
Iteration no. 2296, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.49532099602605, Training Error = 0.248046875
Iteration no. 2297, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.52828368876304, Training Error = 0.263671875
Iteration no. 2298, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48731833555068, Training Error = 0.234375
Iteration no. 2299, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.48922100560089, Training Error = 0.248046875
Iteration no. 2300, lr = 0.0007819642097166, attribute lr =7.819642097166e-06, average batch_loss = 0.51083119989131, Training Error = 0.23828125
Testing... average test_loss = 0.97428476580813, average test_pred_err = 0.485
Snapshotting C_model... done
Iteration no. 2301, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.52667283670158, Training Error = 0.267578125
Iteration no. 2302, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49645272729201, Training Error = 0.2578125
Iteration no. 2303, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49670033900567, Training Error = 0.251953125
Iteration no. 2304, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49810480062683, Training Error = 0.263671875
Iteration no. 2305, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.52511231366166, Training Error = 0.275390625
Iteration no. 2306, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49909676216385, Training Error = 0.24609375
Iteration no. 2307, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48751351807932, Training Error = 0.25
Iteration no. 2308, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49471279497982, Training Error = 0.22265625
Iteration no. 2309, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.4823695588564, Training Error = 0.240234375
Iteration no. 2310, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48268305210803, Training Error = 0.22265625
Iteration no. 2311, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51086455536852, Training Error = 0.267578125
Iteration no. 2312, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50223688862588, Training Error = 0.267578125
Iteration no. 2313, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49699863270093, Training Error = 0.259765625
Iteration no. 2314, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51633770085552, Training Error = 0.248046875
Iteration no. 2315, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48911365398826, Training Error = 0.244140625
Iteration no. 2316, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49250872076071, Training Error = 0.2421875
Iteration no. 2317, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49035393917759, Training Error = 0.240234375
Iteration no. 2318, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51161916116348, Training Error = 0.251953125
Iteration no. 2319, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51110640367219, Training Error = 0.251953125
Iteration no. 2320, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.45080132717694, Training Error = 0.21484375
Iteration no. 2321, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.4899547666574, Training Error = 0.236328125
Iteration no. 2322, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50121207591711, Training Error = 0.240234375
Iteration no. 2323, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50920915530155, Training Error = 0.265625
Iteration no. 2324, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49519325957539, Training Error = 0.26171875
Iteration no. 2325, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.5255939886505, Training Error = 0.259765625
Iteration no. 2326, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.53190617076347, Training Error = 0.306640625
Iteration no. 2327, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.53022065815286, Training Error = 0.279296875
Iteration no. 2328, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.46106805853678, Training Error = 0.228515625
Iteration no. 2329, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49739473676207, Training Error = 0.255859375
Iteration no. 2330, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50485625389773, Training Error = 0.265625
Iteration no. 2331, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.52220407987975, Training Error = 0.25390625
Iteration no. 2332, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48843713833913, Training Error = 0.234375
Iteration no. 2333, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48398457409016, Training Error = 0.220703125
Iteration no. 2334, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49499350027006, Training Error = 0.232421875
Iteration no. 2335, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49536978352391, Training Error = 0.263671875
Iteration no. 2336, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51122518335745, Training Error = 0.240234375
Iteration no. 2337, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47953354444055, Training Error = 0.23046875
Iteration no. 2338, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48808394914664, Training Error = 0.2421875
Iteration no. 2339, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50952653168199, Training Error = 0.25
Iteration no. 2340, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48956190437958, Training Error = 0.2421875
Iteration no. 2341, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48280213674786, Training Error = 0.234375
Iteration no. 2342, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.53456649175209, Training Error = 0.2890625
Iteration no. 2343, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51107640177788, Training Error = 0.251953125
Iteration no. 2344, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51995780463783, Training Error = 0.251953125
Iteration no. 2345, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47112872500321, Training Error = 0.2421875
Iteration no. 2346, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50375193139644, Training Error = 0.2578125
Iteration no. 2347, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.5378673999879, Training Error = 0.27734375
Iteration no. 2348, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.54957653717834, Training Error = 0.279296875
Iteration no. 2349, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50655694704251, Training Error = 0.25390625
Iteration no. 2350, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50552217070289, Training Error = 0.248046875
Testing... average test_loss = 0.94647119749057, average test_pred_err = 0.471
Iteration no. 2351, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47095011999082, Training Error = 0.25390625
Iteration no. 2352, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.4997735950698, Training Error = 0.240234375
Iteration no. 2353, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.53342034457649, Training Error = 0.26953125
Iteration no. 2354, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.4995700129239, Training Error = 0.251953125
Iteration no. 2355, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49800328851749, Training Error = 0.26953125
Iteration no. 2356, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49235470940257, Training Error = 0.244140625
Iteration no. 2357, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.46866810990374, Training Error = 0.23046875
Iteration no. 2358, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.5454823626834, Training Error = 0.275390625
Iteration no. 2359, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.5066506515487, Training Error = 0.244140625
Iteration no. 2360, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.52594994601071, Training Error = 0.2734375
Iteration no. 2361, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48417600778775, Training Error = 0.244140625
Iteration no. 2362, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49788619136371, Training Error = 0.26171875
Iteration no. 2363, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50058112288933, Training Error = 0.25390625
Iteration no. 2364, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49570654957692, Training Error = 0.255859375
Iteration no. 2365, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51710363605798, Training Error = 0.275390625
Iteration no. 2366, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47217311666311, Training Error = 0.2265625
Iteration no. 2367, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49418917554425, Training Error = 0.23828125
Iteration no. 2368, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49325975713476, Training Error = 0.240234375
Iteration no. 2369, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51609273916169, Training Error = 0.26171875
Iteration no. 2370, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48125429372271, Training Error = 0.2421875
Iteration no. 2371, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51154414745679, Training Error = 0.271484375
Iteration no. 2372, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50881347325008, Training Error = 0.2421875
Iteration no. 2373, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47469145359312, Training Error = 0.23046875
Iteration no. 2374, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50747062404695, Training Error = 0.271484375
Iteration no. 2375, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51945790350755, Training Error = 0.27734375
Iteration no. 2376, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.4673260345809, Training Error = 0.212890625
Iteration no. 2377, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47808625119433, Training Error = 0.212890625
Iteration no. 2378, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51441565493758, Training Error = 0.275390625
Iteration no. 2379, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49020253170116, Training Error = 0.240234375
Iteration no. 2380, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50985484401669, Training Error = 0.2421875
Iteration no. 2381, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51852664914769, Training Error = 0.267578125
Iteration no. 2382, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48784113875481, Training Error = 0.228515625
Iteration no. 2383, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.53911462560471, Training Error = 0.267578125
Iteration no. 2384, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.5038271074966, Training Error = 0.248046875
Iteration no. 2385, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51717329620002, Training Error = 0.2734375
Iteration no. 2386, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49687090162539, Training Error = 0.25
Iteration no. 2387, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51545719030298, Training Error = 0.259765625
Iteration no. 2388, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47727418423586, Training Error = 0.23828125
Iteration no. 2389, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47530936836812, Training Error = 0.224609375
Iteration no. 2390, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.494376548963, Training Error = 0.2578125
Iteration no. 2391, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.47723601097918, Training Error = 0.240234375
Iteration no. 2392, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.52536725755417, Training Error = 0.263671875
Iteration no. 2393, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48706968986413, Training Error = 0.232421875
Iteration no. 2394, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49313464889032, Training Error = 0.255859375
Iteration no. 2395, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.52022945653827, Training Error = 0.263671875
Iteration no. 2396, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51391844446883, Training Error = 0.26953125
Iteration no. 2397, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.49060285524773, Training Error = 0.236328125
Iteration no. 2398, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.50889829522878, Training Error = 0.267578125
Iteration no. 2399, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.48839589312747, Training Error = 0.2578125
Iteration no. 2400, lr = 0.00054737494680162, attribute lr =5.4737494680162e-06, average batch_loss = 0.51052761331926, Training Error = 0.265625
Testing... average test_loss = 0.9478080561636, average test_pred_err = 0.475
Snapshotting C_model... done
Iteration no. 2401, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50791314664141, Training Error = 0.263671875
Iteration no. 2402, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47937063092567, Training Error = 0.240234375
Iteration no. 2403, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47487954893525, Training Error = 0.2109375
Iteration no. 2404, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51338625192967, Training Error = 0.263671875
Iteration no. 2405, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51959995310062, Training Error = 0.275390625
Iteration no. 2406, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51224333545809, Training Error = 0.236328125
Iteration no. 2407, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.53781586880157, Training Error = 0.267578125
Iteration no. 2408, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.46112909585733, Training Error = 0.2109375
Iteration no. 2409, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49640608721187, Training Error = 0.251953125
Iteration no. 2410, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50486308450144, Training Error = 0.25390625
Iteration no. 2411, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51549287014648, Training Error = 0.251953125
Iteration no. 2412, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50997433148935, Training Error = 0.248046875
Iteration no. 2413, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51814048163908, Training Error = 0.263671875
Iteration no. 2414, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49200904119247, Training Error = 0.244140625
Iteration no. 2415, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50392626306175, Training Error = 0.251953125
Iteration no. 2416, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48495279660101, Training Error = 0.251953125
Iteration no. 2417, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48803891947134, Training Error = 0.220703125
Iteration no. 2418, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47927590071815, Training Error = 0.240234375
Iteration no. 2419, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50507902737428, Training Error = 0.265625
Iteration no. 2420, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.53560503584446, Training Error = 0.29296875
Iteration no. 2421, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52169823624671, Training Error = 0.28515625
Iteration no. 2422, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52385449682417, Training Error = 0.255859375
Iteration no. 2423, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.46934641532391, Training Error = 0.220703125
Iteration no. 2424, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49566666810133, Training Error = 0.251953125
Iteration no. 2425, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51060805011299, Training Error = 0.2578125
Iteration no. 2426, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48204887143001, Training Error = 0.251953125
Iteration no. 2427, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5064229246304, Training Error = 0.25390625
Iteration no. 2428, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51871732226121, Training Error = 0.26171875
Iteration no. 2429, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51749347003661, Training Error = 0.2734375
Iteration no. 2430, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52598230529119, Training Error = 0.27734375
Iteration no. 2431, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49146957678294, Training Error = 0.23046875
Iteration no. 2432, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51153466412142, Training Error = 0.25390625
Iteration no. 2433, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50892771798492, Training Error = 0.2578125
Iteration no. 2434, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47876113376261, Training Error = 0.25
Iteration no. 2435, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50923628818383, Training Error = 0.251953125
Iteration no. 2436, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51626472269646, Training Error = 0.271484375
Iteration no. 2437, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49875278301392, Training Error = 0.25
Iteration no. 2438, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51409506248136, Training Error = 0.275390625
Iteration no. 2439, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48784322682302, Training Error = 0.240234375
Iteration no. 2440, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49662265653466, Training Error = 0.2421875
Iteration no. 2441, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48644266008842, Training Error = 0.24609375
Iteration no. 2442, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51158784313462, Training Error = 0.2578125
Iteration no. 2443, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51781879095574, Training Error = 0.26953125
Iteration no. 2444, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50689107658314, Training Error = 0.28125
Iteration no. 2445, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.541821339619, Training Error = 0.28125
Iteration no. 2446, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.4786133065397, Training Error = 0.216796875
Iteration no. 2447, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52307151907067, Training Error = 0.24609375
Iteration no. 2448, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52400309352222, Training Error = 0.275390625
Iteration no. 2449, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5110018326792, Training Error = 0.25
Iteration no. 2450, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50079177651699, Training Error = 0.240234375
Testing... average test_loss = 0.92842182554744, average test_pred_err = 0.468
Iteration no. 2451, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.4780151722708, Training Error = 0.25390625
Iteration no. 2452, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.54623330412054, Training Error = 0.283203125
Iteration no. 2453, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.45336401882353, Training Error = 0.205078125
Iteration no. 2454, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49035839293436, Training Error = 0.240234375
Iteration no. 2455, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.55952689832578, Training Error = 0.29296875
Iteration no. 2456, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48385579507815, Training Error = 0.248046875
Iteration no. 2457, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50617598192938, Training Error = 0.251953125
Iteration no. 2458, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47477081883007, Training Error = 0.224609375
Iteration no. 2459, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.46092624535551, Training Error = 0.2265625
Iteration no. 2460, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50890351440554, Training Error = 0.26953125
Iteration no. 2461, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5309192614644, Training Error = 0.26953125
Iteration no. 2462, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52429874961569, Training Error = 0.28125
Iteration no. 2463, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.54532822105986, Training Error = 0.298828125
Iteration no. 2464, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.46881287523193, Training Error = 0.21875
Iteration no. 2465, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49407861772238, Training Error = 0.232421875
Iteration no. 2466, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48450674323501, Training Error = 0.244140625
Iteration no. 2467, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48919147732599, Training Error = 0.236328125
Iteration no. 2468, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52156740817036, Training Error = 0.28125
Iteration no. 2469, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48219346096675, Training Error = 0.236328125
Iteration no. 2470, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47888193375068, Training Error = 0.251953125
Iteration no. 2471, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47304579116586, Training Error = 0.228515625
Iteration no. 2472, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50088362942573, Training Error = 0.236328125
Iteration no. 2473, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51576530492955, Training Error = 0.267578125
Iteration no. 2474, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48965536736259, Training Error = 0.248046875
Iteration no. 2475, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.45786789750751, Training Error = 0.23828125
Iteration no. 2476, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.51265565453194, Training Error = 0.275390625
Iteration no. 2477, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49776060747462, Training Error = 0.259765625
Iteration no. 2478, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5123580360109, Training Error = 0.251953125
Iteration no. 2479, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49575740692004, Training Error = 0.24609375
Iteration no. 2480, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5210877934623, Training Error = 0.2890625
Iteration no. 2481, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.4691344769934, Training Error = 0.232421875
Iteration no. 2482, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52292041902567, Training Error = 0.26953125
Iteration no. 2483, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50877008360422, Training Error = 0.275390625
Iteration no. 2484, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50451424257152, Training Error = 0.2421875
Iteration no. 2485, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49065692528528, Training Error = 0.24609375
Iteration no. 2486, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48221900824324, Training Error = 0.228515625
Iteration no. 2487, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50055253562675, Training Error = 0.259765625
Iteration no. 2488, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49496188870882, Training Error = 0.265625
Iteration no. 2489, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.43475665543273, Training Error = 0.205078125
Iteration no. 2490, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.53421270892935, Training Error = 0.2734375
Iteration no. 2491, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.48901168440535, Training Error = 0.24609375
Iteration no. 2492, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.49279184159197, Training Error = 0.2578125
Iteration no. 2493, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.53726166760102, Training Error = 0.287109375
Iteration no. 2494, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5119403503931, Training Error = 0.25390625
Iteration no. 2495, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.47276677022625, Training Error = 0.228515625
Iteration no. 2496, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52841462107147, Training Error = 0.283203125
Iteration no. 2497, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.52714863979247, Training Error = 0.271484375
Iteration no. 2498, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.55179889052927, Training Error = 0.29296875
Iteration no. 2499, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.5163957607724, Training Error = 0.267578125
Iteration no. 2500, lr = 0.00038316246276113, attribute lr =3.8316246276113e-06, average batch_loss = 0.50041378598505, Training Error = 0.25390625
Testing... average test_loss = 0.94256436341449, average test_pred_err = 0.465
Snapshotting C_model... done
Iteration no. 2501, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.52039641951355, Training Error = 0.279296875
Iteration no. 2502, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.47561018686973, Training Error = 0.216796875
Iteration no. 2503, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48769414131572, Training Error = 0.240234375
Iteration no. 2504, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48536325569732, Training Error = 0.232421875
Iteration no. 2505, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51994460669627, Training Error = 0.271484375
Iteration no. 2506, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50515118898252, Training Error = 0.275390625
Iteration no. 2507, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.4834599526265, Training Error = 0.236328125
Iteration no. 2508, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.52356420132061, Training Error = 0.259765625
Iteration no. 2509, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48820409338531, Training Error = 0.2265625
Iteration no. 2510, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49769578278645, Training Error = 0.26953125
Iteration no. 2511, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49170577904204, Training Error = 0.23828125
Iteration no. 2512, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48478060317209, Training Error = 0.244140625
Iteration no. 2513, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50012486638706, Training Error = 0.234375
Iteration no. 2514, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.47338427354423, Training Error = 0.23046875
Iteration no. 2515, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.45681609408762, Training Error = 0.2109375
Iteration no. 2516, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49870625541219, Training Error = 0.232421875
Iteration no. 2517, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51341938060047, Training Error = 0.251953125
Iteration no. 2518, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50154466839676, Training Error = 0.267578125
Iteration no. 2519, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.54091934199424, Training Error = 0.263671875
Iteration no. 2520, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49035142360266, Training Error = 0.23046875
Iteration no. 2521, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.55813318256254, Training Error = 0.3046875
Iteration no. 2522, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50561875200619, Training Error = 0.2578125
Iteration no. 2523, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.54268023433806, Training Error = 0.28515625
Iteration no. 2524, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.47619099268329, Training Error = 0.25
Iteration no. 2525, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50167236382808, Training Error = 0.24609375
Iteration no. 2526, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.4942712772664, Training Error = 0.24609375
Iteration no. 2527, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50820665275794, Training Error = 0.27734375
Iteration no. 2528, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50813390358838, Training Error = 0.265625
Iteration no. 2529, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.46788785869391, Training Error = 0.201171875
Iteration no. 2530, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.52959628356494, Training Error = 0.259765625
Iteration no. 2531, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.5198186730818, Training Error = 0.267578125
Iteration no. 2532, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51242407857345, Training Error = 0.24609375
Iteration no. 2533, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.45784237449767, Training Error = 0.228515625
Iteration no. 2534, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50222106073635, Training Error = 0.271484375
Iteration no. 2535, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.5104569251878, Training Error = 0.263671875
Iteration no. 2536, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48981404962697, Training Error = 0.255859375
Iteration no. 2537, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51271669584855, Training Error = 0.263671875
Iteration no. 2538, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49195271177774, Training Error = 0.2421875
Iteration no. 2539, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49537546141122, Training Error = 0.248046875
Iteration no. 2540, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50275821338072, Training Error = 0.244140625
Iteration no. 2541, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49400815424538, Training Error = 0.234375
Iteration no. 2542, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50340626140576, Training Error = 0.24609375
Iteration no. 2543, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50046481133407, Training Error = 0.232421875
Iteration no. 2544, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48661831847886, Training Error = 0.232421875
Iteration no. 2545, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51614086368927, Training Error = 0.265625
Iteration no. 2546, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48865317207723, Training Error = 0.23828125
Iteration no. 2547, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49201471243885, Training Error = 0.236328125
Iteration no. 2548, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50255832911351, Training Error = 0.25
Iteration no. 2549, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.55526035071924, Training Error = 0.2890625
Iteration no. 2550, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48824002664182, Training Error = 0.25390625
Testing... average test_loss = 0.98511592706612, average test_pred_err = 0.501
Iteration no. 2551, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.52403766963099, Training Error = 0.267578125
Iteration no. 2552, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49695877575971, Training Error = 0.236328125
Iteration no. 2553, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50106649249701, Training Error = 0.248046875
Iteration no. 2554, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49423954057838, Training Error = 0.251953125
Iteration no. 2555, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49994773486151, Training Error = 0.248046875
Iteration no. 2556, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.45425267612931, Training Error = 0.208984375
Iteration no. 2557, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.52002279306243, Training Error = 0.283203125
Iteration no. 2558, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50098454940638, Training Error = 0.255859375
Iteration no. 2559, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49421463031963, Training Error = 0.248046875
Iteration no. 2560, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.5177559118688, Training Error = 0.2734375
Iteration no. 2561, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.47377202271819, Training Error = 0.236328125
Iteration no. 2562, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49762084503599, Training Error = 0.26171875
Iteration no. 2563, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50815074063579, Training Error = 0.248046875
Iteration no. 2564, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50464919851583, Training Error = 0.2578125
Iteration no. 2565, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.45511935255716, Training Error = 0.189453125
Iteration no. 2566, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.54241455175693, Training Error = 0.296875
Iteration no. 2567, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.46344318542666, Training Error = 0.2109375
Iteration no. 2568, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50643684161566, Training Error = 0.248046875
Iteration no. 2569, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51326221934538, Training Error = 0.26171875
Iteration no. 2570, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.5036321362547, Training Error = 0.259765625
Iteration no. 2571, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.47686352979677, Training Error = 0.23046875
Iteration no. 2572, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.54400410115896, Training Error = 0.306640625
Iteration no. 2573, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50605736804741, Training Error = 0.248046875
Iteration no. 2574, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51428962985447, Training Error = 0.28125
Iteration no. 2575, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48436521428249, Training Error = 0.24609375
Iteration no. 2576, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51344761746267, Training Error = 0.259765625
Iteration no. 2577, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.4660287502192, Training Error = 0.236328125
Iteration no. 2578, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.5247759227973, Training Error = 0.267578125
Iteration no. 2579, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48552498496734, Training Error = 0.265625
Iteration no. 2580, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50658175101189, Training Error = 0.251953125
Iteration no. 2581, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51940049107268, Training Error = 0.279296875
Iteration no. 2582, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.46027484023718, Training Error = 0.220703125
Iteration no. 2583, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51183714843922, Training Error = 0.26171875
Iteration no. 2584, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49603727743745, Training Error = 0.255859375
Iteration no. 2585, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50508074693024, Training Error = 0.255859375
Iteration no. 2586, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48383099040561, Training Error = 0.248046875
Iteration no. 2587, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50848940502983, Training Error = 0.244140625
Iteration no. 2588, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51741008538576, Training Error = 0.267578125
Iteration no. 2589, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49815823897026, Training Error = 0.263671875
Iteration no. 2590, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.49204281405565, Training Error = 0.2421875
Iteration no. 2591, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.4986699431519, Training Error = 0.263671875
Iteration no. 2592, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50149034501403, Training Error = 0.23828125
Iteration no. 2593, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48378298789734, Training Error = 0.23828125
Iteration no. 2594, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.47881305106534, Training Error = 0.2265625
Iteration no. 2595, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.53660848928343, Training Error = 0.26953125
Iteration no. 2596, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.53282218494941, Training Error = 0.28125
Iteration no. 2597, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.51489008765822, Training Error = 0.263671875
Iteration no. 2598, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.50616061750882, Training Error = 0.251953125
Iteration no. 2599, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48495690859965, Training Error = 0.240234375
Iteration no. 2600, lr = 0.00026821372393279, attribute lr =2.6821372393279e-06, average batch_loss = 0.48992428736293, Training Error = 0.251953125
Testing... average test_loss = 0.95359738018954, average test_pred_err = 0.483
Snapshotting C_model... done
Iteration no. 2601, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50124025526487, Training Error = 0.2578125
Iteration no. 2602, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.46032541727302, Training Error = 0.21875
Iteration no. 2603, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52376280755921, Training Error = 0.275390625
Iteration no. 2604, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52435063154651, Training Error = 0.27734375
Iteration no. 2605, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.483359786438, Training Error = 0.24609375
Iteration no. 2606, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.55804900107919, Training Error = 0.310546875
Iteration no. 2607, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49821907008417, Training Error = 0.248046875
Iteration no. 2608, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50090774944362, Training Error = 0.267578125
Iteration no. 2609, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48597992300895, Training Error = 0.248046875
Iteration no. 2610, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50693388348956, Training Error = 0.2578125
Iteration no. 2611, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51652915391177, Training Error = 0.255859375
Iteration no. 2612, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.53974852515455, Training Error = 0.26171875
Iteration no. 2613, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50164813285424, Training Error = 0.251953125
Iteration no. 2614, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51919388922612, Training Error = 0.26953125
Iteration no. 2615, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51568532129895, Training Error = 0.251953125
Iteration no. 2616, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50134922164358, Training Error = 0.234375
Iteration no. 2617, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52813648639902, Training Error = 0.28125
Iteration no. 2618, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51284080136138, Training Error = 0.26953125
Iteration no. 2619, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.53134385813759, Training Error = 0.25
Iteration no. 2620, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47875076267975, Training Error = 0.216796875
Iteration no. 2621, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50351951815032, Training Error = 0.2578125
Iteration no. 2622, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50787508859192, Training Error = 0.26171875
Iteration no. 2623, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51258159390179, Training Error = 0.251953125
Iteration no. 2624, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51092367797209, Training Error = 0.263671875
Iteration no. 2625, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50232702789109, Training Error = 0.23828125
Iteration no. 2626, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47186771890827, Training Error = 0.232421875
Iteration no. 2627, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50554067658326, Training Error = 0.24609375
Iteration no. 2628, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51380825065367, Training Error = 0.267578125
Iteration no. 2629, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48773448194734, Training Error = 0.267578125
Iteration no. 2630, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50086445841191, Training Error = 0.26171875
Iteration no. 2631, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48331168539947, Training Error = 0.228515625
Iteration no. 2632, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.5340307606474, Training Error = 0.2578125
Iteration no. 2633, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52706828133163, Training Error = 0.25
Iteration no. 2634, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49120987224978, Training Error = 0.220703125
Iteration no. 2635, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52979421222219, Training Error = 0.263671875
Iteration no. 2636, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52428553547138, Training Error = 0.283203125
Iteration no. 2637, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48254652008516, Training Error = 0.2421875
Iteration no. 2638, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50145858380815, Training Error = 0.259765625
Iteration no. 2639, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52874197847377, Training Error = 0.25390625
Iteration no. 2640, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47891471649739, Training Error = 0.2421875
Iteration no. 2641, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.45493812578306, Training Error = 0.208984375
Iteration no. 2642, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48370777648977, Training Error = 0.2421875
Iteration no. 2643, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48883130550746, Training Error = 0.24609375
Iteration no. 2644, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51826986470223, Training Error = 0.279296875
Iteration no. 2645, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48610544890649, Training Error = 0.23828125
Iteration no. 2646, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.53742905196268, Training Error = 0.275390625
Iteration no. 2647, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48827662604597, Training Error = 0.234375
Iteration no. 2648, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48568532070359, Training Error = 0.224609375
Iteration no. 2649, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47054784166231, Training Error = 0.212890625
Iteration no. 2650, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48746575948179, Training Error = 0.22265625
Testing... average test_loss = 0.95789443023018, average test_pred_err = 0.485
Iteration no. 2651, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50949162489612, Training Error = 0.240234375
Iteration no. 2652, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47996493886279, Training Error = 0.23046875
Iteration no. 2653, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47838638874956, Training Error = 0.23046875
Iteration no. 2654, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49653494993735, Training Error = 0.25
Iteration no. 2655, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.46487060364196, Training Error = 0.234375
Iteration no. 2656, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.5096402362059, Training Error = 0.2421875
Iteration no. 2657, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51495164102417, Training Error = 0.248046875
Iteration no. 2658, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48006169006047, Training Error = 0.236328125
Iteration no. 2659, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50126779179485, Training Error = 0.25390625
Iteration no. 2660, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.5141101884521, Training Error = 0.267578125
Iteration no. 2661, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50172536569954, Training Error = 0.244140625
Iteration no. 2662, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49792913126929, Training Error = 0.240234375
Iteration no. 2663, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52473274629026, Training Error = 0.2578125
Iteration no. 2664, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50396744521612, Training Error = 0.2734375
Iteration no. 2665, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49443847414793, Training Error = 0.234375
Iteration no. 2666, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49932554416279, Training Error = 0.259765625
Iteration no. 2667, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51742160995927, Training Error = 0.2578125
Iteration no. 2668, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47001213525178, Training Error = 0.240234375
Iteration no. 2669, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49923780657581, Training Error = 0.240234375
Iteration no. 2670, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49148026469349, Training Error = 0.23828125
Iteration no. 2671, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.53451439272617, Training Error = 0.26953125
Iteration no. 2672, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51022504115738, Training Error = 0.259765625
Iteration no. 2673, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.4933131678974, Training Error = 0.244140625
Iteration no. 2674, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.46363368158759, Training Error = 0.21484375
Iteration no. 2675, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52970753172617, Training Error = 0.24609375
Iteration no. 2676, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51320492577183, Training Error = 0.228515625
Iteration no. 2677, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.45023017716082, Training Error = 0.2109375
Iteration no. 2678, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49161477671475, Training Error = 0.251953125
Iteration no. 2679, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47759968944699, Training Error = 0.26171875
Iteration no. 2680, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49502434761957, Training Error = 0.234375
Iteration no. 2681, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48507890350448, Training Error = 0.265625
Iteration no. 2682, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52900413767561, Training Error = 0.279296875
Iteration no. 2683, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49835342770801, Training Error = 0.2421875
Iteration no. 2684, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48805525956948, Training Error = 0.25390625
Iteration no. 2685, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48704430573821, Training Error = 0.234375
Iteration no. 2686, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49255576944781, Training Error = 0.25390625
Iteration no. 2687, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48751145966812, Training Error = 0.26171875
Iteration no. 2688, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.52562369207002, Training Error = 0.255859375
Iteration no. 2689, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51984990693776, Training Error = 0.275390625
Iteration no. 2690, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47305115206606, Training Error = 0.2109375
Iteration no. 2691, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.50853318144224, Training Error = 0.259765625
Iteration no. 2692, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.47826691260337, Training Error = 0.2421875
Iteration no. 2693, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48768471133781, Training Error = 0.236328125
Iteration no. 2694, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48989819105349, Training Error = 0.251953125
Iteration no. 2695, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51969782987154, Training Error = 0.263671875
Iteration no. 2696, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.51785718391415, Training Error = 0.255859375
Iteration no. 2697, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.48596526100098, Training Error = 0.23828125
Iteration no. 2698, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.5068257048694, Training Error = 0.244140625
Iteration no. 2699, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.49003292440748, Training Error = 0.25
Iteration no. 2700, lr = 0.00018774960675295, attribute lr =1.8774960675295e-06, average batch_loss = 0.4894510054286, Training Error = 0.240234375
Testing... average test_loss = 0.90489498734628, average test_pred_err = 0.47
Snapshotting C_model... done
Iteration no. 2701, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50423396622162, Training Error = 0.255859375
Iteration no. 2702, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49624989191194, Training Error = 0.251953125
Iteration no. 2703, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51253762111574, Training Error = 0.255859375
Iteration no. 2704, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50362455591433, Training Error = 0.26953125
Iteration no. 2705, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49519717678694, Training Error = 0.234375
Iteration no. 2706, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53377743929747, Training Error = 0.28515625
Iteration no. 2707, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49583192455497, Training Error = 0.255859375
Iteration no. 2708, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49399399663561, Training Error = 0.228515625
Iteration no. 2709, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51779494149398, Training Error = 0.2578125
Iteration no. 2710, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5769684425937, Training Error = 0.3203125
Iteration no. 2711, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48747300168293, Training Error = 0.23828125
Iteration no. 2712, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49909254650683, Training Error = 0.25
Iteration no. 2713, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53731889157517, Training Error = 0.275390625
Iteration no. 2714, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49495343917593, Training Error = 0.23828125
Iteration no. 2715, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.47021204993075, Training Error = 0.220703125
Iteration no. 2716, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49334315579715, Training Error = 0.25390625
Iteration no. 2717, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.4737496820264, Training Error = 0.232421875
Iteration no. 2718, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48278628283723, Training Error = 0.240234375
Iteration no. 2719, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.52110704856804, Training Error = 0.275390625
Iteration no. 2720, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.54142064932267, Training Error = 0.265625
Iteration no. 2721, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50009942804577, Training Error = 0.24609375
Iteration no. 2722, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.45954803779986, Training Error = 0.212890625
Iteration no. 2723, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.55591689836664, Training Error = 0.3046875
Iteration no. 2724, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50346707403819, Training Error = 0.26953125
Iteration no. 2725, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49626946477912, Training Error = 0.224609375
Iteration no. 2726, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46115275233502, Training Error = 0.21484375
Iteration no. 2727, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51951104063939, Training Error = 0.263671875
Iteration no. 2728, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48105121084106, Training Error = 0.23828125
Iteration no. 2729, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46571064171876, Training Error = 0.228515625
Iteration no. 2730, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48675348777084, Training Error = 0.240234375
Iteration no. 2731, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48812508313265, Training Error = 0.240234375
Iteration no. 2732, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51750440222737, Training Error = 0.259765625
Iteration no. 2733, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53775069380408, Training Error = 0.283203125
Iteration no. 2734, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.52595327891564, Training Error = 0.2734375
Iteration no. 2735, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51744229960622, Training Error = 0.275390625
Iteration no. 2736, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46227027012637, Training Error = 0.21484375
Iteration no. 2737, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53184170433636, Training Error = 0.265625
Iteration no. 2738, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46305805222109, Training Error = 0.22265625
Iteration no. 2739, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49562768031852, Training Error = 0.22265625
Iteration no. 2740, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5216606433528, Training Error = 0.2578125
Iteration no. 2741, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.55173083183701, Training Error = 0.287109375
Iteration no. 2742, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50496058396178, Training Error = 0.267578125
Iteration no. 2743, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50069972548214, Training Error = 0.24609375
Iteration no. 2744, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5005165981593, Training Error = 0.23046875
Iteration no. 2745, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.47728591396296, Training Error = 0.212890625
Iteration no. 2746, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48985256055731, Training Error = 0.234375
Iteration no. 2747, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.4884691205983, Training Error = 0.2421875
Iteration no. 2748, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48036183835465, Training Error = 0.232421875
Iteration no. 2749, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51652275212956, Training Error = 0.23828125
Iteration no. 2750, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53908179731798, Training Error = 0.271484375
Testing... average test_loss = 0.99160003010406, average test_pred_err = 0.485
Iteration no. 2751, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.54422649618978, Training Error = 0.30859375
Iteration no. 2752, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5112566971382, Training Error = 0.244140625
Iteration no. 2753, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46326701416721, Training Error = 0.2265625
Iteration no. 2754, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53441376165998, Training Error = 0.275390625
Iteration no. 2755, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50764217776906, Training Error = 0.267578125
Iteration no. 2756, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50594992303555, Training Error = 0.248046875
Iteration no. 2757, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.47359886322983, Training Error = 0.24609375
Iteration no. 2758, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.52556792612895, Training Error = 0.271484375
Iteration no. 2759, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.54292812289352, Training Error = 0.279296875
Iteration no. 2760, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49821677104866, Training Error = 0.2578125
Iteration no. 2761, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.47680731686169, Training Error = 0.240234375
Iteration no. 2762, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50826808214256, Training Error = 0.25
Iteration no. 2763, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48594759503736, Training Error = 0.2421875
Iteration no. 2764, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.47805145283525, Training Error = 0.2421875
Iteration no. 2765, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50165001767085, Training Error = 0.24609375
Iteration no. 2766, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53548195160954, Training Error = 0.28515625
Iteration no. 2767, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5029471070495, Training Error = 0.271484375
Iteration no. 2768, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48902878333685, Training Error = 0.224609375
Iteration no. 2769, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.52646642594759, Training Error = 0.29296875
Iteration no. 2770, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50112888471016, Training Error = 0.2421875
Iteration no. 2771, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49321860320432, Training Error = 0.21875
Iteration no. 2772, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48505310246017, Training Error = 0.255859375
Iteration no. 2773, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5659935617614, Training Error = 0.294921875
Iteration no. 2774, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49461832476729, Training Error = 0.23828125
Iteration no. 2775, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.52089134468886, Training Error = 0.251953125
Iteration no. 2776, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50689715804454, Training Error = 0.2578125
Iteration no. 2777, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5143957855563, Training Error = 0.267578125
Iteration no. 2778, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48723008928957, Training Error = 0.23828125
Iteration no. 2779, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46110750523208, Training Error = 0.216796875
Iteration no. 2780, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.52011862246736, Training Error = 0.275390625
Iteration no. 2781, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51660918661058, Training Error = 0.25390625
Iteration no. 2782, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48032852274161, Training Error = 0.23828125
Iteration no. 2783, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48829554513735, Training Error = 0.244140625
Iteration no. 2784, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48140843395923, Training Error = 0.244140625
Iteration no. 2785, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49591476267151, Training Error = 0.25
Iteration no. 2786, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51747646978649, Training Error = 0.2734375
Iteration no. 2787, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50119329552015, Training Error = 0.25390625
Iteration no. 2788, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51802044833096, Training Error = 0.24609375
Iteration no. 2789, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48576072030515, Training Error = 0.228515625
Iteration no. 2790, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50090658070275, Training Error = 0.244140625
Iteration no. 2791, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.49416344682032, Training Error = 0.25
Iteration no. 2792, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50392220827655, Training Error = 0.244140625
Iteration no. 2793, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.48350811522251, Training Error = 0.236328125
Iteration no. 2794, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51515224076994, Training Error = 0.26953125
Iteration no. 2795, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.50115150108814, Training Error = 0.26171875
Iteration no. 2796, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.46714968773282, Training Error = 0.224609375
Iteration no. 2797, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.5013955234089, Training Error = 0.2265625
Iteration no. 2798, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.51576166941497, Training Error = 0.26953125
Iteration no. 2799, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.53019487799316, Training Error = 0.263671875
Iteration no. 2800, lr = 0.00013142472472707, attribute lr =1.3142472472707e-06, average batch_loss = 0.4905236018767, Training Error = 0.232421875
Testing... average test_loss = 0.93723036562587, average test_pred_err = 0.466
Snapshotting C_model... done
Iteration no. 2801, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.4919923707024, Training Error = 0.220703125
Iteration no. 2802, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49222302559633, Training Error = 0.248046875
Iteration no. 2803, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53271586005028, Training Error = 0.271484375
Iteration no. 2804, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53162896827825, Training Error = 0.267578125
Iteration no. 2805, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50266542949535, Training Error = 0.25390625
Iteration no. 2806, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53826939227564, Training Error = 0.28515625
Iteration no. 2807, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52025729449898, Training Error = 0.27734375
Iteration no. 2808, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51418792066294, Training Error = 0.25
Iteration no. 2809, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51811060285386, Training Error = 0.26171875
Iteration no. 2810, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49748363239969, Training Error = 0.23828125
Iteration no. 2811, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48396825372687, Training Error = 0.232421875
Iteration no. 2812, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52625584605695, Training Error = 0.267578125
Iteration no. 2813, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52203377156962, Training Error = 0.263671875
Iteration no. 2814, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51459392781711, Training Error = 0.26171875
Iteration no. 2815, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50068419349572, Training Error = 0.271484375
Iteration no. 2816, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50649815212845, Training Error = 0.263671875
Iteration no. 2817, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49248386431893, Training Error = 0.24609375
Iteration no. 2818, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50862088225894, Training Error = 0.248046875
Iteration no. 2819, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49856278638302, Training Error = 0.2578125
Iteration no. 2820, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.55203209033361, Training Error = 0.275390625
Iteration no. 2821, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49864473379569, Training Error = 0.263671875
Iteration no. 2822, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50643287896605, Training Error = 0.25390625
Iteration no. 2823, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49327573654241, Training Error = 0.228515625
Iteration no. 2824, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52282908355207, Training Error = 0.28515625
Iteration no. 2825, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52871872667236, Training Error = 0.271484375
Iteration no. 2826, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51902514596369, Training Error = 0.255859375
Iteration no. 2827, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48493680997871, Training Error = 0.232421875
Iteration no. 2828, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52916069965077, Training Error = 0.24609375
Iteration no. 2829, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.46933008017944, Training Error = 0.234375
Iteration no. 2830, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.46949522425061, Training Error = 0.2421875
Iteration no. 2831, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47882783751924, Training Error = 0.23828125
Iteration no. 2832, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.4741493799545, Training Error = 0.240234375
Iteration no. 2833, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.54520169333759, Training Error = 0.26171875
Iteration no. 2834, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.54813138479932, Training Error = 0.28125
Iteration no. 2835, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53261796180915, Training Error = 0.271484375
Iteration no. 2836, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47761109779813, Training Error = 0.23046875
Iteration no. 2837, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49579125559428, Training Error = 0.26953125
Iteration no. 2838, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50115591902605, Training Error = 0.25390625
Iteration no. 2839, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49365485644989, Training Error = 0.25
Iteration no. 2840, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50829070234413, Training Error = 0.25390625
Iteration no. 2841, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53668074847585, Training Error = 0.28125
Iteration no. 2842, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52918375947444, Training Error = 0.2734375
Iteration no. 2843, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.46563268577616, Training Error = 0.22265625
Iteration no. 2844, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52573977579171, Training Error = 0.259765625
Iteration no. 2845, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49562476781808, Training Error = 0.23046875
Iteration no. 2846, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50379879508737, Training Error = 0.23046875
Iteration no. 2847, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51895020574525, Training Error = 0.26171875
Iteration no. 2848, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48951449671164, Training Error = 0.23828125
Iteration no. 2849, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49302169422491, Training Error = 0.24609375
Iteration no. 2850, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51178779568539, Training Error = 0.255859375
Testing... average test_loss = 0.94082023470739, average test_pred_err = 0.467
Iteration no. 2851, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52825950770284, Training Error = 0.2734375
Iteration no. 2852, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51889882236763, Training Error = 0.267578125
Iteration no. 2853, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51230201274736, Training Error = 0.267578125
Iteration no. 2854, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47960845947065, Training Error = 0.234375
Iteration no. 2855, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.55030994011369, Training Error = 0.28515625
Iteration no. 2856, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.5245121824719, Training Error = 0.26171875
Iteration no. 2857, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48460463992099, Training Error = 0.24609375
Iteration no. 2858, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48608160526536, Training Error = 0.23828125
Iteration no. 2859, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51351431245773, Training Error = 0.244140625
Iteration no. 2860, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.4836172396365, Training Error = 0.240234375
Iteration no. 2861, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49379558705204, Training Error = 0.234375
Iteration no. 2862, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.55322683063221, Training Error = 0.2890625
Iteration no. 2863, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.4960482759935, Training Error = 0.259765625
Iteration no. 2864, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49653770739762, Training Error = 0.24609375
Iteration no. 2865, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.46205025727681, Training Error = 0.224609375
Iteration no. 2866, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48945238871673, Training Error = 0.255859375
Iteration no. 2867, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50507504409587, Training Error = 0.244140625
Iteration no. 2868, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49920000805787, Training Error = 0.23046875
Iteration no. 2869, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50855276442631, Training Error = 0.248046875
Iteration no. 2870, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53120067279338, Training Error = 0.275390625
Iteration no. 2871, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50920506589502, Training Error = 0.267578125
Iteration no. 2872, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47502607292529, Training Error = 0.22265625
Iteration no. 2873, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.54074236345172, Training Error = 0.291015625
Iteration no. 2874, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51026105570318, Training Error = 0.275390625
Iteration no. 2875, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51237335129548, Training Error = 0.25390625
Iteration no. 2876, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52047386556834, Training Error = 0.279296875
Iteration no. 2877, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47498040277363, Training Error = 0.251953125
Iteration no. 2878, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.53740057329347, Training Error = 0.28515625
Iteration no. 2879, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50462876567848, Training Error = 0.2734375
Iteration no. 2880, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50440875658121, Training Error = 0.2578125
Iteration no. 2881, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48949142126812, Training Error = 0.244140625
Iteration no. 2882, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50350337956508, Training Error = 0.265625
Iteration no. 2883, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48405958686022, Training Error = 0.24609375
Iteration no. 2884, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51294560018093, Training Error = 0.251953125
Iteration no. 2885, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48748851725641, Training Error = 0.23828125
Iteration no. 2886, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50440971510617, Training Error = 0.23046875
Iteration no. 2887, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.51177224208015, Training Error = 0.26171875
Iteration no. 2888, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50158306619393, Training Error = 0.24609375
Iteration no. 2889, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.52868454116227, Training Error = 0.279296875
Iteration no. 2890, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.48937461303969, Training Error = 0.23828125
Iteration no. 2891, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47738530875231, Training Error = 0.234375
Iteration no. 2892, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49461459678901, Training Error = 0.259765625
Iteration no. 2893, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49123163020705, Training Error = 0.22265625
Iteration no. 2894, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.46558933791078, Training Error = 0.23046875
Iteration no. 2895, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.49468902171497, Training Error = 0.228515625
Iteration no. 2896, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.47668398424352, Training Error = 0.244140625
Iteration no. 2897, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.5288044165847, Training Error = 0.2734375
Iteration no. 2898, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.45975139784713, Training Error = 0.23828125
Iteration no. 2899, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.50330243514518, Training Error = 0.255859375
Iteration no. 2900, lr = 9.1997307308948e-05, attribute lr =9.1997307308948e-07, average batch_loss = 0.46254959904331, Training Error = 0.232421875
Testing... average test_loss = 0.95463205733719, average test_pred_err = 0.477
Snapshotting C_model... done
Iteration no. 2901, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.4920626076526, Training Error = 0.255859375
Iteration no. 2902, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.46268842617062, Training Error = 0.220703125
Iteration no. 2903, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.50555601593627, Training Error = 0.255859375
Iteration no. 2904, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.48976366550692, Training Error = 0.244140625
Iteration no. 2905, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.52421660990873, Training Error = 0.255859375
Iteration no. 2906, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51144480699782, Training Error = 0.2578125
Iteration no. 2907, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.52423044887519, Training Error = 0.2578125
Iteration no. 2908, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.5202153558342, Training Error = 0.25390625
Iteration no. 2909, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.45886161708021, Training Error = 0.22265625
Iteration no. 2910, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.47988102948287, Training Error = 0.2265625
Iteration no. 2911, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.50512666260047, Training Error = 0.283203125
Iteration no. 2912, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.55300131586631, Training Error = 0.275390625
Iteration no. 2913, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51837213236201, Training Error = 0.26953125
Iteration no. 2914, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.49370941283972, Training Error = 0.244140625
Iteration no. 2915, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51966092466082, Training Error = 0.26171875
Iteration no. 2916, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51497583828732, Training Error = 0.265625
Iteration no. 2917, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.46476235589217, Training Error = 0.220703125
Iteration no. 2918, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.48458617294869, Training Error = 0.24609375
Iteration no. 2919, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.55076267058658, Training Error = 0.26953125
Iteration no. 2920, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.47431089046722, Training Error = 0.232421875
Iteration no. 2921, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51858247832303, Training Error = 0.2578125
Iteration no. 2922, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.50456875926472, Training Error = 0.244140625
Iteration no. 2923, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51035213930091, Training Error = 0.25
Iteration no. 2924, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.50259851493436, Training Error = 0.263671875
Iteration no. 2925, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.46503596351566, Training Error = 0.228515625
Iteration no. 2926, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.4899810451956, Training Error = 0.267578125
Iteration no. 2927, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.49019669257118, Training Error = 0.236328125
Iteration no. 2928, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.52517752939004, Training Error = 0.265625
Iteration no. 2929, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.50098659628969, Training Error = 0.234375
Iteration no. 2930, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.50447813625808, Training Error = 0.25
Iteration no. 2931, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.51663642977201, Training Error = 0.251953125
Iteration no. 2932, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.48213248313741, Training Error = 0.236328125
Iteration no. 2933, lr = 6.4398115116263e-05, attribute lr =6.4398115116263e-07, average batch_loss = 0.47740015933833, Training Error = 0.224609375
