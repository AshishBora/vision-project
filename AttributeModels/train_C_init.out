Loading pretrained model... done
Training... 
Testing... average test_loss = 0.74641795767934, average test_pred_err = 0.498
Iteration no. 1, lr = 2, average batch_loss = 0.74059130713164, Training Error = 0.49609375
Iteration no. 2, lr = 2, average batch_loss = 0.69297842400632, Training Error = 0.515625
Iteration no. 3, lr = 2, average batch_loss = 0.68202140054268, Training Error = 0.48046875
Iteration no. 4, lr = 2, average batch_loss = 0.67224970983554, Training Error = 0.291015625
Iteration no. 5, lr = 2, average batch_loss = 0.6641283369259, Training Error = 0.27734375
Iteration no. 6, lr = 2, average batch_loss = 0.65921039976772, Training Error = 0.265625
Iteration no. 7, lr = 2, average batch_loss = 0.64997067790277, Training Error = 0.248046875
Iteration no. 8, lr = 2, average batch_loss = 0.64007326369475, Training Error = 0.20703125
Iteration no. 9, lr = 2, average batch_loss = 0.63601925045904, Training Error = 0.271484375
Iteration no. 10, lr = 2, average batch_loss = 0.62050620560496, Training Error = 0.21484375
Iteration no. 11, lr = 2, average batch_loss = 0.62491249187462, Training Error = 0.26171875
Iteration no. 12, lr = 2, average batch_loss = 0.65804686170681, Training Error = 0.484375
Iteration no. 13, lr = 2, average batch_loss = 0.65059816393178, Training Error = 0.443359375
Iteration no. 14, lr = 2, average batch_loss = 0.66302169625165, Training Error = 0.47265625
Iteration no. 15, lr = 2, average batch_loss = 0.60749855001203, Training Error = 0.296875
Iteration no. 16, lr = 2, average batch_loss = 0.5982676358455, Training Error = 0.333984375
Iteration no. 17, lr = 2, average batch_loss = 0.73947261447565, Training Error = 0.55078125
Iteration no. 18, lr = 2, average batch_loss = 0.70472241821631, Training Error = 0.599609375
Iteration no. 19, lr = 2, average batch_loss = 0.62870020365087, Training Error = 0.337890625
Iteration no. 20, lr = 2, average batch_loss = 0.56316549655706, Training Error = 0.20703125
Iteration no. 21, lr = 2, average batch_loss = 0.54080542042123, Training Error = 0.16796875
Iteration no. 22, lr = 2, average batch_loss = 0.53705129368606, Training Error = 0.265625
Iteration no. 23, lr = 2, average batch_loss = 0.6174161748904, Training Error = 0.419921875
Iteration no. 24, lr = 2, average batch_loss = 0.71369742719818, Training Error = 0.5078125
Iteration no. 25, lr = 2, average batch_loss = 0.75103133391472, Training Error = 0.640625
Iteration no. 26, lr = 2, average batch_loss = 0.62481286168081, Training Error = 0.36328125
Iteration no. 27, lr = 2, average batch_loss = 0.56626714450311, Training Error = 0.2421875
Iteration no. 28, lr = 2, average batch_loss = 0.54620896732611, Training Error = 0.16796875
Iteration no. 29, lr = 2, average batch_loss = 0.57750087665155, Training Error = 0.3671875
Iteration no. 30, lr = 2, average batch_loss = 0.58459981572473, Training Error = 0.232421875
Iteration no. 31, lr = 2, average batch_loss = 0.57426114833472, Training Error = 0.2890625
Iteration no. 32, lr = 2, average batch_loss = 0.53737315615259, Training Error = 0.212890625
Iteration no. 33, lr = 2, average batch_loss = 0.51164144628692, Training Error = 0.275390625
Iteration no. 34, lr = 2, average batch_loss = 0.64075737948965, Training Error = 0.45703125
Iteration no. 35, lr = 2, average batch_loss = 0.64733832431162, Training Error = 0.423828125
Iteration no. 36, lr = 2, average batch_loss = 0.59074082865691, Training Error = 0.27734375
Iteration no. 37, lr = 2, average batch_loss = 0.53358488747311, Training Error = 0.271484375
Iteration no. 38, lr = 2, average batch_loss = 0.49576891104899, Training Error = 0.1875
Iteration no. 39, lr = 2, average batch_loss = 0.45082507261179, Training Error = 0.181640625
Iteration no. 40, lr = 2, average batch_loss = 0.42176712113034, Training Error = 0.146484375
Iteration no. 41, lr = 2, average batch_loss = 0.40601338697713, Training Error = 0.146484375
Iteration no. 42, lr = 2, average batch_loss = 0.41398775215067, Training Error = 0.15234375
Iteration no. 43, lr = 2, average batch_loss = 0.42369302280989, Training Error = 0.142578125
Iteration no. 44, lr = 2, average batch_loss = 0.47241324549898, Training Error = 0.236328125
Iteration no. 45, lr = 2, average batch_loss = 0.78047677743135, Training Error = 0.6328125
Iteration no. 46, lr = 2, average batch_loss = 1.3173696082648, Training Error = 0.611328125
Iteration no. 47, lr = 2, average batch_loss = 1.1266795888142, Training Error = 0.5390625
Iteration no. 48, lr = 2, average batch_loss = 0.5836749627098, Training Error = 0.236328125
Iteration no. 49, lr = 2, average batch_loss = 0.46788061298325, Training Error = 0.2265625
Iteration no. 50, lr = 2, average batch_loss = 0.42638067599161, Training Error = 0.158203125
Testing... average test_loss = 0.51463380638248, average test_pred_err = 0.202
Iteration no. 51, lr = 2, average batch_loss = 0.52470738345432, Training Error = 0.21875
Iteration no. 52, lr = 2, average batch_loss = 0.53114655329787, Training Error = 0.34765625
Iteration no. 53, lr = 2, average batch_loss = 0.71007878236789, Training Error = 0.5703125
Iteration no. 54, lr = 2, average batch_loss = 0.60266777646344, Training Error = 0.3203125
Iteration no. 55, lr = 2, average batch_loss = 0.49808631857862, Training Error = 0.201171875
Iteration no. 56, lr = 2, average batch_loss = 0.4417183893328, Training Error = 0.220703125
Iteration no. 57, lr = 2, average batch_loss = 0.40551433697259, Training Error = 0.16796875
Iteration no. 58, lr = 2, average batch_loss = 0.38620695552166, Training Error = 0.154296875
Iteration no. 59, lr = 2, average batch_loss = 0.42744902878214, Training Error = 0.1640625
Iteration no. 60, lr = 2, average batch_loss = 0.60675470835157, Training Error = 0.412109375
Iteration no. 61, lr = 2, average batch_loss = 1.0349922156175, Training Error = 0.623046875
Iteration no. 62, lr = 2, average batch_loss = 0.72265446715225, Training Error = 0.3671875
Iteration no. 63, lr = 2, average batch_loss = 0.58127432855158, Training Error = 0.283203125
Iteration no. 64, lr = 2, average batch_loss = 0.54127868834522, Training Error = 0.263671875
Iteration no. 65, lr = 2, average batch_loss = 0.40514349037181, Training Error = 0.173828125
Iteration no. 66, lr = 2, average batch_loss = 0.39320827161033, Training Error = 0.150390625
Iteration no. 67, lr = 2, average batch_loss = 0.36889089709752, Training Error = 0.14453125
Iteration no. 68, lr = 2, average batch_loss = 0.39071767564783, Training Error = 0.16796875
Iteration no. 69, lr = 2, average batch_loss = 0.40205466549651, Training Error = 0.171875
Iteration no. 70, lr = 2, average batch_loss = 0.56450256268857, Training Error = 0.40234375
Iteration no. 71, lr = 2, average batch_loss = 0.87431252432596, Training Error = 0.513671875
Iteration no. 72, lr = 2, average batch_loss = 0.73571940683485, Training Error = 0.45703125
Iteration no. 73, lr = 2, average batch_loss = 0.58288395933568, Training Error = 0.294921875
Iteration no. 74, lr = 2, average batch_loss = 0.4425866471549, Training Error = 0.201171875
Iteration no. 75, lr = 2, average batch_loss = 0.45070877100125, Training Error = 0.205078125
Iteration no. 76, lr = 2, average batch_loss = 0.46676881703308, Training Error = 0.166015625
Iteration no. 77, lr = 2, average batch_loss = 0.35275332480092, Training Error = 0.111328125
Iteration no. 78, lr = 2, average batch_loss = 0.44955331036435, Training Error = 0.173828125
Iteration no. 79, lr = 2, average batch_loss = 0.42170806598206, Training Error = 0.171875
Iteration no. 80, lr = 2, average batch_loss = 0.53296829511294, Training Error = 0.255859375
Iteration no. 81, lr = 2, average batch_loss = 0.48797792998103, Training Error = 0.251953125
Iteration no. 82, lr = 2, average batch_loss = 0.64757568816556, Training Error = 0.494140625
Iteration no. 83, lr = 2, average batch_loss = 0.53322172780889, Training Error = 0.263671875
Iteration no. 84, lr = 2, average batch_loss = 0.45530313763445, Training Error = 0.2265625
Iteration no. 85, lr = 2, average batch_loss = 0.36478669757361, Training Error = 0.158203125
Iteration no. 86, lr = 2, average batch_loss = 0.37739650357709, Training Error = 0.169921875
Iteration no. 87, lr = 2, average batch_loss = 0.3534193868826, Training Error = 0.126953125
Iteration no. 88, lr = 2, average batch_loss = 0.4311491812055, Training Error = 0.16796875
Iteration no. 89, lr = 2, average batch_loss = 0.70152546450764, Training Error = 0.447265625
Iteration no. 90, lr = 2, average batch_loss = 0.65077146160291, Training Error = 0.44921875
Iteration no. 91, lr = 2, average batch_loss = 0.57888264932088, Training Error = 0.291015625
Iteration no. 92, lr = 2, average batch_loss = 0.40558275215235, Training Error = 0.19140625
Iteration no. 93, lr = 2, average batch_loss = 0.35121366911132, Training Error = 0.16015625
Iteration no. 94, lr = 2, average batch_loss = 0.32918650382473, Training Error = 0.1328125
Iteration no. 95, lr = 2, average batch_loss = 0.35838344551532, Training Error = 0.142578125
Iteration no. 96, lr = 2, average batch_loss = 0.42188580719047, Training Error = 0.197265625
Iteration no. 97, lr = 2, average batch_loss = 0.73974967941533, Training Error = 0.62890625
Iteration no. 98, lr = 2, average batch_loss = 0.84091240374812, Training Error = 0.501953125
Iteration no. 99, lr = 2, average batch_loss = 0.69696174848917, Training Error = 0.34375
Iteration no. 100, lr = 2, average batch_loss = 0.3947123047654, Training Error = 0.126953125
Testing... average test_loss = 0.37839603552363, average test_pred_err = 0.156
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 101, lr = 1.4, average batch_loss = 0.41342088726024, Training Error = 0.181640625
Iteration no. 102, lr = 1.4, average batch_loss = 0.33414534011227, Training Error = 0.111328125
Iteration no. 103, lr = 1.4, average batch_loss = 0.39201204916328, Training Error = 0.1640625
Iteration no. 104, lr = 1.4, average batch_loss = 0.39088895485161, Training Error = 0.15625
Iteration no. 105, lr = 1.4, average batch_loss = 0.42024046996101, Training Error = 0.13671875
Iteration no. 106, lr = 1.4, average batch_loss = 0.40208701716726, Training Error = 0.15234375
Iteration no. 107, lr = 1.4, average batch_loss = 0.41320701963175, Training Error = 0.185546875
Iteration no. 108, lr = 1.4, average batch_loss = 0.41062111777713, Training Error = 0.154296875
Iteration no. 109, lr = 1.4, average batch_loss = 0.48949820106021, Training Error = 0.181640625
Iteration no. 110, lr = 1.4, average batch_loss = 0.36884897348841, Training Error = 0.16796875
Iteration no. 111, lr = 1.4, average batch_loss = 0.35422526026406, Training Error = 0.16015625
Iteration no. 112, lr = 1.4, average batch_loss = 0.35908758440239, Training Error = 0.115234375
Iteration no. 113, lr = 1.4, average batch_loss = 0.42200880892641, Training Error = 0.138671875
Iteration no. 114, lr = 1.4, average batch_loss = 0.38009894449631, Training Error = 0.130859375
Iteration no. 115, lr = 1.4, average batch_loss = 0.36135851013556, Training Error = 0.154296875
Iteration no. 116, lr = 1.4, average batch_loss = 0.38497425501458, Training Error = 0.16796875
Iteration no. 117, lr = 1.4, average batch_loss = 0.32415475611082, Training Error = 0.130859375
Iteration no. 118, lr = 1.4, average batch_loss = 0.31905750701078, Training Error = 0.109375
Iteration no. 119, lr = 1.4, average batch_loss = 0.39134221401683, Training Error = 0.158203125
Iteration no. 120, lr = 1.4, average batch_loss = 0.35293117951795, Training Error = 0.109375
Iteration no. 121, lr = 1.4, average batch_loss = 0.40795787381666, Training Error = 0.119140625
Iteration no. 122, lr = 1.4, average batch_loss = 0.53776666975031, Training Error = 0.34765625
Iteration no. 123, lr = 1.4, average batch_loss = 0.79009758310324, Training Error = 0.62890625
Iteration no. 124, lr = 1.4, average batch_loss = 0.68025804768731, Training Error = 0.431640625
Iteration no. 125, lr = 1.4, average batch_loss = 0.47551473103625, Training Error = 0.21484375
Iteration no. 126, lr = 1.4, average batch_loss = 0.43000404766714, Training Error = 0.1875
Iteration no. 127, lr = 1.4, average batch_loss = 0.35964073814062, Training Error = 0.173828125
Iteration no. 128, lr = 1.4, average batch_loss = 0.30444450428145, Training Error = 0.130859375
Iteration no. 129, lr = 1.4, average batch_loss = 0.33656622835578, Training Error = 0.154296875
Iteration no. 130, lr = 1.4, average batch_loss = 0.312033068633, Training Error = 0.138671875
Iteration no. 131, lr = 1.4, average batch_loss = 0.28841786615001, Training Error = 0.1171875
Iteration no. 132, lr = 1.4, average batch_loss = 0.35670078833876, Training Error = 0.12890625
Iteration no. 133, lr = 1.4, average batch_loss = 0.35421021471662, Training Error = 0.107421875
Iteration no. 134, lr = 1.4, average batch_loss = 0.46971063098947, Training Error = 0.2421875
Iteration no. 135, lr = 1.4, average batch_loss = 0.71987114348201, Training Error = 0.45703125
Iteration no. 136, lr = 1.4, average batch_loss = 0.6989515172434, Training Error = 0.455078125
Iteration no. 137, lr = 1.4, average batch_loss = 0.46297957475543, Training Error = 0.2265625
Iteration no. 138, lr = 1.4, average batch_loss = 0.36690336671384, Training Error = 0.150390625
Iteration no. 139, lr = 1.4, average batch_loss = 0.32379881976304, Training Error = 0.146484375
Iteration no. 140, lr = 1.4, average batch_loss = 0.39216645707059, Training Error = 0.16015625
Iteration no. 141, lr = 1.4, average batch_loss = 0.40743705726977, Training Error = 0.169921875
Iteration no. 142, lr = 1.4, average batch_loss = 0.35997091335103, Training Error = 0.125
Iteration no. 143, lr = 1.4, average batch_loss = 0.41725171650171, Training Error = 0.169921875
Iteration no. 144, lr = 1.4, average batch_loss = 0.31587811223812, Training Error = 0.1171875
Iteration no. 145, lr = 1.4, average batch_loss = 0.32988786708688, Training Error = 0.12890625
Iteration no. 146, lr = 1.4, average batch_loss = 0.32338434809665, Training Error = 0.109375
Iteration no. 147, lr = 1.4, average batch_loss = 0.36528666956124, Training Error = 0.12109375
Iteration no. 148, lr = 1.4, average batch_loss = 0.33371110375207, Training Error = 0.103515625
Iteration no. 149, lr = 1.4, average batch_loss = 0.36596040505307, Training Error = 0.126953125
Iteration no. 150, lr = 1.4, average batch_loss = 0.3184291500008, Training Error = 0.11328125
Testing... average test_loss = 0.34056882467632, average test_pred_err = 0.122
Iteration no. 151, lr = 1.4, average batch_loss = 0.33910539980588, Training Error = 0.1171875
Iteration no. 152, lr = 1.4, average batch_loss = 0.29167039289184, Training Error = 0.1171875
Iteration no. 153, lr = 1.4, average batch_loss = 0.34464295437554, Training Error = 0.119140625
Iteration no. 154, lr = 1.4, average batch_loss = 0.37187825817264, Training Error = 0.130859375
Iteration no. 155, lr = 1.4, average batch_loss = 0.37245003052766, Training Error = 0.134765625
Iteration no. 156, lr = 1.4, average batch_loss = 0.34782030044237, Training Error = 0.115234375
Iteration no. 157, lr = 1.4, average batch_loss = 0.34392317512631, Training Error = 0.103515625
Iteration no. 158, lr = 1.4, average batch_loss = 0.33373911472734, Training Error = 0.125
Iteration no. 159, lr = 1.4, average batch_loss = 0.25195040667393, Training Error = 0.099609375
Iteration no. 160, lr = 1.4, average batch_loss = 0.30982899083004, Training Error = 0.119140625
Iteration no. 161, lr = 1.4, average batch_loss = 0.30524681821792, Training Error = 0.103515625
Iteration no. 162, lr = 1.4, average batch_loss = 0.31622454551441, Training Error = 0.09375
Iteration no. 163, lr = 1.4, average batch_loss = 0.37494523331806, Training Error = 0.126953125
Iteration no. 164, lr = 1.4, average batch_loss = 0.44225528450355, Training Error = 0.19921875
Iteration no. 165, lr = 1.4, average batch_loss = 0.39092592588375, Training Error = 0.109375
Iteration no. 166, lr = 1.4, average batch_loss = 0.32695689518403, Training Error = 0.12890625
Iteration no. 167, lr = 1.4, average batch_loss = 0.27081244471493, Training Error = 0.099609375
Iteration no. 168, lr = 1.4, average batch_loss = 0.31116686817857, Training Error = 0.140625
Iteration no. 169, lr = 1.4, average batch_loss = 0.30916690717985, Training Error = 0.13671875
Iteration no. 170, lr = 1.4, average batch_loss = 0.23145628587723, Training Error = 0.08984375
Iteration no. 171, lr = 1.4, average batch_loss = 0.28656445974578, Training Error = 0.1015625
Iteration no. 172, lr = 1.4, average batch_loss = 0.36364096320912, Training Error = 0.109375
Iteration no. 173, lr = 1.4, average batch_loss = 0.40013160567655, Training Error = 0.173828125
Iteration no. 174, lr = 1.4, average batch_loss = 0.57763990583743, Training Error = 0.392578125
Iteration no. 175, lr = 1.4, average batch_loss = 1.053063234715, Training Error = 0.50390625
Iteration no. 176, lr = 1.4, average batch_loss = 0.67888120414704, Training Error = 0.42578125
Iteration no. 177, lr = 1.4, average batch_loss = 0.47580971688196, Training Error = 0.21875
Iteration no. 178, lr = 1.4, average batch_loss = 0.4207171393894, Training Error = 0.185546875
Iteration no. 179, lr = 1.4, average batch_loss = 0.35580103754739, Training Error = 0.171875
Iteration no. 180, lr = 1.4, average batch_loss = 0.30106682045583, Training Error = 0.1328125
Iteration no. 181, lr = 1.4, average batch_loss = 0.25983973912397, Training Error = 0.095703125
Iteration no. 182, lr = 1.4, average batch_loss = 0.3308874308294, Training Error = 0.12890625
Iteration no. 183, lr = 1.4, average batch_loss = 0.30518105951059, Training Error = 0.12109375
Iteration no. 184, lr = 1.4, average batch_loss = 0.33798841499896, Training Error = 0.15234375
Iteration no. 185, lr = 1.4, average batch_loss = 0.37942777714273, Training Error = 0.181640625
Iteration no. 186, lr = 1.4, average batch_loss = 0.47511572242968, Training Error = 0.220703125
Iteration no. 187, lr = 1.4, average batch_loss = 0.36521483037442, Training Error = 0.11328125
Iteration no. 188, lr = 1.4, average batch_loss = 0.28441905356374, Training Error = 0.125
Iteration no. 189, lr = 1.4, average batch_loss = 0.3424488217738, Training Error = 0.16015625
Iteration no. 190, lr = 1.4, average batch_loss = 0.27014626488167, Training Error = 0.119140625
Iteration no. 191, lr = 1.4, average batch_loss = 0.25571373160157, Training Error = 0.1015625
Iteration no. 192, lr = 1.4, average batch_loss = 0.28699141872623, Training Error = 0.107421875
Iteration no. 193, lr = 1.4, average batch_loss = 0.34238580092676, Training Error = 0.11328125
Iteration no. 194, lr = 1.4, average batch_loss = 0.34652787436547, Training Error = 0.1171875
Iteration no. 195, lr = 1.4, average batch_loss = 0.3204643501431, Training Error = 0.1484375
Iteration no. 196, lr = 1.4, average batch_loss = 0.26703493834358, Training Error = 0.11328125
Iteration no. 197, lr = 1.4, average batch_loss = 0.2826700499362, Training Error = 0.109375
Iteration no. 198, lr = 1.4, average batch_loss = 0.38080611121955, Training Error = 0.10546875
Iteration no. 199, lr = 1.4, average batch_loss = 0.62471196111255, Training Error = 0.423828125
Iteration no. 200, lr = 1.4, average batch_loss = 1.2217255251498, Training Error = 0.63671875
Testing... average test_loss = 0.73894344539755, average test_pred_err = 0.434
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 201, lr = 0.98, average batch_loss = 0.70700699782578, Training Error = 0.38671875
Iteration no. 202, lr = 0.98, average batch_loss = 0.5038535774309, Training Error = 0.205078125
Iteration no. 203, lr = 0.98, average batch_loss = 0.43560323730239, Training Error = 0.17578125
Iteration no. 204, lr = 0.98, average batch_loss = 0.42462790610146, Training Error = 0.17578125
Iteration no. 205, lr = 0.98, average batch_loss = 0.36512741712299, Training Error = 0.15625
Iteration no. 206, lr = 0.98, average batch_loss = 0.31294859103445, Training Error = 0.134765625
Iteration no. 207, lr = 0.98, average batch_loss = 0.33464596561212, Training Error = 0.15625
Iteration no. 208, lr = 0.98, average batch_loss = 0.33628688397305, Training Error = 0.150390625
Iteration no. 209, lr = 0.98, average batch_loss = 0.27548467471104, Training Error = 0.10546875
Iteration no. 210, lr = 0.98, average batch_loss = 0.30935834761743, Training Error = 0.123046875
Iteration no. 211, lr = 0.98, average batch_loss = 0.30177549432779, Training Error = 0.103515625
Iteration no. 212, lr = 0.98, average batch_loss = 0.33101707115933, Training Error = 0.08203125
Iteration no. 213, lr = 0.98, average batch_loss = 0.35758450636972, Training Error = 0.15234375
Iteration no. 214, lr = 0.98, average batch_loss = 0.29838376706399, Training Error = 0.12890625
Iteration no. 215, lr = 0.98, average batch_loss = 0.30887280810905, Training Error = 0.12109375
Iteration no. 216, lr = 0.98, average batch_loss = 0.29663475567729, Training Error = 0.10546875
Iteration no. 217, lr = 0.98, average batch_loss = 0.33885725337563, Training Error = 0.1171875
Iteration no. 218, lr = 0.98, average batch_loss = 0.32629697068326, Training Error = 0.115234375
Iteration no. 219, lr = 0.98, average batch_loss = 0.28737118045664, Training Error = 0.1171875
Iteration no. 220, lr = 0.98, average batch_loss = 0.28266264836312, Training Error = 0.107421875
Iteration no. 221, lr = 0.98, average batch_loss = 0.32742404448226, Training Error = 0.11328125
Iteration no. 222, lr = 0.98, average batch_loss = 0.32442608300252, Training Error = 0.111328125
Iteration no. 223, lr = 0.98, average batch_loss = 0.30742162384867, Training Error = 0.1171875
Iteration no. 224, lr = 0.98, average batch_loss = 0.32885654995836, Training Error = 0.123046875
Iteration no. 225, lr = 0.98, average batch_loss = 0.28802851947528, Training Error = 0.1171875
Iteration no. 226, lr = 0.98, average batch_loss = 0.31588870395522, Training Error = 0.115234375
Iteration no. 227, lr = 0.98, average batch_loss = 0.28870473893948, Training Error = 0.11328125
Iteration no. 228, lr = 0.98, average batch_loss = 0.28522341392096, Training Error = 0.095703125
Iteration no. 229, lr = 0.98, average batch_loss = 0.30321129624369, Training Error = 0.115234375
Iteration no. 230, lr = 0.98, average batch_loss = 0.30854091315118, Training Error = 0.091796875
Iteration no. 231, lr = 0.98, average batch_loss = 0.34337903789169, Training Error = 0.125
Iteration no. 232, lr = 0.98, average batch_loss = 0.28115333044221, Training Error = 0.107421875
Iteration no. 233, lr = 0.98, average batch_loss = 0.29409071122062, Training Error = 0.119140625
Iteration no. 234, lr = 0.98, average batch_loss = 0.26827579183774, Training Error = 0.095703125
Iteration no. 235, lr = 0.98, average batch_loss = 0.28905238192333, Training Error = 0.115234375
Iteration no. 236, lr = 0.98, average batch_loss = 0.29896328489992, Training Error = 0.087890625
Iteration no. 237, lr = 0.98, average batch_loss = 0.31847200202606, Training Error = 0.09765625
Iteration no. 238, lr = 0.98, average batch_loss = 0.36285969577761, Training Error = 0.1171875
Iteration no. 239, lr = 0.98, average batch_loss = 0.28107592837766, Training Error = 0.10546875
Iteration no. 240, lr = 0.98, average batch_loss = 0.29795308081123, Training Error = 0.091796875
Iteration no. 241, lr = 0.98, average batch_loss = 0.26840113191123, Training Error = 0.107421875
Iteration no. 242, lr = 0.98, average batch_loss = 0.24232994676685, Training Error = 0.091796875
Iteration no. 243, lr = 0.98, average batch_loss = 0.28534811210096, Training Error = 0.11328125
Iteration no. 244, lr = 0.98, average batch_loss = 0.2628532908082, Training Error = 0.08984375
Iteration no. 245, lr = 0.98, average batch_loss = 0.23902123801134, Training Error = 0.091796875
Iteration no. 246, lr = 0.98, average batch_loss = 0.24072638353447, Training Error = 0.08984375
Iteration no. 247, lr = 0.98, average batch_loss = 0.26460935807794, Training Error = 0.076171875
Iteration no. 248, lr = 0.98, average batch_loss = 0.34106972453008, Training Error = 0.107421875
Iteration no. 249, lr = 0.98, average batch_loss = 0.29481608344222, Training Error = 0.103515625
Iteration no. 250, lr = 0.98, average batch_loss = 0.30573100397444, Training Error = 0.10546875
Testing... average test_loss = 0.25165541446844, average test_pred_err = 0.104
Iteration no. 251, lr = 0.98, average batch_loss = 0.23727123921093, Training Error = 0.099609375
Iteration no. 252, lr = 0.98, average batch_loss = 0.30442853529847, Training Error = 0.130859375
Iteration no. 253, lr = 0.98, average batch_loss = 0.28579016744921, Training Error = 0.09375
Iteration no. 254, lr = 0.98, average batch_loss = 0.32682053510771, Training Error = 0.083984375
Iteration no. 255, lr = 0.98, average batch_loss = 0.33364586330927, Training Error = 0.109375
Iteration no. 256, lr = 0.98, average batch_loss = 0.2520940701733, Training Error = 0.087890625
Iteration no. 257, lr = 0.98, average batch_loss = 0.27678553646734, Training Error = 0.11328125
Iteration no. 258, lr = 0.98, average batch_loss = 0.26057437196792, Training Error = 0.0859375
Iteration no. 259, lr = 0.98, average batch_loss = 0.28351811067811, Training Error = 0.123046875
Iteration no. 260, lr = 0.98, average batch_loss = 0.28039722695716, Training Error = 0.119140625
Iteration no. 261, lr = 0.98, average batch_loss = 0.26457216256, Training Error = 0.09375
Iteration no. 262, lr = 0.98, average batch_loss = 0.23509702308562, Training Error = 0.0859375
Iteration no. 263, lr = 0.98, average batch_loss = 0.31742136225569, Training Error = 0.095703125
Iteration no. 264, lr = 0.98, average batch_loss = 0.31241454256037, Training Error = 0.10546875
Iteration no. 265, lr = 0.98, average batch_loss = 0.2953640248792, Training Error = 0.09765625
Iteration no. 266, lr = 0.98, average batch_loss = 0.28666345190868, Training Error = 0.10546875
Iteration no. 267, lr = 0.98, average batch_loss = 0.22110532899427, Training Error = 0.08203125
Iteration no. 268, lr = 0.98, average batch_loss = 0.23447457097744, Training Error = 0.076171875
Iteration no. 269, lr = 0.98, average batch_loss = 0.27013679657708, Training Error = 0.080078125
Iteration no. 270, lr = 0.98, average batch_loss = 0.30474077351783, Training Error = 0.0859375
Iteration no. 271, lr = 0.98, average batch_loss = 0.31512495043152, Training Error = 0.091796875
Iteration no. 272, lr = 0.98, average batch_loss = 0.27814966410846, Training Error = 0.076171875
Iteration no. 273, lr = 0.98, average batch_loss = 0.28990554407017, Training Error = 0.08984375
Iteration no. 274, lr = 0.98, average batch_loss = 0.31023540070582, Training Error = 0.078125
Iteration no. 275, lr = 0.98, average batch_loss = 0.27510251914999, Training Error = 0.08984375
Iteration no. 276, lr = 0.98, average batch_loss = 0.31226669883715, Training Error = 0.111328125
Iteration no. 277, lr = 0.98, average batch_loss = 0.25384557743, Training Error = 0.111328125
Iteration no. 278, lr = 0.98, average batch_loss = 0.24789682479984, Training Error = 0.0859375
Iteration no. 279, lr = 0.98, average batch_loss = 0.25350452591184, Training Error = 0.107421875
Iteration no. 280, lr = 0.98, average batch_loss = 0.22722279138413, Training Error = 0.08984375
Iteration no. 281, lr = 0.98, average batch_loss = 0.25792287853974, Training Error = 0.07421875
Iteration no. 282, lr = 0.98, average batch_loss = 0.30938924125883, Training Error = 0.095703125
Iteration no. 283, lr = 0.98, average batch_loss = 0.20783425873419, Training Error = 0.072265625
Iteration no. 284, lr = 0.98, average batch_loss = 0.23619559048799, Training Error = 0.083984375
Iteration no. 285, lr = 0.98, average batch_loss = 0.26707464225138, Training Error = 0.083984375
Iteration no. 286, lr = 0.98, average batch_loss = 0.29811706546263, Training Error = 0.111328125
Iteration no. 287, lr = 0.98, average batch_loss = 0.21242354654367, Training Error = 0.0703125
Iteration no. 288, lr = 0.98, average batch_loss = 0.18439331879124, Training Error = 0.0625
Iteration no. 289, lr = 0.98, average batch_loss = 0.23540062339103, Training Error = 0.083984375
Iteration no. 290, lr = 0.98, average batch_loss = 0.26638390907049, Training Error = 0.083984375
Iteration no. 291, lr = 0.98, average batch_loss = 0.27350560438423, Training Error = 0.068359375
Iteration no. 292, lr = 0.98, average batch_loss = 0.31846936454701, Training Error = 0.08203125
Iteration no. 293, lr = 0.98, average batch_loss = 0.38286026621469, Training Error = 0.1640625
Iteration no. 294, lr = 0.98, average batch_loss = 0.52641114193479, Training Error = 0.330078125
Iteration no. 295, lr = 0.98, average batch_loss = 0.85799275295259, Training Error = 0.484375
Iteration no. 296, lr = 0.98, average batch_loss = 0.89713141467094, Training Error = 0.5078125
Iteration no. 297, lr = 0.98, average batch_loss = 0.39103083426239, Training Error = 0.12890625
Iteration no. 298, lr = 0.98, average batch_loss = 0.33789966541671, Training Error = 0.13671875
Iteration no. 299, lr = 0.98, average batch_loss = 0.32056773932312, Training Error = 0.138671875
Iteration no. 300, lr = 0.98, average batch_loss = 0.25429163409155, Training Error = 0.1171875
Testing... average test_loss = 0.29513084382241, average test_pred_err = 0.139
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 301, lr = 0.686, average batch_loss = 0.27825686020494, Training Error = 0.126953125
Iteration no. 302, lr = 0.686, average batch_loss = 0.24912716215642, Training Error = 0.109375
Iteration no. 303, lr = 0.686, average batch_loss = 0.23855781347755, Training Error = 0.1015625
Iteration no. 304, lr = 0.686, average batch_loss = 0.25155375028319, Training Error = 0.111328125
Iteration no. 305, lr = 0.686, average batch_loss = 0.25084530158932, Training Error = 0.087890625
Iteration no. 306, lr = 0.686, average batch_loss = 0.27205259535698, Training Error = 0.11328125
Iteration no. 307, lr = 0.686, average batch_loss = 0.24938860171026, Training Error = 0.099609375
Iteration no. 308, lr = 0.686, average batch_loss = 0.256698141321, Training Error = 0.109375
Iteration no. 309, lr = 0.686, average batch_loss = 0.26735662907264, Training Error = 0.1015625
Iteration no. 310, lr = 0.686, average batch_loss = 0.20201286478422, Training Error = 0.076171875
Iteration no. 311, lr = 0.686, average batch_loss = 0.25984250591367, Training Error = 0.076171875
Iteration no. 312, lr = 0.686, average batch_loss = 0.25788965671876, Training Error = 0.091796875
Iteration no. 313, lr = 0.686, average batch_loss = 0.24546107841703, Training Error = 0.095703125
Iteration no. 314, lr = 0.686, average batch_loss = 0.23195437999388, Training Error = 0.087890625
Iteration no. 315, lr = 0.686, average batch_loss = 0.28185211324949, Training Error = 0.091796875
Iteration no. 316, lr = 0.686, average batch_loss = 0.24712534994613, Training Error = 0.1015625
Iteration no. 317, lr = 0.686, average batch_loss = 0.24185043113434, Training Error = 0.0859375
Iteration no. 318, lr = 0.686, average batch_loss = 0.22457987577012, Training Error = 0.078125
Iteration no. 319, lr = 0.686, average batch_loss = 0.31630032540904, Training Error = 0.11328125
Iteration no. 320, lr = 0.686, average batch_loss = 0.25913935308945, Training Error = 0.099609375
Iteration no. 321, lr = 0.686, average batch_loss = 0.30109742729058, Training Error = 0.095703125
Iteration no. 322, lr = 0.686, average batch_loss = 0.26708240584602, Training Error = 0.1015625
Iteration no. 323, lr = 0.686, average batch_loss = 0.2287592844558, Training Error = 0.068359375
Iteration no. 324, lr = 0.686, average batch_loss = 0.24863170595927, Training Error = 0.099609375
Iteration no. 325, lr = 0.686, average batch_loss = 0.20858716297366, Training Error = 0.0625
Iteration no. 326, lr = 0.686, average batch_loss = 0.23517744630129, Training Error = 0.091796875
Iteration no. 327, lr = 0.686, average batch_loss = 0.2555040117126, Training Error = 0.078125
Iteration no. 328, lr = 0.686, average batch_loss = 0.28571539241696, Training Error = 0.12109375
Iteration no. 329, lr = 0.686, average batch_loss = 0.23559526331628, Training Error = 0.08203125
Iteration no. 330, lr = 0.686, average batch_loss = 0.23748126765086, Training Error = 0.1015625
Iteration no. 331, lr = 0.686, average batch_loss = 0.2402999741445, Training Error = 0.0859375
Iteration no. 332, lr = 0.686, average batch_loss = 0.23715600943664, Training Error = 0.103515625
Iteration no. 333, lr = 0.686, average batch_loss = 0.26519901856509, Training Error = 0.0859375
Iteration no. 334, lr = 0.686, average batch_loss = 0.27256161822778, Training Error = 0.076171875
Iteration no. 335, lr = 0.686, average batch_loss = 0.28593011899975, Training Error = 0.095703125
Iteration no. 336, lr = 0.686, average batch_loss = 0.3159700873239, Training Error = 0.115234375
Iteration no. 337, lr = 0.686, average batch_loss = 0.2896217955922, Training Error = 0.10546875
Iteration no. 338, lr = 0.686, average batch_loss = 0.19517854620601, Training Error = 0.078125
Iteration no. 339, lr = 0.686, average batch_loss = 0.21793800688609, Training Error = 0.064453125
Iteration no. 340, lr = 0.686, average batch_loss = 0.25092195695989, Training Error = 0.087890625
Iteration no. 341, lr = 0.686, average batch_loss = 0.26713943104241, Training Error = 0.111328125
Iteration no. 342, lr = 0.686, average batch_loss = 0.22802088526346, Training Error = 0.099609375
Iteration no. 343, lr = 0.686, average batch_loss = 0.26908975016739, Training Error = 0.09375
Iteration no. 344, lr = 0.686, average batch_loss = 0.2339919178315, Training Error = 0.072265625
Iteration no. 345, lr = 0.686, average batch_loss = 0.23594855917323, Training Error = 0.06640625
Iteration no. 346, lr = 0.686, average batch_loss = 0.26979775418855, Training Error = 0.083984375
Iteration no. 347, lr = 0.686, average batch_loss = 0.21058872416052, Training Error = 0.076171875
Iteration no. 348, lr = 0.686, average batch_loss = 0.24654109263059, Training Error = 0.09375
Iteration no. 349, lr = 0.686, average batch_loss = 0.23003081795037, Training Error = 0.078125
Iteration no. 350, lr = 0.686, average batch_loss = 0.29170078183894, Training Error = 0.083984375
Testing... average test_loss = 0.25297777439585, average test_pred_err = 0.09
Iteration no. 351, lr = 0.686, average batch_loss = 0.23896834825296, Training Error = 0.076171875
Iteration no. 352, lr = 0.686, average batch_loss = 0.26458652265473, Training Error = 0.09375
Iteration no. 353, lr = 0.686, average batch_loss = 0.24827327060175, Training Error = 0.078125
Iteration no. 354, lr = 0.686, average batch_loss = 0.2202527701699, Training Error = 0.091796875
Iteration no. 355, lr = 0.686, average batch_loss = 0.21506403652252, Training Error = 0.072265625
Iteration no. 356, lr = 0.686, average batch_loss = 0.24659980026706, Training Error = 0.09765625
Iteration no. 357, lr = 0.686, average batch_loss = 0.22748851460237, Training Error = 0.080078125
Iteration no. 358, lr = 0.686, average batch_loss = 0.18354673119341, Training Error = 0.056640625
Iteration no. 359, lr = 0.686, average batch_loss = 0.26430991101467, Training Error = 0.095703125
Iteration no. 360, lr = 0.686, average batch_loss = 0.24494201619566, Training Error = 0.091796875
Iteration no. 361, lr = 0.686, average batch_loss = 0.23726421432183, Training Error = 0.091796875
Iteration no. 362, lr = 0.686, average batch_loss = 0.21646657284994, Training Error = 0.0859375
Iteration no. 363, lr = 0.686, average batch_loss = 0.24535475554064, Training Error = 0.078125
Iteration no. 364, lr = 0.686, average batch_loss = 0.26718881594028, Training Error = 0.09765625
Iteration no. 365, lr = 0.686, average batch_loss = 0.28148065136974, Training Error = 0.078125
Iteration no. 366, lr = 0.686, average batch_loss = 0.27756134363268, Training Error = 0.103515625
Iteration no. 367, lr = 0.686, average batch_loss = 0.2475497368894, Training Error = 0.08984375
Iteration no. 368, lr = 0.686, average batch_loss = 0.29143588360552, Training Error = 0.134765625
Iteration no. 369, lr = 0.686, average batch_loss = 0.18427089942113, Training Error = 0.076171875
Iteration no. 370, lr = 0.686, average batch_loss = 0.21119492951615, Training Error = 0.072265625
Iteration no. 371, lr = 0.686, average batch_loss = 0.33496126407098, Training Error = 0.08203125
Iteration no. 372, lr = 0.686, average batch_loss = 0.38620541112727, Training Error = 0.126953125
Iteration no. 373, lr = 0.686, average batch_loss = 0.26501699141066, Training Error = 0.068359375
Iteration no. 374, lr = 0.686, average batch_loss = 0.24141001647102, Training Error = 0.080078125
Iteration no. 375, lr = 0.686, average batch_loss = 0.22927412058177, Training Error = 0.09375
Iteration no. 376, lr = 0.686, average batch_loss = 0.21177056655061, Training Error = 0.0859375
Iteration no. 377, lr = 0.686, average batch_loss = 0.20810779018691, Training Error = 0.09375
Iteration no. 378, lr = 0.686, average batch_loss = 0.20116242432674, Training Error = 0.0703125
Iteration no. 379, lr = 0.686, average batch_loss = 0.20664137729689, Training Error = 0.076171875
Iteration no. 380, lr = 0.686, average batch_loss = 0.22370574473621, Training Error = 0.0703125
Iteration no. 381, lr = 0.686, average batch_loss = 0.23287974426431, Training Error = 0.09375
Iteration no. 382, lr = 0.686, average batch_loss = 0.20157310707364, Training Error = 0.080078125
Iteration no. 383, lr = 0.686, average batch_loss = 0.20506585349511, Training Error = 0.0703125
Iteration no. 384, lr = 0.686, average batch_loss = 0.20145539737913, Training Error = 0.07421875
Iteration no. 385, lr = 0.686, average batch_loss = 0.19773492891182, Training Error = 0.0703125
Iteration no. 386, lr = 0.686, average batch_loss = 0.28676279529407, Training Error = 0.111328125
Iteration no. 387, lr = 0.686, average batch_loss = 0.28776278678282, Training Error = 0.072265625
Iteration no. 388, lr = 0.686, average batch_loss = 0.34043728940063, Training Error = 0.078125
Iteration no. 389, lr = 0.686, average batch_loss = 0.27118505406588, Training Error = 0.083984375
Iteration no. 390, lr = 0.686, average batch_loss = 0.21224247907569, Training Error = 0.08984375
Iteration no. 391, lr = 0.686, average batch_loss = 0.23778185222539, Training Error = 0.09765625
Iteration no. 392, lr = 0.686, average batch_loss = 0.19771766590126, Training Error = 0.080078125
Iteration no. 393, lr = 0.686, average batch_loss = 0.18787044800171, Training Error = 0.080078125
Iteration no. 394, lr = 0.686, average batch_loss = 0.23095345443557, Training Error = 0.0859375
Iteration no. 395, lr = 0.686, average batch_loss = 0.22890971120871, Training Error = 0.0859375
Iteration no. 396, lr = 0.686, average batch_loss = 0.17265967048314, Training Error = 0.0625
Iteration no. 397, lr = 0.686, average batch_loss = 0.23109964340596, Training Error = 0.095703125
Iteration no. 398, lr = 0.686, average batch_loss = 0.16869927939225, Training Error = 0.064453125
Iteration no. 399, lr = 0.686, average batch_loss = 0.22418780551682, Training Error = 0.095703125
Iteration no. 400, lr = 0.686, average batch_loss = 0.21189905029451, Training Error = 0.0859375
Testing... average test_loss = 0.20727849663245, average test_pred_err = 0.087
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 401, lr = 0.4802, average batch_loss = 0.18572638037439, Training Error = 0.0625
Iteration no. 402, lr = 0.4802, average batch_loss = 0.19874329989265, Training Error = 0.080078125
Iteration no. 403, lr = 0.4802, average batch_loss = 0.19269650523754, Training Error = 0.078125
Iteration no. 404, lr = 0.4802, average batch_loss = 0.18200192802688, Training Error = 0.068359375
Iteration no. 405, lr = 0.4802, average batch_loss = 0.21557747873574, Training Error = 0.083984375
Iteration no. 406, lr = 0.4802, average batch_loss = 0.23410649073299, Training Error = 0.08984375
Iteration no. 407, lr = 0.4802, average batch_loss = 0.170031876898, Training Error = 0.056640625
Iteration no. 408, lr = 0.4802, average batch_loss = 0.20146811519705, Training Error = 0.08984375
Iteration no. 409, lr = 0.4802, average batch_loss = 0.21412553005071, Training Error = 0.0859375
Iteration no. 410, lr = 0.4802, average batch_loss = 0.2475636957359, Training Error = 0.07421875
Iteration no. 411, lr = 0.4802, average batch_loss = 0.18703655690383, Training Error = 0.064453125
Iteration no. 412, lr = 0.4802, average batch_loss = 0.22025051272201, Training Error = 0.072265625
Iteration no. 413, lr = 0.4802, average batch_loss = 0.18695417653807, Training Error = 0.07421875
Iteration no. 414, lr = 0.4802, average batch_loss = 0.19181224544715, Training Error = 0.068359375
Iteration no. 415, lr = 0.4802, average batch_loss = 0.18651473582472, Training Error = 0.06640625
Iteration no. 416, lr = 0.4802, average batch_loss = 0.20309925761352, Training Error = 0.0546875
Iteration no. 417, lr = 0.4802, average batch_loss = 0.19088919061056, Training Error = 0.072265625
Iteration no. 418, lr = 0.4802, average batch_loss = 0.17818799799418, Training Error = 0.07421875
Iteration no. 419, lr = 0.4802, average batch_loss = 0.21420714538477, Training Error = 0.087890625
Iteration no. 420, lr = 0.4802, average batch_loss = 0.21016410869583, Training Error = 0.08203125
Iteration no. 421, lr = 0.4802, average batch_loss = 0.20127785379412, Training Error = 0.072265625
Iteration no. 422, lr = 0.4802, average batch_loss = 0.19637180529555, Training Error = 0.060546875
Iteration no. 423, lr = 0.4802, average batch_loss = 0.21654313746623, Training Error = 0.080078125
Iteration no. 424, lr = 0.4802, average batch_loss = 0.23391415877382, Training Error = 0.080078125
Iteration no. 425, lr = 0.4802, average batch_loss = 0.18989486021288, Training Error = 0.056640625
Iteration no. 426, lr = 0.4802, average batch_loss = 0.15845253956573, Training Error = 0.060546875
Iteration no. 427, lr = 0.4802, average batch_loss = 0.217320373127, Training Error = 0.076171875
Iteration no. 428, lr = 0.4802, average batch_loss = 0.18641770808587, Training Error = 0.068359375
Iteration no. 429, lr = 0.4802, average batch_loss = 0.22553815529058, Training Error = 0.0703125
Iteration no. 430, lr = 0.4802, average batch_loss = 0.24403796847124, Training Error = 0.0859375
Iteration no. 431, lr = 0.4802, average batch_loss = 0.19465236668428, Training Error = 0.0703125
Iteration no. 432, lr = 0.4802, average batch_loss = 0.17980626427262, Training Error = 0.07421875
Iteration no. 433, lr = 0.4802, average batch_loss = 0.19364632765907, Training Error = 0.064453125
Iteration no. 434, lr = 0.4802, average batch_loss = 0.17704912047179, Training Error = 0.048828125
Iteration no. 435, lr = 0.4802, average batch_loss = 0.22551980963877, Training Error = 0.072265625
Iteration no. 436, lr = 0.4802, average batch_loss = 0.19197295518591, Training Error = 0.0703125
Iteration no. 437, lr = 0.4802, average batch_loss = 0.21721387385304, Training Error = 0.078125
Iteration no. 438, lr = 0.4802, average batch_loss = 0.18412681412753, Training Error = 0.0703125
Iteration no. 439, lr = 0.4802, average batch_loss = 0.19909871151313, Training Error = 0.0703125
Iteration no. 440, lr = 0.4802, average batch_loss = 0.16248100451289, Training Error = 0.0546875
Iteration no. 441, lr = 0.4802, average batch_loss = 0.21886490485889, Training Error = 0.08203125
Iteration no. 442, lr = 0.4802, average batch_loss = 0.18976803916978, Training Error = 0.064453125
Iteration no. 443, lr = 0.4802, average batch_loss = 0.18177564460471, Training Error = 0.0625
Iteration no. 444, lr = 0.4802, average batch_loss = 0.21109507013765, Training Error = 0.072265625
Iteration no. 445, lr = 0.4802, average batch_loss = 0.21060375352532, Training Error = 0.076171875
Iteration no. 446, lr = 0.4802, average batch_loss = 0.20223918591239, Training Error = 0.0625
Iteration no. 447, lr = 0.4802, average batch_loss = 0.22674998930902, Training Error = 0.076171875
Iteration no. 448, lr = 0.4802, average batch_loss = 0.20594658150913, Training Error = 0.06640625
Iteration no. 449, lr = 0.4802, average batch_loss = 0.16184523691648, Training Error = 0.048828125
Iteration no. 450, lr = 0.4802, average batch_loss = 0.15704008575172, Training Error = 0.044921875
Testing... average test_loss = 0.21926222355728, average test_pred_err = 0.066
Iteration no. 451, lr = 0.4802, average batch_loss = 0.22433268797811, Training Error = 0.068359375
Iteration no. 452, lr = 0.4802, average batch_loss = 0.20174769052448, Training Error = 0.0703125
Iteration no. 453, lr = 0.4802, average batch_loss = 0.1904039891854, Training Error = 0.072265625
Iteration no. 454, lr = 0.4802, average batch_loss = 0.1962980669671, Training Error = 0.048828125
Iteration no. 455, lr = 0.4802, average batch_loss = 0.21277942796913, Training Error = 0.080078125
Iteration no. 456, lr = 0.4802, average batch_loss = 0.17408641664943, Training Error = 0.064453125
Iteration no. 457, lr = 0.4802, average batch_loss = 0.20093025972701, Training Error = 0.08203125
Iteration no. 458, lr = 0.4802, average batch_loss = 0.19638076171293, Training Error = 0.083984375
Iteration no. 459, lr = 0.4802, average batch_loss = 0.18883059898606, Training Error = 0.06640625
Iteration no. 460, lr = 0.4802, average batch_loss = 0.20751050850007, Training Error = 0.083984375
Iteration no. 461, lr = 0.4802, average batch_loss = 0.1909443328388, Training Error = 0.06640625
Iteration no. 462, lr = 0.4802, average batch_loss = 0.18568937367276, Training Error = 0.05859375
Iteration no. 463, lr = 0.4802, average batch_loss = 0.17693296466406, Training Error = 0.0546875
Iteration no. 464, lr = 0.4802, average batch_loss = 0.21105044269492, Training Error = 0.060546875
Iteration no. 465, lr = 0.4802, average batch_loss = 0.2025104957379, Training Error = 0.06640625
Iteration no. 466, lr = 0.4802, average batch_loss = 0.18893216040645, Training Error = 0.0546875
Iteration no. 467, lr = 0.4802, average batch_loss = 0.19702959967401, Training Error = 0.06640625
Iteration no. 468, lr = 0.4802, average batch_loss = 0.22061873615089, Training Error = 0.07421875
Iteration no. 469, lr = 0.4802, average batch_loss = 0.22070331068632, Training Error = 0.072265625
Iteration no. 470, lr = 0.4802, average batch_loss = 0.20575505666325, Training Error = 0.060546875
Iteration no. 471, lr = 0.4802, average batch_loss = 0.21704181582496, Training Error = 0.05859375
Iteration no. 472, lr = 0.4802, average batch_loss = 0.25299403792369, Training Error = 0.060546875
Iteration no. 473, lr = 0.4802, average batch_loss = 0.24402729374217, Training Error = 0.07421875
Iteration no. 474, lr = 0.4802, average batch_loss = 0.23115202104307, Training Error = 0.068359375
Iteration no. 475, lr = 0.4802, average batch_loss = 0.20268980402942, Training Error = 0.064453125
Iteration no. 476, lr = 0.4802, average batch_loss = 0.22025670873295, Training Error = 0.0625
Iteration no. 477, lr = 0.4802, average batch_loss = 0.21412205090623, Training Error = 0.064453125
Iteration no. 478, lr = 0.4802, average batch_loss = 0.19470744609567, Training Error = 0.076171875
Iteration no. 479, lr = 0.4802, average batch_loss = 0.19422161921657, Training Error = 0.080078125
Iteration no. 480, lr = 0.4802, average batch_loss = 0.18724953519759, Training Error = 0.072265625
Iteration no. 481, lr = 0.4802, average batch_loss = 0.18125540789178, Training Error = 0.05859375
Iteration no. 482, lr = 0.4802, average batch_loss = 0.20858750788654, Training Error = 0.083984375
Iteration no. 483, lr = 0.4802, average batch_loss = 0.20734898989329, Training Error = 0.083984375
Iteration no. 484, lr = 0.4802, average batch_loss = 0.20981184311143, Training Error = 0.0859375
Iteration no. 485, lr = 0.4802, average batch_loss = 0.17490924972858, Training Error = 0.046875
Iteration no. 486, lr = 0.4802, average batch_loss = 0.19438428244108, Training Error = 0.07421875
Iteration no. 487, lr = 0.4802, average batch_loss = 0.20290998149784, Training Error = 0.07421875
Iteration no. 488, lr = 0.4802, average batch_loss = 0.15687512276129, Training Error = 0.05078125
Iteration no. 489, lr = 0.4802, average batch_loss = 0.20490409789119, Training Error = 0.0625
Iteration no. 490, lr = 0.4802, average batch_loss = 0.19938667310916, Training Error = 0.052734375
Iteration no. 491, lr = 0.4802, average batch_loss = 0.19308866825266, Training Error = 0.064453125
Iteration no. 492, lr = 0.4802, average batch_loss = 0.19067005682471, Training Error = 0.06640625
Iteration no. 493, lr = 0.4802, average batch_loss = 0.19665414993098, Training Error = 0.068359375
Iteration no. 494, lr = 0.4802, average batch_loss = 0.19772146630006, Training Error = 0.080078125
Iteration no. 495, lr = 0.4802, average batch_loss = 0.1765994580944, Training Error = 0.0703125
Iteration no. 496, lr = 0.4802, average batch_loss = 0.19309524776082, Training Error = 0.052734375
Iteration no. 497, lr = 0.4802, average batch_loss = 0.2173739526924, Training Error = 0.06640625
Iteration no. 498, lr = 0.4802, average batch_loss = 0.22219529786992, Training Error = 0.05859375
Iteration no. 499, lr = 0.4802, average batch_loss = 0.18239487032902, Training Error = 0.068359375
Iteration no. 500, lr = 0.4802, average batch_loss = 0.21224500945047, Training Error = 0.07421875
Testing... average test_loss = 0.20055445175868, average test_pred_err = 0.08
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 501, lr = 0.33614, average batch_loss = 0.20804449910912, Training Error = 0.08203125
Iteration no. 502, lr = 0.33614, average batch_loss = 0.16523502983799, Training Error = 0.0625
Iteration no. 503, lr = 0.33614, average batch_loss = 0.17743845791549, Training Error = 0.072265625
Iteration no. 504, lr = 0.33614, average batch_loss = 0.16926318201577, Training Error = 0.056640625
Iteration no. 505, lr = 0.33614, average batch_loss = 0.15765685736553, Training Error = 0.05859375
Iteration no. 506, lr = 0.33614, average batch_loss = 0.21100715393237, Training Error = 0.076171875
Iteration no. 507, lr = 0.33614, average batch_loss = 0.14001272857855, Training Error = 0.056640625
Iteration no. 508, lr = 0.33614, average batch_loss = 0.21134839351952, Training Error = 0.08984375
Iteration no. 509, lr = 0.33614, average batch_loss = 0.18103054133214, Training Error = 0.076171875
Iteration no. 510, lr = 0.33614, average batch_loss = 0.15602114497084, Training Error = 0.0546875
Iteration no. 511, lr = 0.33614, average batch_loss = 0.17546998077589, Training Error = 0.060546875
Iteration no. 512, lr = 0.33614, average batch_loss = 0.22262017285252, Training Error = 0.08984375
Iteration no. 513, lr = 0.33614, average batch_loss = 0.15351693959171, Training Error = 0.05078125
Iteration no. 514, lr = 0.33614, average batch_loss = 0.15636135946519, Training Error = 0.041015625
Iteration no. 515, lr = 0.33614, average batch_loss = 0.19720252348419, Training Error = 0.08203125
Iteration no. 516, lr = 0.33614, average batch_loss = 0.16169176816464, Training Error = 0.04296875
Iteration no. 517, lr = 0.33614, average batch_loss = 0.18010475653795, Training Error = 0.048828125
Iteration no. 518, lr = 0.33614, average batch_loss = 0.16754871243545, Training Error = 0.056640625
Iteration no. 519, lr = 0.33614, average batch_loss = 0.17480384780217, Training Error = 0.056640625
Iteration no. 520, lr = 0.33614, average batch_loss = 0.16421552034847, Training Error = 0.056640625
Iteration no. 521, lr = 0.33614, average batch_loss = 0.20389246008044, Training Error = 0.08203125
Iteration no. 522, lr = 0.33614, average batch_loss = 0.17513515812811, Training Error = 0.0703125
Iteration no. 523, lr = 0.33614, average batch_loss = 0.20414610579456, Training Error = 0.068359375
Iteration no. 524, lr = 0.33614, average batch_loss = 0.16388068438174, Training Error = 0.06640625
Iteration no. 525, lr = 0.33614, average batch_loss = 0.16920813568016, Training Error = 0.0546875
Iteration no. 526, lr = 0.33614, average batch_loss = 0.16694079943513, Training Error = 0.060546875
Iteration no. 527, lr = 0.33614, average batch_loss = 0.20975755795635, Training Error = 0.072265625
Iteration no. 528, lr = 0.33614, average batch_loss = 0.15712295553438, Training Error = 0.064453125
Iteration no. 529, lr = 0.33614, average batch_loss = 0.14249730043041, Training Error = 0.048828125
Iteration no. 530, lr = 0.33614, average batch_loss = 0.19420279179475, Training Error = 0.060546875
Iteration no. 531, lr = 0.33614, average batch_loss = 0.16121313965638, Training Error = 0.048828125
Iteration no. 532, lr = 0.33614, average batch_loss = 0.16948883726473, Training Error = 0.060546875
Iteration no. 533, lr = 0.33614, average batch_loss = 0.15551295504081, Training Error = 0.044921875
Iteration no. 534, lr = 0.33614, average batch_loss = 0.20883199955603, Training Error = 0.068359375
Iteration no. 535, lr = 0.33614, average batch_loss = 0.17931411650692, Training Error = 0.072265625
Iteration no. 536, lr = 0.33614, average batch_loss = 0.19348658278616, Training Error = 0.078125
Iteration no. 537, lr = 0.33614, average batch_loss = 0.15155521170428, Training Error = 0.05078125
Iteration no. 538, lr = 0.33614, average batch_loss = 0.17917165377502, Training Error = 0.060546875
Iteration no. 539, lr = 0.33614, average batch_loss = 0.18265137006553, Training Error = 0.060546875
Iteration no. 540, lr = 0.33614, average batch_loss = 0.19142507201625, Training Error = 0.064453125
Iteration no. 541, lr = 0.33614, average batch_loss = 0.17321120520469, Training Error = 0.060546875
Iteration no. 542, lr = 0.33614, average batch_loss = 0.15477941569134, Training Error = 0.056640625
Iteration no. 543, lr = 0.33614, average batch_loss = 0.17409830929381, Training Error = 0.056640625
Iteration no. 544, lr = 0.33614, average batch_loss = 0.16015970467342, Training Error = 0.04296875
Iteration no. 545, lr = 0.33614, average batch_loss = 0.13869745347496, Training Error = 0.048828125
Iteration no. 546, lr = 0.33614, average batch_loss = 0.15157131101112, Training Error = 0.044921875
Iteration no. 547, lr = 0.33614, average batch_loss = 0.18512241216755, Training Error = 0.060546875
Iteration no. 548, lr = 0.33614, average batch_loss = 0.16485138835894, Training Error = 0.044921875
Iteration no. 549, lr = 0.33614, average batch_loss = 0.16709762772999, Training Error = 0.052734375
Iteration no. 550, lr = 0.33614, average batch_loss = 0.1655295939888, Training Error = 0.052734375
Testing... average test_loss = 0.16438004767628, average test_pred_err = 0.049
Iteration no. 551, lr = 0.33614, average batch_loss = 0.17041059420317, Training Error = 0.056640625
Iteration no. 552, lr = 0.33614, average batch_loss = 0.15599837737295, Training Error = 0.0546875
Iteration no. 553, lr = 0.33614, average batch_loss = 0.16799638468143, Training Error = 0.06640625
Iteration no. 554, lr = 0.33614, average batch_loss = 0.16199890650079, Training Error = 0.048828125
Iteration no. 555, lr = 0.33614, average batch_loss = 0.16747795079938, Training Error = 0.0546875
Iteration no. 556, lr = 0.33614, average batch_loss = 0.19088576056041, Training Error = 0.056640625
Iteration no. 557, lr = 0.33614, average batch_loss = 0.19988187251298, Training Error = 0.076171875
Iteration no. 558, lr = 0.33614, average batch_loss = 0.20112344465671, Training Error = 0.072265625
Iteration no. 559, lr = 0.33614, average batch_loss = 0.14886761331899, Training Error = 0.048828125
Iteration no. 560, lr = 0.33614, average batch_loss = 0.20408926360797, Training Error = 0.087890625
Iteration no. 561, lr = 0.33614, average batch_loss = 0.16318769360688, Training Error = 0.05859375
Iteration no. 562, lr = 0.33614, average batch_loss = 0.14598425556876, Training Error = 0.04296875
Iteration no. 563, lr = 0.33614, average batch_loss = 0.17037244335098, Training Error = 0.0625
Iteration no. 564, lr = 0.33614, average batch_loss = 0.21232092342142, Training Error = 0.09375
Iteration no. 565, lr = 0.33614, average batch_loss = 0.17818488632774, Training Error = 0.06640625
Iteration no. 566, lr = 0.33614, average batch_loss = 0.14491507789253, Training Error = 0.041015625
Iteration no. 567, lr = 0.33614, average batch_loss = 0.17151158327832, Training Error = 0.056640625
Iteration no. 568, lr = 0.33614, average batch_loss = 0.21624914831679, Training Error = 0.078125
Iteration no. 569, lr = 0.33614, average batch_loss = 0.19524441854092, Training Error = 0.0546875
Iteration no. 570, lr = 0.33614, average batch_loss = 0.19226754915844, Training Error = 0.064453125
Iteration no. 571, lr = 0.33614, average batch_loss = 0.16367952652907, Training Error = 0.068359375
Iteration no. 572, lr = 0.33614, average batch_loss = 0.16894993922661, Training Error = 0.052734375
Iteration no. 573, lr = 0.33614, average batch_loss = 0.1525544501558, Training Error = 0.052734375
Iteration no. 574, lr = 0.33614, average batch_loss = 0.15072018452599, Training Error = 0.0546875
Iteration no. 575, lr = 0.33614, average batch_loss = 0.14894907271025, Training Error = 0.044921875
Iteration no. 576, lr = 0.33614, average batch_loss = 0.16438501145567, Training Error = 0.0546875
Iteration no. 577, lr = 0.33614, average batch_loss = 0.15928279059261, Training Error = 0.0546875
Iteration no. 578, lr = 0.33614, average batch_loss = 0.18228853264212, Training Error = 0.0625
Iteration no. 579, lr = 0.33614, average batch_loss = 0.18595925705724, Training Error = 0.0703125
Iteration no. 580, lr = 0.33614, average batch_loss = 0.14317576632412, Training Error = 0.05078125
Iteration no. 581, lr = 0.33614, average batch_loss = 0.20454658600206, Training Error = 0.078125
Iteration no. 582, lr = 0.33614, average batch_loss = 0.17100222384939, Training Error = 0.056640625
Iteration no. 583, lr = 0.33614, average batch_loss = 0.17877182003012, Training Error = 0.06640625
Iteration no. 584, lr = 0.33614, average batch_loss = 0.17965813174295, Training Error = 0.0703125
Iteration no. 585, lr = 0.33614, average batch_loss = 0.14191406287409, Training Error = 0.046875
Iteration no. 586, lr = 0.33614, average batch_loss = 0.18446058347198, Training Error = 0.05859375
Iteration no. 587, lr = 0.33614, average batch_loss = 0.14184703657694, Training Error = 0.044921875
Iteration no. 588, lr = 0.33614, average batch_loss = 0.17332636125464, Training Error = 0.060546875
Iteration no. 589, lr = 0.33614, average batch_loss = 0.15263315197418, Training Error = 0.04296875
Iteration no. 590, lr = 0.33614, average batch_loss = 0.18104619109989, Training Error = 0.06640625
Iteration no. 591, lr = 0.33614, average batch_loss = 0.16467611660007, Training Error = 0.044921875
Iteration no. 592, lr = 0.33614, average batch_loss = 0.17169863943495, Training Error = 0.05859375
Iteration no. 593, lr = 0.33614, average batch_loss = 0.19950172073338, Training Error = 0.072265625
Iteration no. 594, lr = 0.33614, average batch_loss = 0.19082314645293, Training Error = 0.07421875
Iteration no. 595, lr = 0.33614, average batch_loss = 0.14104496123719, Training Error = 0.052734375
Iteration no. 596, lr = 0.33614, average batch_loss = 0.1549766654673, Training Error = 0.0625
Iteration no. 597, lr = 0.33614, average batch_loss = 0.16128381677974, Training Error = 0.072265625
Iteration no. 598, lr = 0.33614, average batch_loss = 0.17021775208946, Training Error = 0.0625
Iteration no. 599, lr = 0.33614, average batch_loss = 0.17542741474288, Training Error = 0.0625
Iteration no. 600, lr = 0.33614, average batch_loss = 0.16262698942091, Training Error = 0.056640625
Testing... average test_loss = 0.1683865222312, average test_pred_err = 0.059
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 601, lr = 0.235298, average batch_loss = 0.15070190355019, Training Error = 0.05078125
Iteration no. 602, lr = 0.235298, average batch_loss = 0.12939973052096, Training Error = 0.0390625
Iteration no. 603, lr = 0.235298, average batch_loss = 0.15004007929122, Training Error = 0.046875
Iteration no. 604, lr = 0.235298, average batch_loss = 0.16606114405337, Training Error = 0.06640625
Iteration no. 605, lr = 0.235298, average batch_loss = 0.16114410477791, Training Error = 0.0546875
Iteration no. 606, lr = 0.235298, average batch_loss = 0.15499565856014, Training Error = 0.0546875
Iteration no. 607, lr = 0.235298, average batch_loss = 0.13152240126429, Training Error = 0.041015625
Iteration no. 608, lr = 0.235298, average batch_loss = 0.15784647272957, Training Error = 0.05078125
Iteration no. 609, lr = 0.235298, average batch_loss = 0.147575915337, Training Error = 0.046875
Iteration no. 610, lr = 0.235298, average batch_loss = 0.14593760255221, Training Error = 0.0625
Iteration no. 611, lr = 0.235298, average batch_loss = 0.17049459223767, Training Error = 0.056640625
Iteration no. 612, lr = 0.235298, average batch_loss = 0.14981033900362, Training Error = 0.056640625
Iteration no. 613, lr = 0.235298, average batch_loss = 0.13264713782369, Training Error = 0.037109375
Iteration no. 614, lr = 0.235298, average batch_loss = 0.13821676312304, Training Error = 0.046875
Iteration no. 615, lr = 0.235298, average batch_loss = 0.16348610867396, Training Error = 0.064453125
Iteration no. 616, lr = 0.235298, average batch_loss = 0.1360716942, Training Error = 0.056640625
Iteration no. 617, lr = 0.235298, average batch_loss = 0.13584537307952, Training Error = 0.046875
Iteration no. 618, lr = 0.235298, average batch_loss = 0.15733806013261, Training Error = 0.052734375
Iteration no. 619, lr = 0.235298, average batch_loss = 0.14194360704234, Training Error = 0.052734375
Iteration no. 620, lr = 0.235298, average batch_loss = 0.13364628607139, Training Error = 0.05078125
Iteration no. 621, lr = 0.235298, average batch_loss = 0.17116781223973, Training Error = 0.05859375
Iteration no. 622, lr = 0.235298, average batch_loss = 0.14328344200872, Training Error = 0.0546875
Iteration no. 623, lr = 0.235298, average batch_loss = 0.18573890266058, Training Error = 0.0703125
Iteration no. 624, lr = 0.235298, average batch_loss = 0.12604692577489, Training Error = 0.04296875
Iteration no. 625, lr = 0.235298, average batch_loss = 0.17102159765294, Training Error = 0.05078125
Iteration no. 626, lr = 0.235298, average batch_loss = 0.1399125342657, Training Error = 0.052734375
Iteration no. 627, lr = 0.235298, average batch_loss = 0.14078660585278, Training Error = 0.044921875
Iteration no. 628, lr = 0.235298, average batch_loss = 0.179261523782, Training Error = 0.076171875
Iteration no. 629, lr = 0.235298, average batch_loss = 0.14344764447763, Training Error = 0.05859375
Iteration no. 630, lr = 0.235298, average batch_loss = 0.15136267575371, Training Error = 0.048828125
Iteration no. 631, lr = 0.235298, average batch_loss = 0.17856527619812, Training Error = 0.06640625
Iteration no. 632, lr = 0.235298, average batch_loss = 0.13042761934981, Training Error = 0.041015625
Iteration no. 633, lr = 0.235298, average batch_loss = 0.17909137466463, Training Error = 0.07421875
Iteration no. 634, lr = 0.235298, average batch_loss = 0.18356638596077, Training Error = 0.0703125
Iteration no. 635, lr = 0.235298, average batch_loss = 0.14641766725239, Training Error = 0.056640625
Iteration no. 636, lr = 0.235298, average batch_loss = 0.15683732060245, Training Error = 0.0546875
Iteration no. 637, lr = 0.235298, average batch_loss = 0.16058171452057, Training Error = 0.0625
Iteration no. 638, lr = 0.235298, average batch_loss = 0.13206059553953, Training Error = 0.03515625
Iteration no. 639, lr = 0.235298, average batch_loss = 0.15516262044422, Training Error = 0.060546875
Iteration no. 640, lr = 0.235298, average batch_loss = 0.15198527741734, Training Error = 0.0546875
Iteration no. 641, lr = 0.235298, average batch_loss = 0.13381064713735, Training Error = 0.046875
Iteration no. 642, lr = 0.235298, average batch_loss = 0.1733273273726, Training Error = 0.06640625
Iteration no. 643, lr = 0.235298, average batch_loss = 0.12554319588651, Training Error = 0.044921875
Iteration no. 644, lr = 0.235298, average batch_loss = 0.15742994698331, Training Error = 0.052734375
Iteration no. 645, lr = 0.235298, average batch_loss = 0.16443180026625, Training Error = 0.05859375
Iteration no. 646, lr = 0.235298, average batch_loss = 0.13612974083735, Training Error = 0.037109375
Iteration no. 647, lr = 0.235298, average batch_loss = 0.13856686032336, Training Error = 0.048828125
Iteration no. 648, lr = 0.235298, average batch_loss = 0.15649311980384, Training Error = 0.05078125
Iteration no. 649, lr = 0.235298, average batch_loss = 0.1687013056817, Training Error = 0.05859375
Iteration no. 650, lr = 0.235298, average batch_loss = 0.15505201027978, Training Error = 0.0546875
Testing... average test_loss = 0.17543516283903, average test_pred_err = 0.064
Iteration no. 651, lr = 0.235298, average batch_loss = 0.15618233699441, Training Error = 0.05078125
Iteration no. 652, lr = 0.235298, average batch_loss = 0.16767684633428, Training Error = 0.0625
Iteration no. 653, lr = 0.235298, average batch_loss = 0.17204892719769, Training Error = 0.052734375
Iteration no. 654, lr = 0.235298, average batch_loss = 0.14607275108805, Training Error = 0.05078125
Iteration no. 655, lr = 0.235298, average batch_loss = 0.15629746660736, Training Error = 0.05078125
Iteration no. 656, lr = 0.235298, average batch_loss = 0.13516956552224, Training Error = 0.0390625
Iteration no. 657, lr = 0.235298, average batch_loss = 0.15847222395573, Training Error = 0.0546875
Iteration no. 658, lr = 0.235298, average batch_loss = 0.18366840324583, Training Error = 0.064453125
Iteration no. 659, lr = 0.235298, average batch_loss = 0.14570298522574, Training Error = 0.05859375
Iteration no. 660, lr = 0.235298, average batch_loss = 0.17903532030388, Training Error = 0.064453125
Iteration no. 661, lr = 0.235298, average batch_loss = 0.13818823370734, Training Error = 0.048828125
Iteration no. 662, lr = 0.235298, average batch_loss = 0.2032335634347, Training Error = 0.064453125
Iteration no. 663, lr = 0.235298, average batch_loss = 0.1626462691528, Training Error = 0.064453125
Iteration no. 664, lr = 0.235298, average batch_loss = 0.13264859796246, Training Error = 0.0390625
Iteration no. 665, lr = 0.235298, average batch_loss = 0.15458813556271, Training Error = 0.060546875
Iteration no. 666, lr = 0.235298, average batch_loss = 0.19522061401768, Training Error = 0.072265625
Iteration no. 667, lr = 0.235298, average batch_loss = 0.15973326573176, Training Error = 0.060546875
Iteration no. 668, lr = 0.235298, average batch_loss = 0.17159177765345, Training Error = 0.05078125
Iteration no. 669, lr = 0.235298, average batch_loss = 0.15275025527609, Training Error = 0.064453125
Iteration no. 670, lr = 0.235298, average batch_loss = 0.16549961870506, Training Error = 0.056640625
Iteration no. 671, lr = 0.235298, average batch_loss = 0.15209846316654, Training Error = 0.052734375
Iteration no. 672, lr = 0.235298, average batch_loss = 0.1741879996036, Training Error = 0.068359375
Iteration no. 673, lr = 0.235298, average batch_loss = 0.16335335305854, Training Error = 0.06640625
Iteration no. 674, lr = 0.235298, average batch_loss = 0.14067887665417, Training Error = 0.0546875
Iteration no. 675, lr = 0.235298, average batch_loss = 0.17501668413285, Training Error = 0.056640625
Iteration no. 676, lr = 0.235298, average batch_loss = 0.18136336396213, Training Error = 0.064453125
Iteration no. 677, lr = 0.235298, average batch_loss = 0.15307915473111, Training Error = 0.05078125
Iteration no. 678, lr = 0.235298, average batch_loss = 0.15739992230774, Training Error = 0.056640625
Iteration no. 679, lr = 0.235298, average batch_loss = 0.15799014218433, Training Error = 0.056640625
Iteration no. 680, lr = 0.235298, average batch_loss = 0.14572585724822, Training Error = 0.05078125
Iteration no. 681, lr = 0.235298, average batch_loss = 0.16522048458272, Training Error = 0.05078125
Iteration no. 682, lr = 0.235298, average batch_loss = 0.12089713660128, Training Error = 0.03515625
Iteration no. 683, lr = 0.235298, average batch_loss = 0.12503646369554, Training Error = 0.041015625
Iteration no. 684, lr = 0.235298, average batch_loss = 0.12992474758649, Training Error = 0.03125
Iteration no. 685, lr = 0.235298, average batch_loss = 0.14880989713912, Training Error = 0.044921875
Iteration no. 686, lr = 0.235298, average batch_loss = 0.15740942154838, Training Error = 0.0546875
Iteration no. 687, lr = 0.235298, average batch_loss = 0.16564543289151, Training Error = 0.068359375
Iteration no. 688, lr = 0.235298, average batch_loss = 0.13937143058482, Training Error = 0.02734375
Iteration no. 689, lr = 0.235298, average batch_loss = 0.13903455225233, Training Error = 0.0546875
Iteration no. 690, lr = 0.235298, average batch_loss = 0.13568214112102, Training Error = 0.037109375
Iteration no. 691, lr = 0.235298, average batch_loss = 0.19256997674227, Training Error = 0.078125
Iteration no. 692, lr = 0.235298, average batch_loss = 0.12263358268773, Training Error = 0.03125
Iteration no. 693, lr = 0.235298, average batch_loss = 0.16899314344719, Training Error = 0.0625
Iteration no. 694, lr = 0.235298, average batch_loss = 0.12473166867795, Training Error = 0.0390625
Iteration no. 695, lr = 0.235298, average batch_loss = 0.1687768738751, Training Error = 0.05859375
Iteration no. 696, lr = 0.235298, average batch_loss = 0.1713427220982, Training Error = 0.07421875
Iteration no. 697, lr = 0.235298, average batch_loss = 0.15680268047318, Training Error = 0.056640625
Iteration no. 698, lr = 0.235298, average batch_loss = 0.13077671024537, Training Error = 0.041015625
Iteration no. 699, lr = 0.235298, average batch_loss = 0.17669706944213, Training Error = 0.064453125
Iteration no. 700, lr = 0.235298, average batch_loss = 0.15488355241284, Training Error = 0.048828125
Testing... average test_loss = 0.16081469768498, average test_pred_err = 0.057
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 701, lr = 0.1647086, average batch_loss = 0.14917065640699, Training Error = 0.048828125
Iteration no. 702, lr = 0.1647086, average batch_loss = 0.13705050563677, Training Error = 0.048828125
Iteration no. 703, lr = 0.1647086, average batch_loss = 0.13620530521944, Training Error = 0.05078125
Iteration no. 704, lr = 0.1647086, average batch_loss = 0.15709530448653, Training Error = 0.05859375
Iteration no. 705, lr = 0.1647086, average batch_loss = 0.15144471903352, Training Error = 0.052734375
Iteration no. 706, lr = 0.1647086, average batch_loss = 0.13288333432157, Training Error = 0.052734375
Iteration no. 707, lr = 0.1647086, average batch_loss = 0.15716559597546, Training Error = 0.048828125
Iteration no. 708, lr = 0.1647086, average batch_loss = 0.12420441891121, Training Error = 0.037109375
Iteration no. 709, lr = 0.1647086, average batch_loss = 0.14107162435877, Training Error = 0.05078125
Iteration no. 710, lr = 0.1647086, average batch_loss = 0.16474660662779, Training Error = 0.0625
Iteration no. 711, lr = 0.1647086, average batch_loss = 0.14733187041504, Training Error = 0.05078125
Iteration no. 712, lr = 0.1647086, average batch_loss = 0.16890841585848, Training Error = 0.068359375
Iteration no. 713, lr = 0.1647086, average batch_loss = 0.14054940590908, Training Error = 0.052734375
Iteration no. 714, lr = 0.1647086, average batch_loss = 0.13212429740972, Training Error = 0.048828125
Iteration no. 715, lr = 0.1647086, average batch_loss = 0.13816552642778, Training Error = 0.05078125
Iteration no. 716, lr = 0.1647086, average batch_loss = 0.12884104969501, Training Error = 0.044921875
Iteration no. 717, lr = 0.1647086, average batch_loss = 0.15197248268114, Training Error = 0.064453125
Iteration no. 718, lr = 0.1647086, average batch_loss = 0.14624629664883, Training Error = 0.056640625
Iteration no. 719, lr = 0.1647086, average batch_loss = 0.14900632168989, Training Error = 0.052734375
Iteration no. 720, lr = 0.1647086, average batch_loss = 0.12145448793449, Training Error = 0.029296875
Iteration no. 721, lr = 0.1647086, average batch_loss = 0.16910782502722, Training Error = 0.05859375
Iteration no. 722, lr = 0.1647086, average batch_loss = 0.17054477149711, Training Error = 0.060546875
Iteration no. 723, lr = 0.1647086, average batch_loss = 0.14189371336598, Training Error = 0.044921875
Iteration no. 724, lr = 0.1647086, average batch_loss = 0.16836730359183, Training Error = 0.076171875
Iteration no. 725, lr = 0.1647086, average batch_loss = 0.1189756562857, Training Error = 0.03515625
Iteration no. 726, lr = 0.1647086, average batch_loss = 0.1684022645304, Training Error = 0.068359375
Iteration no. 727, lr = 0.1647086, average batch_loss = 0.15206820182948, Training Error = 0.0546875
Iteration no. 728, lr = 0.1647086, average batch_loss = 0.16562002133167, Training Error = 0.064453125
Iteration no. 729, lr = 0.1647086, average batch_loss = 0.17347356978604, Training Error = 0.0625
Iteration no. 730, lr = 0.1647086, average batch_loss = 0.13287729872788, Training Error = 0.048828125
Iteration no. 731, lr = 0.1647086, average batch_loss = 0.17225262187812, Training Error = 0.064453125
Iteration no. 732, lr = 0.1647086, average batch_loss = 0.15962508759145, Training Error = 0.056640625
Iteration no. 733, lr = 0.1647086, average batch_loss = 0.13053449287543, Training Error = 0.033203125
Iteration no. 734, lr = 0.1647086, average batch_loss = 0.15654861393244, Training Error = 0.05078125
Iteration no. 735, lr = 0.1647086, average batch_loss = 0.12769116694382, Training Error = 0.037109375
Iteration no. 736, lr = 0.1647086, average batch_loss = 0.15483103051082, Training Error = 0.048828125
Iteration no. 737, lr = 0.1647086, average batch_loss = 0.12615330856705, Training Error = 0.046875
Iteration no. 738, lr = 0.1647086, average batch_loss = 0.16282400447186, Training Error = 0.056640625
Iteration no. 739, lr = 0.1647086, average batch_loss = 0.14264364257663, Training Error = 0.052734375
Iteration no. 740, lr = 0.1647086, average batch_loss = 0.12454233123065, Training Error = 0.0390625
Iteration no. 741, lr = 0.1647086, average batch_loss = 0.14512687816715, Training Error = 0.0546875
Iteration no. 742, lr = 0.1647086, average batch_loss = 0.15487932427918, Training Error = 0.05078125
Iteration no. 743, lr = 0.1647086, average batch_loss = 0.14449846857602, Training Error = 0.05078125
Iteration no. 744, lr = 0.1647086, average batch_loss = 0.17465633112182, Training Error = 0.072265625
Iteration no. 745, lr = 0.1647086, average batch_loss = 0.15519727431828, Training Error = 0.064453125
Iteration no. 746, lr = 0.1647086, average batch_loss = 0.17300876053268, Training Error = 0.06640625
Iteration no. 747, lr = 0.1647086, average batch_loss = 0.15469876451601, Training Error = 0.060546875
Iteration no. 748, lr = 0.1647086, average batch_loss = 0.18648939492499, Training Error = 0.06640625
Iteration no. 749, lr = 0.1647086, average batch_loss = 0.1356369344221, Training Error = 0.05078125
Iteration no. 750, lr = 0.1647086, average batch_loss = 0.14435313619456, Training Error = 0.044921875
Testing... average test_loss = 0.15635125495069, average test_pred_err = 0.061
Iteration no. 751, lr = 0.1647086, average batch_loss = 0.13905112114295, Training Error = 0.05078125
Iteration no. 752, lr = 0.1647086, average batch_loss = 0.12989854377617, Training Error = 0.048828125
Iteration no. 753, lr = 0.1647086, average batch_loss = 0.14524970475389, Training Error = 0.05078125
Iteration no. 754, lr = 0.1647086, average batch_loss = 0.14541056849241, Training Error = 0.048828125
Iteration no. 755, lr = 0.1647086, average batch_loss = 0.14814796417603, Training Error = 0.056640625
Iteration no. 756, lr = 0.1647086, average batch_loss = 0.11979767229362, Training Error = 0.029296875
Iteration no. 757, lr = 0.1647086, average batch_loss = 0.13067160968182, Training Error = 0.0390625
Iteration no. 758, lr = 0.1647086, average batch_loss = 0.13085975796302, Training Error = 0.048828125
Iteration no. 759, lr = 0.1647086, average batch_loss = 0.14046761836013, Training Error = 0.044921875
Iteration no. 760, lr = 0.1647086, average batch_loss = 0.14945255886712, Training Error = 0.056640625
Iteration no. 761, lr = 0.1647086, average batch_loss = 0.14338764029393, Training Error = 0.046875
Iteration no. 762, lr = 0.1647086, average batch_loss = 0.15174489469537, Training Error = 0.05859375
Iteration no. 763, lr = 0.1647086, average batch_loss = 0.13474916232375, Training Error = 0.04296875
Iteration no. 764, lr = 0.1647086, average batch_loss = 0.11399411649223, Training Error = 0.03515625
Iteration no. 765, lr = 0.1647086, average batch_loss = 0.14686260948495, Training Error = 0.064453125
Iteration no. 766, lr = 0.1647086, average batch_loss = 0.14986364491305, Training Error = 0.052734375
Iteration no. 767, lr = 0.1647086, average batch_loss = 0.15535907052401, Training Error = 0.056640625
Iteration no. 768, lr = 0.1647086, average batch_loss = 0.1477382367653, Training Error = 0.060546875
Iteration no. 769, lr = 0.1647086, average batch_loss = 0.14832437720422, Training Error = 0.0546875
Iteration no. 770, lr = 0.1647086, average batch_loss = 0.16336558190136, Training Error = 0.0703125
Iteration no. 771, lr = 0.1647086, average batch_loss = 0.14386952091581, Training Error = 0.052734375
Iteration no. 772, lr = 0.1647086, average batch_loss = 0.11419798543115, Training Error = 0.029296875
Iteration no. 773, lr = 0.1647086, average batch_loss = 0.12312000103744, Training Error = 0.0390625
Iteration no. 774, lr = 0.1647086, average batch_loss = 0.14983513110368, Training Error = 0.052734375
Iteration no. 775, lr = 0.1647086, average batch_loss = 0.14533250468747, Training Error = 0.044921875
Iteration no. 776, lr = 0.1647086, average batch_loss = 0.16619184586074, Training Error = 0.052734375
Iteration no. 777, lr = 0.1647086, average batch_loss = 0.14053198579651, Training Error = 0.05078125
Iteration no. 778, lr = 0.1647086, average batch_loss = 0.13476113436087, Training Error = 0.041015625
Iteration no. 779, lr = 0.1647086, average batch_loss = 0.14081692626863, Training Error = 0.05078125
Iteration no. 780, lr = 0.1647086, average batch_loss = 0.12516349308145, Training Error = 0.033203125
Iteration no. 781, lr = 0.1647086, average batch_loss = 0.14272397507808, Training Error = 0.052734375
Iteration no. 782, lr = 0.1647086, average batch_loss = 0.1980912728546, Training Error = 0.08203125
Iteration no. 783, lr = 0.1647086, average batch_loss = 0.13970378668316, Training Error = 0.048828125
Iteration no. 784, lr = 0.1647086, average batch_loss = 0.15185173976905, Training Error = 0.05859375
Iteration no. 785, lr = 0.1647086, average batch_loss = 0.14658961318174, Training Error = 0.044921875
Iteration no. 786, lr = 0.1647086, average batch_loss = 0.16715130369932, Training Error = 0.060546875
Iteration no. 787, lr = 0.1647086, average batch_loss = 0.17263914781903, Training Error = 0.060546875
Iteration no. 788, lr = 0.1647086, average batch_loss = 0.13539325340487, Training Error = 0.048828125
Iteration no. 789, lr = 0.1647086, average batch_loss = 0.14424256433139, Training Error = 0.046875
Iteration no. 790, lr = 0.1647086, average batch_loss = 0.11638051613408, Training Error = 0.037109375
Iteration no. 791, lr = 0.1647086, average batch_loss = 0.1333642533642, Training Error = 0.041015625
Iteration no. 792, lr = 0.1647086, average batch_loss = 0.13027523900483, Training Error = 0.046875
Iteration no. 793, lr = 0.1647086, average batch_loss = 0.14417451669268, Training Error = 0.048828125
Iteration no. 794, lr = 0.1647086, average batch_loss = 0.12447004970779, Training Error = 0.029296875
Iteration no. 795, lr = 0.1647086, average batch_loss = 0.11987406701098, Training Error = 0.03515625
Iteration no. 796, lr = 0.1647086, average batch_loss = 0.11687728496745, Training Error = 0.0390625
Iteration no. 797, lr = 0.1647086, average batch_loss = 0.13382610165334, Training Error = 0.048828125
Iteration no. 798, lr = 0.1647086, average batch_loss = 0.14014519109264, Training Error = 0.041015625
Iteration no. 799, lr = 0.1647086, average batch_loss = 0.16974034359647, Training Error = 0.056640625
Iteration no. 800, lr = 0.1647086, average batch_loss = 0.14573932895985, Training Error = 0.05078125
Testing... average test_loss = 0.1426947454114, average test_pred_err = 0.044
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 801, lr = 0.11529602, average batch_loss = 0.17322572605689, Training Error = 0.060546875
Iteration no. 802, lr = 0.11529602, average batch_loss = 0.11889884903934, Training Error = 0.046875
Iteration no. 803, lr = 0.11529602, average batch_loss = 0.1337746444758, Training Error = 0.044921875
Iteration no. 804, lr = 0.11529602, average batch_loss = 0.14138868725525, Training Error = 0.056640625
Iteration no. 805, lr = 0.11529602, average batch_loss = 0.11992530663225, Training Error = 0.0390625
Iteration no. 806, lr = 0.11529602, average batch_loss = 0.14611796075214, Training Error = 0.0546875
Iteration no. 807, lr = 0.11529602, average batch_loss = 0.13571441854128, Training Error = 0.05078125
Iteration no. 808, lr = 0.11529602, average batch_loss = 0.12104020449389, Training Error = 0.04296875
Iteration no. 809, lr = 0.11529602, average batch_loss = 0.194882054367, Training Error = 0.0859375
Iteration no. 810, lr = 0.11529602, average batch_loss = 0.14902408438595, Training Error = 0.060546875
Iteration no. 811, lr = 0.11529602, average batch_loss = 0.14315056918927, Training Error = 0.0546875
Iteration no. 812, lr = 0.11529602, average batch_loss = 0.13870839552692, Training Error = 0.048828125
Iteration no. 813, lr = 0.11529602, average batch_loss = 0.16199220722255, Training Error = 0.060546875
Iteration no. 814, lr = 0.11529602, average batch_loss = 0.10781744481269, Training Error = 0.03515625
Iteration no. 815, lr = 0.11529602, average batch_loss = 0.14923438711464, Training Error = 0.056640625
Iteration no. 816, lr = 0.11529602, average batch_loss = 0.13308056315824, Training Error = 0.048828125
Iteration no. 817, lr = 0.11529602, average batch_loss = 0.1553302362755, Training Error = 0.056640625
Iteration no. 818, lr = 0.11529602, average batch_loss = 0.12641480542816, Training Error = 0.046875
Iteration no. 819, lr = 0.11529602, average batch_loss = 0.14758758976951, Training Error = 0.044921875
Iteration no. 820, lr = 0.11529602, average batch_loss = 0.15124951966147, Training Error = 0.0625
Iteration no. 821, lr = 0.11529602, average batch_loss = 0.12694174386146, Training Error = 0.0390625
Iteration no. 822, lr = 0.11529602, average batch_loss = 0.12013239884124, Training Error = 0.041015625
Iteration no. 823, lr = 0.11529602, average batch_loss = 0.1351109345614, Training Error = 0.044921875
Iteration no. 824, lr = 0.11529602, average batch_loss = 0.1506260855201, Training Error = 0.0546875
Iteration no. 825, lr = 0.11529602, average batch_loss = 0.14367971368865, Training Error = 0.056640625
Iteration no. 826, lr = 0.11529602, average batch_loss = 0.14472944789789, Training Error = 0.052734375
Iteration no. 827, lr = 0.11529602, average batch_loss = 0.13807005293674, Training Error = 0.048828125
Iteration no. 828, lr = 0.11529602, average batch_loss = 0.12551639861678, Training Error = 0.0390625
Iteration no. 829, lr = 0.11529602, average batch_loss = 0.14784507673022, Training Error = 0.046875
Iteration no. 830, lr = 0.11529602, average batch_loss = 0.15752558860879, Training Error = 0.0703125
Iteration no. 831, lr = 0.11529602, average batch_loss = 0.13884289041536, Training Error = 0.048828125
Iteration no. 832, lr = 0.11529602, average batch_loss = 0.1629079525509, Training Error = 0.05859375
Iteration no. 833, lr = 0.11529602, average batch_loss = 0.15365868889503, Training Error = 0.064453125
Iteration no. 834, lr = 0.11529602, average batch_loss = 0.13722120205305, Training Error = 0.05078125
Iteration no. 835, lr = 0.11529602, average batch_loss = 0.14716781408657, Training Error = 0.05078125
Iteration no. 836, lr = 0.11529602, average batch_loss = 0.14364086260691, Training Error = 0.048828125
Iteration no. 837, lr = 0.11529602, average batch_loss = 0.14465655622747, Training Error = 0.05859375
Iteration no. 838, lr = 0.11529602, average batch_loss = 0.11903120030304, Training Error = 0.033203125
Iteration no. 839, lr = 0.11529602, average batch_loss = 0.13302666779343, Training Error = 0.041015625
Iteration no. 840, lr = 0.11529602, average batch_loss = 0.13345983801572, Training Error = 0.04296875
Iteration no. 841, lr = 0.11529602, average batch_loss = 0.11551336592198, Training Error = 0.03125
Iteration no. 842, lr = 0.11529602, average batch_loss = 0.15676514224583, Training Error = 0.056640625
Iteration no. 843, lr = 0.11529602, average batch_loss = 0.12724695099713, Training Error = 0.048828125
Iteration no. 844, lr = 0.11529602, average batch_loss = 0.12345197318806, Training Error = 0.03125
Iteration no. 845, lr = 0.11529602, average batch_loss = 0.11849133064453, Training Error = 0.037109375
Iteration no. 846, lr = 0.11529602, average batch_loss = 0.1415900816657, Training Error = 0.044921875
Iteration no. 847, lr = 0.11529602, average batch_loss = 0.12953326898646, Training Error = 0.04296875
Iteration no. 848, lr = 0.11529602, average batch_loss = 0.14712605170808, Training Error = 0.056640625
Iteration no. 849, lr = 0.11529602, average batch_loss = 0.13851821238831, Training Error = 0.05078125
Iteration no. 850, lr = 0.11529602, average batch_loss = 0.11823392370243, Training Error = 0.03515625
Testing... average test_loss = 0.1398014795007, average test_pred_err = 0.043
Iteration no. 851, lr = 0.11529602, average batch_loss = 0.13956475287901, Training Error = 0.044921875
Iteration no. 852, lr = 0.11529602, average batch_loss = 0.13277317536193, Training Error = 0.046875
Iteration no. 853, lr = 0.11529602, average batch_loss = 0.13482369441291, Training Error = 0.056640625
Iteration no. 854, lr = 0.11529602, average batch_loss = 0.10554225487738, Training Error = 0.03125
Iteration no. 855, lr = 0.11529602, average batch_loss = 0.11837441816326, Training Error = 0.03515625
Iteration no. 856, lr = 0.11529602, average batch_loss = 0.14645915394111, Training Error = 0.0625
Iteration no. 857, lr = 0.11529602, average batch_loss = 0.14455210319544, Training Error = 0.0546875
Iteration no. 858, lr = 0.11529602, average batch_loss = 0.14711358073857, Training Error = 0.052734375
Iteration no. 859, lr = 0.11529602, average batch_loss = 0.13336331837184, Training Error = 0.044921875
Iteration no. 860, lr = 0.11529602, average batch_loss = 0.12348623969936, Training Error = 0.044921875
Iteration no. 861, lr = 0.11529602, average batch_loss = 0.13801326923727, Training Error = 0.04296875
Iteration no. 862, lr = 0.11529602, average batch_loss = 0.12588145014376, Training Error = 0.044921875
Iteration no. 863, lr = 0.11529602, average batch_loss = 0.16779953965934, Training Error = 0.0625
Iteration no. 864, lr = 0.11529602, average batch_loss = 0.14091865426453, Training Error = 0.048828125
Iteration no. 865, lr = 0.11529602, average batch_loss = 0.13546626498582, Training Error = 0.046875
Iteration no. 866, lr = 0.11529602, average batch_loss = 0.13328394527552, Training Error = 0.044921875
Iteration no. 867, lr = 0.11529602, average batch_loss = 0.12626277786285, Training Error = 0.033203125
Iteration no. 868, lr = 0.11529602, average batch_loss = 0.12949613420386, Training Error = 0.046875
Iteration no. 869, lr = 0.11529602, average batch_loss = 0.13131388595382, Training Error = 0.04296875
Iteration no. 870, lr = 0.11529602, average batch_loss = 0.1368154846793, Training Error = 0.05078125
Iteration no. 871, lr = 0.11529602, average batch_loss = 0.14648045329209, Training Error = 0.048828125
Iteration no. 872, lr = 0.11529602, average batch_loss = 0.15358044017777, Training Error = 0.05078125
Iteration no. 873, lr = 0.11529602, average batch_loss = 0.14395131320175, Training Error = 0.05078125
Iteration no. 874, lr = 0.11529602, average batch_loss = 0.13166270973281, Training Error = 0.04296875
Iteration no. 875, lr = 0.11529602, average batch_loss = 0.15995245783255, Training Error = 0.060546875
Iteration no. 876, lr = 0.11529602, average batch_loss = 0.14857190098038, Training Error = 0.056640625
Iteration no. 877, lr = 0.11529602, average batch_loss = 0.12812272326779, Training Error = 0.044921875
Iteration no. 878, lr = 0.11529602, average batch_loss = 0.14200745181496, Training Error = 0.052734375
Iteration no. 879, lr = 0.11529602, average batch_loss = 0.16977850687171, Training Error = 0.060546875
Iteration no. 880, lr = 0.11529602, average batch_loss = 0.14199115981015, Training Error = 0.041015625
Iteration no. 881, lr = 0.11529602, average batch_loss = 0.10688695567182, Training Error = 0.03515625
Iteration no. 882, lr = 0.11529602, average batch_loss = 0.11791245570814, Training Error = 0.046875
Iteration no. 883, lr = 0.11529602, average batch_loss = 0.15091257516021, Training Error = 0.05859375
Iteration no. 884, lr = 0.11529602, average batch_loss = 0.13902080785103, Training Error = 0.0546875
Iteration no. 885, lr = 0.11529602, average batch_loss = 0.11820149394558, Training Error = 0.033203125
Iteration no. 886, lr = 0.11529602, average batch_loss = 0.13712560296085, Training Error = 0.046875
Iteration no. 887, lr = 0.11529602, average batch_loss = 0.14776868428301, Training Error = 0.05078125
Iteration no. 888, lr = 0.11529602, average batch_loss = 0.11276350647866, Training Error = 0.033203125
Iteration no. 889, lr = 0.11529602, average batch_loss = 0.14368322794237, Training Error = 0.0546875
Iteration no. 890, lr = 0.11529602, average batch_loss = 0.12611669439335, Training Error = 0.04296875
Iteration no. 891, lr = 0.11529602, average batch_loss = 0.1426287850619, Training Error = 0.04296875
Iteration no. 892, lr = 0.11529602, average batch_loss = 0.12977999590262, Training Error = 0.048828125
Iteration no. 893, lr = 0.11529602, average batch_loss = 0.13078891298845, Training Error = 0.044921875
Iteration no. 894, lr = 0.11529602, average batch_loss = 0.13521436407925, Training Error = 0.044921875
Iteration no. 895, lr = 0.11529602, average batch_loss = 0.11681060577776, Training Error = 0.03515625
Iteration no. 896, lr = 0.11529602, average batch_loss = 0.12990868798544, Training Error = 0.041015625
Iteration no. 897, lr = 0.11529602, average batch_loss = 0.15464886143309, Training Error = 0.05859375
Iteration no. 898, lr = 0.11529602, average batch_loss = 0.17778671393878, Training Error = 0.072265625
Iteration no. 899, lr = 0.11529602, average batch_loss = 0.14227447879511, Training Error = 0.052734375
Iteration no. 900, lr = 0.11529602, average batch_loss = 0.14118980826016, Training Error = 0.046875
Testing... average test_loss = 0.13439014409984, average test_pred_err = 0.054
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 901, lr = 0.080707214, average batch_loss = 0.12108781314501, Training Error = 0.03515625
Iteration no. 902, lr = 0.080707214, average batch_loss = 0.15747982422314, Training Error = 0.056640625
Iteration no. 903, lr = 0.080707214, average batch_loss = 0.12553695895322, Training Error = 0.041015625
Iteration no. 904, lr = 0.080707214, average batch_loss = 0.1260408009508, Training Error = 0.046875
Iteration no. 905, lr = 0.080707214, average batch_loss = 0.15059572051661, Training Error = 0.056640625
Iteration no. 906, lr = 0.080707214, average batch_loss = 0.15801416420198, Training Error = 0.056640625
Iteration no. 907, lr = 0.080707214, average batch_loss = 0.1094370138204, Training Error = 0.033203125
Iteration no. 908, lr = 0.080707214, average batch_loss = 0.15208800571531, Training Error = 0.064453125
Iteration no. 909, lr = 0.080707214, average batch_loss = 0.13155099634066, Training Error = 0.044921875
Iteration no. 910, lr = 0.080707214, average batch_loss = 0.13482060132393, Training Error = 0.052734375
Iteration no. 911, lr = 0.080707214, average batch_loss = 0.12944932395829, Training Error = 0.0546875
Iteration no. 912, lr = 0.080707214, average batch_loss = 0.13506554131228, Training Error = 0.048828125
Iteration no. 913, lr = 0.080707214, average batch_loss = 0.14532391103238, Training Error = 0.0546875
Iteration no. 914, lr = 0.080707214, average batch_loss = 0.15786217908113, Training Error = 0.064453125
Iteration no. 915, lr = 0.080707214, average batch_loss = 0.12576479565121, Training Error = 0.041015625
Iteration no. 916, lr = 0.080707214, average batch_loss = 0.1392201722819, Training Error = 0.052734375
Iteration no. 917, lr = 0.080707214, average batch_loss = 0.16239574001047, Training Error = 0.060546875
Iteration no. 918, lr = 0.080707214, average batch_loss = 0.15180373877938, Training Error = 0.056640625
Iteration no. 919, lr = 0.080707214, average batch_loss = 0.13593435535987, Training Error = 0.048828125
Iteration no. 920, lr = 0.080707214, average batch_loss = 0.13445329515122, Training Error = 0.046875
Iteration no. 921, lr = 0.080707214, average batch_loss = 0.13941441437306, Training Error = 0.044921875
Iteration no. 922, lr = 0.080707214, average batch_loss = 0.13232565539915, Training Error = 0.041015625
Iteration no. 923, lr = 0.080707214, average batch_loss = 0.12711454458891, Training Error = 0.04296875
Iteration no. 924, lr = 0.080707214, average batch_loss = 0.15607273076197, Training Error = 0.068359375
Iteration no. 925, lr = 0.080707214, average batch_loss = 0.1517671796789, Training Error = 0.056640625
Iteration no. 926, lr = 0.080707214, average batch_loss = 0.12966267350225, Training Error = 0.044921875
Iteration no. 927, lr = 0.080707214, average batch_loss = 0.12752315684118, Training Error = 0.0390625
Iteration no. 928, lr = 0.080707214, average batch_loss = 0.13856425606515, Training Error = 0.0546875
Iteration no. 929, lr = 0.080707214, average batch_loss = 0.12943028585778, Training Error = 0.05078125
Iteration no. 930, lr = 0.080707214, average batch_loss = 0.13340052112277, Training Error = 0.048828125
Iteration no. 931, lr = 0.080707214, average batch_loss = 0.10826353813073, Training Error = 0.02734375
Iteration no. 932, lr = 0.080707214, average batch_loss = 0.1145470567773, Training Error = 0.033203125
Iteration no. 933, lr = 0.080707214, average batch_loss = 0.13177518143655, Training Error = 0.044921875
Iteration no. 934, lr = 0.080707214, average batch_loss = 0.14043411882445, Training Error = 0.056640625
Iteration no. 935, lr = 0.080707214, average batch_loss = 0.13267196057741, Training Error = 0.046875
Iteration no. 936, lr = 0.080707214, average batch_loss = 0.15480170419948, Training Error = 0.05859375
Iteration no. 937, lr = 0.080707214, average batch_loss = 0.13515687468213, Training Error = 0.052734375
Iteration no. 938, lr = 0.080707214, average batch_loss = 0.15422395027732, Training Error = 0.064453125
Iteration no. 939, lr = 0.080707214, average batch_loss = 0.11620440300543, Training Error = 0.03515625
Iteration no. 940, lr = 0.080707214, average batch_loss = 0.14518529389323, Training Error = 0.048828125
Iteration no. 941, lr = 0.080707214, average batch_loss = 0.1109856805496, Training Error = 0.03515625
Iteration no. 942, lr = 0.080707214, average batch_loss = 0.13181225609451, Training Error = 0.0390625
Iteration no. 943, lr = 0.080707214, average batch_loss = 0.14188035801946, Training Error = 0.056640625
Iteration no. 944, lr = 0.080707214, average batch_loss = 0.13127112771318, Training Error = 0.037109375
Iteration no. 945, lr = 0.080707214, average batch_loss = 0.13522687009061, Training Error = 0.05078125
Iteration no. 946, lr = 0.080707214, average batch_loss = 0.13522895374695, Training Error = 0.046875
Iteration no. 947, lr = 0.080707214, average batch_loss = 0.15600371612647, Training Error = 0.05859375
Iteration no. 948, lr = 0.080707214, average batch_loss = 0.13699303759384, Training Error = 0.052734375
Iteration no. 949, lr = 0.080707214, average batch_loss = 0.12903505224186, Training Error = 0.041015625
Iteration no. 950, lr = 0.080707214, average batch_loss = 0.15249504984668, Training Error = 0.0625
Testing... average test_loss = 0.13855365881794, average test_pred_err = 0.057
Iteration no. 951, lr = 0.080707214, average batch_loss = 0.11665016739158, Training Error = 0.046875
Iteration no. 952, lr = 0.080707214, average batch_loss = 0.11113665863105, Training Error = 0.037109375
Iteration no. 953, lr = 0.080707214, average batch_loss = 0.1426242849593, Training Error = 0.044921875
Iteration no. 954, lr = 0.080707214, average batch_loss = 0.12090162679336, Training Error = 0.0390625
Iteration no. 955, lr = 0.080707214, average batch_loss = 0.13278868231569, Training Error = 0.04296875
Iteration no. 956, lr = 0.080707214, average batch_loss = 0.1354943921173, Training Error = 0.052734375
Iteration no. 957, lr = 0.080707214, average batch_loss = 0.14060076518514, Training Error = 0.052734375
Iteration no. 958, lr = 0.080707214, average batch_loss = 0.11319098765309, Training Error = 0.037109375
Iteration no. 959, lr = 0.080707214, average batch_loss = 0.1186390980737, Training Error = 0.04296875
Iteration no. 960, lr = 0.080707214, average batch_loss = 0.1544777024724, Training Error = 0.0703125
Iteration no. 961, lr = 0.080707214, average batch_loss = 0.11441199860148, Training Error = 0.037109375
Iteration no. 962, lr = 0.080707214, average batch_loss = 0.14300777320766, Training Error = 0.048828125
Iteration no. 963, lr = 0.080707214, average batch_loss = 0.1413110333494, Training Error = 0.052734375
Iteration no. 964, lr = 0.080707214, average batch_loss = 0.11498473606592, Training Error = 0.04296875
Iteration no. 965, lr = 0.080707214, average batch_loss = 0.13119312803931, Training Error = 0.05078125
Iteration no. 966, lr = 0.080707214, average batch_loss = 0.12363228834883, Training Error = 0.048828125
Iteration no. 967, lr = 0.080707214, average batch_loss = 0.11888070980385, Training Error = 0.03515625
Iteration no. 968, lr = 0.080707214, average batch_loss = 0.12962256407259, Training Error = 0.04296875
Iteration no. 969, lr = 0.080707214, average batch_loss = 0.14520537518299, Training Error = 0.05078125
Iteration no. 970, lr = 0.080707214, average batch_loss = 0.12725993313879, Training Error = 0.0390625
Iteration no. 971, lr = 0.080707214, average batch_loss = 0.12330387195598, Training Error = 0.044921875
Iteration no. 972, lr = 0.080707214, average batch_loss = 0.13399102908137, Training Error = 0.052734375
Iteration no. 973, lr = 0.080707214, average batch_loss = 0.11297975093589, Training Error = 0.029296875
Iteration no. 974, lr = 0.080707214, average batch_loss = 0.14090816506358, Training Error = 0.046875
Iteration no. 975, lr = 0.080707214, average batch_loss = 0.1149262330548, Training Error = 0.029296875
Iteration no. 976, lr = 0.080707214, average batch_loss = 0.12081767720277, Training Error = 0.037109375
Iteration no. 977, lr = 0.080707214, average batch_loss = 0.14054212963718, Training Error = 0.052734375
Iteration no. 978, lr = 0.080707214, average batch_loss = 0.12844722199573, Training Error = 0.03125
Iteration no. 979, lr = 0.080707214, average batch_loss = 0.14263627539633, Training Error = 0.0546875
Iteration no. 980, lr = 0.080707214, average batch_loss = 0.14530924616143, Training Error = 0.052734375
Iteration no. 981, lr = 0.080707214, average batch_loss = 0.15093750189319, Training Error = 0.052734375
Iteration no. 982, lr = 0.080707214, average batch_loss = 0.11218203201779, Training Error = 0.037109375
Iteration no. 983, lr = 0.080707214, average batch_loss = 0.11671536952503, Training Error = 0.033203125
Iteration no. 984, lr = 0.080707214, average batch_loss = 0.14079354609246, Training Error = 0.05078125
Iteration no. 985, lr = 0.080707214, average batch_loss = 0.13984064579417, Training Error = 0.052734375
Iteration no. 986, lr = 0.080707214, average batch_loss = 0.12589054731628, Training Error = 0.046875
Iteration no. 987, lr = 0.080707214, average batch_loss = 0.1339050893395, Training Error = 0.05078125
Iteration no. 988, lr = 0.080707214, average batch_loss = 0.14620599840038, Training Error = 0.052734375
Iteration no. 989, lr = 0.080707214, average batch_loss = 0.12190755856597, Training Error = 0.041015625
Iteration no. 990, lr = 0.080707214, average batch_loss = 0.14477345252753, Training Error = 0.046875
Iteration no. 991, lr = 0.080707214, average batch_loss = 0.14211971191142, Training Error = 0.048828125
Iteration no. 992, lr = 0.080707214, average batch_loss = 0.13881067342078, Training Error = 0.0546875
Iteration no. 993, lr = 0.080707214, average batch_loss = 0.15809187973267, Training Error = 0.06640625
Iteration no. 994, lr = 0.080707214, average batch_loss = 0.14352613991101, Training Error = 0.05078125
Iteration no. 995, lr = 0.080707214, average batch_loss = 0.1106346488114, Training Error = 0.033203125
Iteration no. 996, lr = 0.080707214, average batch_loss = 0.12449136463461, Training Error = 0.04296875
Iteration no. 997, lr = 0.080707214, average batch_loss = 0.15824208516072, Training Error = 0.0625
Iteration no. 998, lr = 0.080707214, average batch_loss = 0.15079255872681, Training Error = 0.06640625
Iteration no. 999, lr = 0.080707214, average batch_loss = 0.16762678109035, Training Error = 0.072265625
Iteration no. 1000, lr = 0.080707214, average batch_loss = 0.12829786985941, Training Error = 0.048828125
Testing... average test_loss = 0.12490025592362, average test_pred_err = 0.04
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 1001, lr = 0.0564950498, average batch_loss = 0.13719303512916, Training Error = 0.046875
Iteration no. 1002, lr = 0.0564950498, average batch_loss = 0.10839691708971, Training Error = 0.037109375
Iteration no. 1003, lr = 0.0564950498, average batch_loss = 0.15344435222859, Training Error = 0.056640625
Iteration no. 1004, lr = 0.0564950498, average batch_loss = 0.11350611019425, Training Error = 0.0390625
Iteration no. 1005, lr = 0.0564950498, average batch_loss = 0.1531710297338, Training Error = 0.0625
Iteration no. 1006, lr = 0.0564950498, average batch_loss = 0.14742068334342, Training Error = 0.060546875
Iteration no. 1007, lr = 0.0564950498, average batch_loss = 0.12723907715346, Training Error = 0.044921875
Iteration no. 1008, lr = 0.0564950498, average batch_loss = 0.13843939733407, Training Error = 0.048828125
Iteration no. 1009, lr = 0.0564950498, average batch_loss = 0.13426447045512, Training Error = 0.046875
Iteration no. 1010, lr = 0.0564950498, average batch_loss = 0.13803314156514, Training Error = 0.046875
Iteration no. 1011, lr = 0.0564950498, average batch_loss = 0.1589357227032, Training Error = 0.0625
Iteration no. 1012, lr = 0.0564950498, average batch_loss = 0.13506938608246, Training Error = 0.05078125
Iteration no. 1013, lr = 0.0564950498, average batch_loss = 0.13100504560462, Training Error = 0.044921875
Iteration no. 1014, lr = 0.0564950498, average batch_loss = 0.1269663212786, Training Error = 0.044921875
Iteration no. 1015, lr = 0.0564950498, average batch_loss = 0.15270127656522, Training Error = 0.06640625
Iteration no. 1016, lr = 0.0564950498, average batch_loss = 0.13625897788774, Training Error = 0.052734375
Iteration no. 1017, lr = 0.0564950498, average batch_loss = 0.15076595825892, Training Error = 0.05078125
Iteration no. 1018, lr = 0.0564950498, average batch_loss = 0.12713055900741, Training Error = 0.041015625
Iteration no. 1019, lr = 0.0564950498, average batch_loss = 0.12743889686177, Training Error = 0.04296875
Iteration no. 1020, lr = 0.0564950498, average batch_loss = 0.12220454388511, Training Error = 0.0390625
Iteration no. 1021, lr = 0.0564950498, average batch_loss = 0.12888772673409, Training Error = 0.0546875
Iteration no. 1022, lr = 0.0564950498, average batch_loss = 0.14525271619757, Training Error = 0.05859375
Iteration no. 1023, lr = 0.0564950498, average batch_loss = 0.13092950609704, Training Error = 0.048828125
Iteration no. 1024, lr = 0.0564950498, average batch_loss = 0.12716781054143, Training Error = 0.0390625
Iteration no. 1025, lr = 0.0564950498, average batch_loss = 0.1555179977953, Training Error = 0.0625
Iteration no. 1026, lr = 0.0564950498, average batch_loss = 0.11932456685739, Training Error = 0.0390625
Iteration no. 1027, lr = 0.0564950498, average batch_loss = 0.13318223015334, Training Error = 0.046875
Iteration no. 1028, lr = 0.0564950498, average batch_loss = 0.13641899952625, Training Error = 0.04296875
Iteration no. 1029, lr = 0.0564950498, average batch_loss = 0.12449932666082, Training Error = 0.044921875
Iteration no. 1030, lr = 0.0564950498, average batch_loss = 0.13101458809118, Training Error = 0.04296875
Iteration no. 1031, lr = 0.0564950498, average batch_loss = 0.12822473742195, Training Error = 0.046875
Iteration no. 1032, lr = 0.0564950498, average batch_loss = 0.13509117066269, Training Error = 0.056640625
Iteration no. 1033, lr = 0.0564950498, average batch_loss = 0.15057252533446, Training Error = 0.056640625
Iteration no. 1034, lr = 0.0564950498, average batch_loss = 0.13119956446003, Training Error = 0.046875
Iteration no. 1035, lr = 0.0564950498, average batch_loss = 0.13026204401117, Training Error = 0.046875
Iteration no. 1036, lr = 0.0564950498, average batch_loss = 0.15932151445907, Training Error = 0.07421875
Iteration no. 1037, lr = 0.0564950498, average batch_loss = 0.12711128973291, Training Error = 0.0390625
Iteration no. 1038, lr = 0.0564950498, average batch_loss = 0.13149697364854, Training Error = 0.041015625
Iteration no. 1039, lr = 0.0564950498, average batch_loss = 0.12180770345233, Training Error = 0.04296875
Iteration no. 1040, lr = 0.0564950498, average batch_loss = 0.13168987540821, Training Error = 0.0390625
Iteration no. 1041, lr = 0.0564950498, average batch_loss = 0.11656283299119, Training Error = 0.041015625
Iteration no. 1042, lr = 0.0564950498, average batch_loss = 0.11794855753812, Training Error = 0.03515625
Iteration no. 1043, lr = 0.0564950498, average batch_loss = 0.14468125968496, Training Error = 0.05859375
Iteration no. 1044, lr = 0.0564950498, average batch_loss = 0.12796963862173, Training Error = 0.048828125
Iteration no. 1045, lr = 0.0564950498, average batch_loss = 0.13399627099783, Training Error = 0.044921875
Iteration no. 1046, lr = 0.0564950498, average batch_loss = 0.12971038102194, Training Error = 0.046875
Iteration no. 1047, lr = 0.0564950498, average batch_loss = 0.12187386377068, Training Error = 0.03515625
Iteration no. 1048, lr = 0.0564950498, average batch_loss = 0.11808728538702, Training Error = 0.0390625
Iteration no. 1049, lr = 0.0564950498, average batch_loss = 0.12073884794977, Training Error = 0.033203125
Iteration no. 1050, lr = 0.0564950498, average batch_loss = 0.11865450153444, Training Error = 0.041015625
Testing... average test_loss = 0.14011800393075, average test_pred_err = 0.048
Iteration no. 1051, lr = 0.0564950498, average batch_loss = 0.11996546448074, Training Error = 0.0390625
Iteration no. 1052, lr = 0.0564950498, average batch_loss = 0.14538512091543, Training Error = 0.044921875
Iteration no. 1053, lr = 0.0564950498, average batch_loss = 0.13731246016983, Training Error = 0.05078125
Iteration no. 1054, lr = 0.0564950498, average batch_loss = 0.12392832920549, Training Error = 0.0390625
Iteration no. 1055, lr = 0.0564950498, average batch_loss = 0.11327732230411, Training Error = 0.03515625
Iteration no. 1056, lr = 0.0564950498, average batch_loss = 0.13799940377263, Training Error = 0.046875
Iteration no. 1057, lr = 0.0564950498, average batch_loss = 0.15020642902356, Training Error = 0.064453125
Iteration no. 1058, lr = 0.0564950498, average batch_loss = 0.13068384311976, Training Error = 0.046875
Iteration no. 1059, lr = 0.0564950498, average batch_loss = 0.12797868240925, Training Error = 0.041015625
Iteration no. 1060, lr = 0.0564950498, average batch_loss = 0.13546772288867, Training Error = 0.05078125
Iteration no. 1061, lr = 0.0564950498, average batch_loss = 0.12188100483215, Training Error = 0.041015625
Iteration no. 1062, lr = 0.0564950498, average batch_loss = 0.12512499828002, Training Error = 0.046875
Iteration no. 1063, lr = 0.0564950498, average batch_loss = 0.12579494324177, Training Error = 0.046875
Iteration no. 1064, lr = 0.0564950498, average batch_loss = 0.1294504786513, Training Error = 0.044921875
Iteration no. 1065, lr = 0.0564950498, average batch_loss = 0.11259285877762, Training Error = 0.033203125
Iteration no. 1066, lr = 0.0564950498, average batch_loss = 0.14165672474578, Training Error = 0.05859375
Iteration no. 1067, lr = 0.0564950498, average batch_loss = 0.11773045250504, Training Error = 0.0390625
Iteration no. 1068, lr = 0.0564950498, average batch_loss = 0.13416862697145, Training Error = 0.052734375
Iteration no. 1069, lr = 0.0564950498, average batch_loss = 0.14318258450442, Training Error = 0.048828125
Iteration no. 1070, lr = 0.0564950498, average batch_loss = 0.11978249066018, Training Error = 0.041015625
Iteration no. 1071, lr = 0.0564950498, average batch_loss = 0.12497555274778, Training Error = 0.0390625
Iteration no. 1072, lr = 0.0564950498, average batch_loss = 0.12332792469211, Training Error = 0.04296875
Iteration no. 1073, lr = 0.0564950498, average batch_loss = 0.11402165473403, Training Error = 0.0390625
Iteration no. 1074, lr = 0.0564950498, average batch_loss = 0.14749704318168, Training Error = 0.05859375
Iteration no. 1075, lr = 0.0564950498, average batch_loss = 0.13947036139202, Training Error = 0.0546875
Iteration no. 1076, lr = 0.0564950498, average batch_loss = 0.16988688082041, Training Error = 0.076171875
Iteration no. 1077, lr = 0.0564950498, average batch_loss = 0.13163072535218, Training Error = 0.044921875
Iteration no. 1078, lr = 0.0564950498, average batch_loss = 0.13031814680106, Training Error = 0.037109375
Iteration no. 1079, lr = 0.0564950498, average batch_loss = 0.11649034250247, Training Error = 0.03515625
Iteration no. 1080, lr = 0.0564950498, average batch_loss = 0.1253353923685, Training Error = 0.041015625
Iteration no. 1081, lr = 0.0564950498, average batch_loss = 0.11992340178631, Training Error = 0.03515625
Iteration no. 1082, lr = 0.0564950498, average batch_loss = 0.098448291452341, Training Error = 0.0234375
Iteration no. 1083, lr = 0.0564950498, average batch_loss = 0.13056340355303, Training Error = 0.04296875
Iteration no. 1084, lr = 0.0564950498, average batch_loss = 0.13541052754402, Training Error = 0.044921875
Iteration no. 1085, lr = 0.0564950498, average batch_loss = 0.12650666157908, Training Error = 0.048828125
Iteration no. 1086, lr = 0.0564950498, average batch_loss = 0.11781551925786, Training Error = 0.0390625
Iteration no. 1087, lr = 0.0564950498, average batch_loss = 0.12532564178385, Training Error = 0.037109375
Iteration no. 1088, lr = 0.0564950498, average batch_loss = 0.12003922642961, Training Error = 0.0390625
Iteration no. 1089, lr = 0.0564950498, average batch_loss = 0.12175812696357, Training Error = 0.041015625
Iteration no. 1090, lr = 0.0564950498, average batch_loss = 0.14438978654712, Training Error = 0.056640625
Iteration no. 1091, lr = 0.0564950498, average batch_loss = 0.14296774449896, Training Error = 0.052734375
Iteration no. 1092, lr = 0.0564950498, average batch_loss = 0.14646181112501, Training Error = 0.0625
Iteration no. 1093, lr = 0.0564950498, average batch_loss = 0.10318761374542, Training Error = 0.033203125
Iteration no. 1094, lr = 0.0564950498, average batch_loss = 0.109856341818, Training Error = 0.025390625
Iteration no. 1095, lr = 0.0564950498, average batch_loss = 0.1306949136704, Training Error = 0.05078125
Iteration no. 1096, lr = 0.0564950498, average batch_loss = 0.13262336742673, Training Error = 0.05078125
Iteration no. 1097, lr = 0.0564950498, average batch_loss = 0.13253109399335, Training Error = 0.05078125
Iteration no. 1098, lr = 0.0564950498, average batch_loss = 0.1386391915303, Training Error = 0.048828125
Iteration no. 1099, lr = 0.0564950498, average batch_loss = 0.14373870066251, Training Error = 0.0546875
Iteration no. 1100, lr = 0.0564950498, average batch_loss = 0.12628776114798, Training Error = 0.04296875
Testing... average test_loss = 0.11807956643468, average test_pred_err = 0.038
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 1101, lr = 0.03954653486, average batch_loss = 0.11650978578401, Training Error = 0.037109375
Iteration no. 1102, lr = 0.03954653486, average batch_loss = 0.12626984939806, Training Error = 0.044921875
Iteration no. 1103, lr = 0.03954653486, average batch_loss = 0.14705669387063, Training Error = 0.0546875
Iteration no. 1104, lr = 0.03954653486, average batch_loss = 0.11504284842257, Training Error = 0.041015625
Iteration no. 1105, lr = 0.03954653486, average batch_loss = 0.13346605044585, Training Error = 0.0546875
Iteration no. 1106, lr = 0.03954653486, average batch_loss = 0.17123889946853, Training Error = 0.076171875
Iteration no. 1107, lr = 0.03954653486, average batch_loss = 0.12783758056939, Training Error = 0.048828125
Iteration no. 1108, lr = 0.03954653486, average batch_loss = 0.13964390786434, Training Error = 0.05859375
Iteration no. 1109, lr = 0.03954653486, average batch_loss = 0.12502975166728, Training Error = 0.041015625
Iteration no. 1110, lr = 0.03954653486, average batch_loss = 0.11246226824413, Training Error = 0.037109375
Iteration no. 1111, lr = 0.03954653486, average batch_loss = 0.14293674388974, Training Error = 0.048828125
Iteration no. 1112, lr = 0.03954653486, average batch_loss = 0.1429821389031, Training Error = 0.052734375
Iteration no. 1113, lr = 0.03954653486, average batch_loss = 0.13688612982323, Training Error = 0.05859375
Iteration no. 1114, lr = 0.03954653486, average batch_loss = 0.14811773715867, Training Error = 0.0625
Iteration no. 1115, lr = 0.03954653486, average batch_loss = 0.12674517666608, Training Error = 0.041015625
Iteration no. 1116, lr = 0.03954653486, average batch_loss = 0.15962202518774, Training Error = 0.0703125
Iteration no. 1117, lr = 0.03954653486, average batch_loss = 0.13283710013278, Training Error = 0.041015625
Iteration no. 1118, lr = 0.03954653486, average batch_loss = 0.12569280057873, Training Error = 0.04296875
Iteration no. 1119, lr = 0.03954653486, average batch_loss = 0.13888708819896, Training Error = 0.05078125
Iteration no. 1120, lr = 0.03954653486, average batch_loss = 0.11668618285033, Training Error = 0.044921875
Iteration no. 1121, lr = 0.03954653486, average batch_loss = 0.11741880976038, Training Error = 0.03515625
Iteration no. 1122, lr = 0.03954653486, average batch_loss = 0.11967706595072, Training Error = 0.041015625
Iteration no. 1123, lr = 0.03954653486, average batch_loss = 0.12572600631079, Training Error = 0.04296875
Iteration no. 1124, lr = 0.03954653486, average batch_loss = 0.12191925306159, Training Error = 0.03515625
Iteration no. 1125, lr = 0.03954653486, average batch_loss = 0.12566941927892, Training Error = 0.044921875
Iteration no. 1126, lr = 0.03954653486, average batch_loss = 0.13847282609663, Training Error = 0.046875
Iteration no. 1127, lr = 0.03954653486, average batch_loss = 0.15590654383802, Training Error = 0.0703125
Iteration no. 1128, lr = 0.03954653486, average batch_loss = 0.10699868917335, Training Error = 0.03125
Iteration no. 1129, lr = 0.03954653486, average batch_loss = 0.10464369332513, Training Error = 0.03515625
Iteration no. 1130, lr = 0.03954653486, average batch_loss = 0.13247283864709, Training Error = 0.046875
Iteration no. 1131, lr = 0.03954653486, average batch_loss = 0.14120484190794, Training Error = 0.05859375
Iteration no. 1132, lr = 0.03954653486, average batch_loss = 0.11428489678796, Training Error = 0.037109375
Iteration no. 1133, lr = 0.03954653486, average batch_loss = 0.11926815927193, Training Error = 0.041015625
Iteration no. 1134, lr = 0.03954653486, average batch_loss = 0.1551597616605, Training Error = 0.060546875
Iteration no. 1135, lr = 0.03954653486, average batch_loss = 0.14760006545601, Training Error = 0.05859375
Iteration no. 1136, lr = 0.03954653486, average batch_loss = 0.13076219902501, Training Error = 0.046875
Iteration no. 1137, lr = 0.03954653486, average batch_loss = 0.12489607021509, Training Error = 0.046875
Iteration no. 1138, lr = 0.03954653486, average batch_loss = 0.11276172599507, Training Error = 0.033203125
Iteration no. 1139, lr = 0.03954653486, average batch_loss = 0.097869026098115, Training Error = 0.0234375
Iteration no. 1140, lr = 0.03954653486, average batch_loss = 0.12420243293023, Training Error = 0.044921875
Iteration no. 1141, lr = 0.03954653486, average batch_loss = 0.11841421017422, Training Error = 0.041015625
Iteration no. 1142, lr = 0.03954653486, average batch_loss = 0.12844666844554, Training Error = 0.048828125
Iteration no. 1143, lr = 0.03954653486, average batch_loss = 0.1148752591787, Training Error = 0.037109375
Iteration no. 1144, lr = 0.03954653486, average batch_loss = 0.14390581083681, Training Error = 0.056640625
Iteration no. 1145, lr = 0.03954653486, average batch_loss = 0.13397872415122, Training Error = 0.041015625
Iteration no. 1146, lr = 0.03954653486, average batch_loss = 0.12152178185857, Training Error = 0.04296875
Iteration no. 1147, lr = 0.03954653486, average batch_loss = 0.13183443284004, Training Error = 0.041015625
Iteration no. 1148, lr = 0.03954653486, average batch_loss = 0.14235804224048, Training Error = 0.0546875
Iteration no. 1149, lr = 0.03954653486, average batch_loss = 0.14612490145885, Training Error = 0.05078125
Iteration no. 1150, lr = 0.03954653486, average batch_loss = 0.14567766115836, Training Error = 0.064453125
Testing... average test_loss = 0.11894624232605, average test_pred_err = 0.032
Iteration no. 1151, lr = 0.03954653486, average batch_loss = 0.13292850559927, Training Error = 0.044921875
Iteration no. 1152, lr = 0.03954653486, average batch_loss = 0.10792371383231, Training Error = 0.03125
Iteration no. 1153, lr = 0.03954653486, average batch_loss = 0.10700579462228, Training Error = 0.03125
Iteration no. 1154, lr = 0.03954653486, average batch_loss = 0.12642183887608, Training Error = 0.04296875
Iteration no. 1155, lr = 0.03954653486, average batch_loss = 0.14795879309136, Training Error = 0.060546875
Iteration no. 1156, lr = 0.03954653486, average batch_loss = 0.11877695998512, Training Error = 0.041015625
Iteration no. 1157, lr = 0.03954653486, average batch_loss = 0.12443339256191, Training Error = 0.046875
Iteration no. 1158, lr = 0.03954653486, average batch_loss = 0.13740239103524, Training Error = 0.05859375
Iteration no. 1159, lr = 0.03954653486, average batch_loss = 0.099860891105457, Training Error = 0.025390625
Iteration no. 1160, lr = 0.03954653486, average batch_loss = 0.1256591335927, Training Error = 0.04296875
Iteration no. 1161, lr = 0.03954653486, average batch_loss = 0.12052807826587, Training Error = 0.0390625
Iteration no. 1162, lr = 0.03954653486, average batch_loss = 0.11566513465585, Training Error = 0.03515625
Iteration no. 1163, lr = 0.03954653486, average batch_loss = 0.12710142974482, Training Error = 0.046875
Iteration no. 1164, lr = 0.03954653486, average batch_loss = 0.13129934958036, Training Error = 0.052734375
Iteration no. 1165, lr = 0.03954653486, average batch_loss = 0.13849462785115, Training Error = 0.046875
Iteration no. 1166, lr = 0.03954653486, average batch_loss = 0.11282680023251, Training Error = 0.033203125
Iteration no. 1167, lr = 0.03954653486, average batch_loss = 0.12145796533032, Training Error = 0.041015625
Iteration no. 1168, lr = 0.03954653486, average batch_loss = 0.12273323185784, Training Error = 0.037109375
Iteration no. 1169, lr = 0.03954653486, average batch_loss = 0.12951489049308, Training Error = 0.056640625
Iteration no. 1170, lr = 0.03954653486, average batch_loss = 0.13204789171773, Training Error = 0.046875
Iteration no. 1171, lr = 0.03954653486, average batch_loss = 0.13275642462084, Training Error = 0.044921875
Iteration no. 1172, lr = 0.03954653486, average batch_loss = 0.14263595805431, Training Error = 0.0546875
Iteration no. 1173, lr = 0.03954653486, average batch_loss = 0.14869821728453, Training Error = 0.0625
Iteration no. 1174, lr = 0.03954653486, average batch_loss = 0.14925401654611, Training Error = 0.056640625
Iteration no. 1175, lr = 0.03954653486, average batch_loss = 0.13142908927241, Training Error = 0.04296875
Iteration no. 1176, lr = 0.03954653486, average batch_loss = 0.11515067156519, Training Error = 0.037109375
Iteration no. 1177, lr = 0.03954653486, average batch_loss = 0.1218580681172, Training Error = 0.03515625
Iteration no. 1178, lr = 0.03954653486, average batch_loss = 0.13711091599246, Training Error = 0.05078125
Iteration no. 1179, lr = 0.03954653486, average batch_loss = 0.12842779589835, Training Error = 0.048828125
Iteration no. 1180, lr = 0.03954653486, average batch_loss = 0.15247647276028, Training Error = 0.06640625
Iteration no. 1181, lr = 0.03954653486, average batch_loss = 0.13432250706917, Training Error = 0.044921875
Iteration no. 1182, lr = 0.03954653486, average batch_loss = 0.13217884393426, Training Error = 0.048828125
Iteration no. 1183, lr = 0.03954653486, average batch_loss = 0.12865994054328, Training Error = 0.046875
Iteration no. 1184, lr = 0.03954653486, average batch_loss = 0.14217622441775, Training Error = 0.046875
Iteration no. 1185, lr = 0.03954653486, average batch_loss = 0.11624422572112, Training Error = 0.033203125
Iteration no. 1186, lr = 0.03954653486, average batch_loss = 0.12626551838526, Training Error = 0.04296875
Iteration no. 1187, lr = 0.03954653486, average batch_loss = 0.1206780469345, Training Error = 0.03515625
Iteration no. 1188, lr = 0.03954653486, average batch_loss = 0.14185328271861, Training Error = 0.048828125
Iteration no. 1189, lr = 0.03954653486, average batch_loss = 0.12874257022567, Training Error = 0.046875
Iteration no. 1190, lr = 0.03954653486, average batch_loss = 0.13958852832438, Training Error = 0.056640625
Iteration no. 1191, lr = 0.03954653486, average batch_loss = 0.14555984253009, Training Error = 0.044921875
Iteration no. 1192, lr = 0.03954653486, average batch_loss = 0.11511169346924, Training Error = 0.041015625
Iteration no. 1193, lr = 0.03954653486, average batch_loss = 0.12964798588456, Training Error = 0.044921875
Iteration no. 1194, lr = 0.03954653486, average batch_loss = 0.11511122082531, Training Error = 0.037109375
Iteration no. 1195, lr = 0.03954653486, average batch_loss = 0.12870020516661, Training Error = 0.05078125
Iteration no. 1196, lr = 0.03954653486, average batch_loss = 0.12722585095922, Training Error = 0.044921875
Iteration no. 1197, lr = 0.03954653486, average batch_loss = 0.12642564152323, Training Error = 0.048828125
Iteration no. 1198, lr = 0.03954653486, average batch_loss = 0.12478849901883, Training Error = 0.0390625
Iteration no. 1199, lr = 0.03954653486, average batch_loss = 0.13322078943277, Training Error = 0.048828125
Iteration no. 1200, lr = 0.03954653486, average batch_loss = 0.13970049570431, Training Error = 0.05859375
Testing... average test_loss = 0.13148475341223, average test_pred_err = 0.049
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 1201, lr = 0.027682574402, average batch_loss = 0.11726395393634, Training Error = 0.0390625
Iteration no. 1202, lr = 0.027682574402, average batch_loss = 0.096830281423143, Training Error = 0.0234375
Iteration no. 1203, lr = 0.027682574402, average batch_loss = 0.15213897810418, Training Error = 0.072265625
Iteration no. 1204, lr = 0.027682574402, average batch_loss = 0.12007534120819, Training Error = 0.0390625
Iteration no. 1205, lr = 0.027682574402, average batch_loss = 0.10085303089463, Training Error = 0.02734375
Iteration no. 1206, lr = 0.027682574402, average batch_loss = 0.12399048722443, Training Error = 0.046875
Iteration no. 1207, lr = 0.027682574402, average batch_loss = 0.12537796292128, Training Error = 0.048828125
Iteration no. 1208, lr = 0.027682574402, average batch_loss = 0.11247287847739, Training Error = 0.03515625
Iteration no. 1209, lr = 0.027682574402, average batch_loss = 0.14405308444767, Training Error = 0.0625
Iteration no. 1210, lr = 0.027682574402, average batch_loss = 0.12608875012637, Training Error = 0.041015625
Iteration no. 1211, lr = 0.027682574402, average batch_loss = 0.14339363972339, Training Error = 0.060546875
Iteration no. 1212, lr = 0.027682574402, average batch_loss = 0.1392970509601, Training Error = 0.052734375
Iteration no. 1213, lr = 0.027682574402, average batch_loss = 0.11207464939008, Training Error = 0.03515625
Iteration no. 1214, lr = 0.027682574402, average batch_loss = 0.12233321125007, Training Error = 0.041015625
Iteration no. 1215, lr = 0.027682574402, average batch_loss = 0.15196590377201, Training Error = 0.05859375
Iteration no. 1216, lr = 0.027682574402, average batch_loss = 0.14478508243122, Training Error = 0.0546875
Iteration no. 1217, lr = 0.027682574402, average batch_loss = 0.14371320255342, Training Error = 0.060546875
Iteration no. 1218, lr = 0.027682574402, average batch_loss = 0.13594398622897, Training Error = 0.064453125
Iteration no. 1219, lr = 0.027682574402, average batch_loss = 0.12258841261456, Training Error = 0.03515625
Iteration no. 1220, lr = 0.027682574402, average batch_loss = 0.12622066473932, Training Error = 0.05078125
Iteration no. 1221, lr = 0.027682574402, average batch_loss = 0.12543040775893, Training Error = 0.041015625
Iteration no. 1222, lr = 0.027682574402, average batch_loss = 0.13796344782955, Training Error = 0.056640625
Iteration no. 1223, lr = 0.027682574402, average batch_loss = 0.11490800070066, Training Error = 0.037109375
Iteration no. 1224, lr = 0.027682574402, average batch_loss = 0.12136702618898, Training Error = 0.03515625
Iteration no. 1225, lr = 0.027682574402, average batch_loss = 0.13347724733894, Training Error = 0.052734375
Iteration no. 1226, lr = 0.027682574402, average batch_loss = 0.1301610977145, Training Error = 0.05078125
Iteration no. 1227, lr = 0.027682574402, average batch_loss = 0.11999592812934, Training Error = 0.03515625
Iteration no. 1228, lr = 0.027682574402, average batch_loss = 0.1248740558792, Training Error = 0.05078125
Iteration no. 1229, lr = 0.027682574402, average batch_loss = 0.15384778604183, Training Error = 0.060546875
Iteration no. 1230, lr = 0.027682574402, average batch_loss = 0.13945046000704, Training Error = 0.05078125
Iteration no. 1231, lr = 0.027682574402, average batch_loss = 0.10771467664489, Training Error = 0.033203125
Iteration no. 1232, lr = 0.027682574402, average batch_loss = 0.11635045309524, Training Error = 0.041015625
Iteration no. 1233, lr = 0.027682574402, average batch_loss = 0.11462921076302, Training Error = 0.0390625
Iteration no. 1234, lr = 0.027682574402, average batch_loss = 0.14357150354822, Training Error = 0.0546875
Iteration no. 1235, lr = 0.027682574402, average batch_loss = 0.096790274393182, Training Error = 0.02734375
Iteration no. 1236, lr = 0.027682574402, average batch_loss = 0.1335336265824, Training Error = 0.05078125
Iteration no. 1237, lr = 0.027682574402, average batch_loss = 0.12548152156404, Training Error = 0.044921875
Iteration no. 1238, lr = 0.027682574402, average batch_loss = 0.1266765383938, Training Error = 0.044921875
Iteration no. 1239, lr = 0.027682574402, average batch_loss = 0.13022690119276, Training Error = 0.048828125
Iteration no. 1240, lr = 0.027682574402, average batch_loss = 0.13602268188972, Training Error = 0.05078125
Iteration no. 1241, lr = 0.027682574402, average batch_loss = 0.13584172634988, Training Error = 0.05859375
Iteration no. 1242, lr = 0.027682574402, average batch_loss = 0.1041312595743, Training Error = 0.025390625
Iteration no. 1243, lr = 0.027682574402, average batch_loss = 0.14137918233412, Training Error = 0.048828125
Iteration no. 1244, lr = 0.027682574402, average batch_loss = 0.13134014018885, Training Error = 0.05078125
Iteration no. 1245, lr = 0.027682574402, average batch_loss = 0.13676900695275, Training Error = 0.044921875
Iteration no. 1246, lr = 0.027682574402, average batch_loss = 0.13293046772078, Training Error = 0.046875
Iteration no. 1247, lr = 0.027682574402, average batch_loss = 0.13879194073995, Training Error = 0.060546875
Iteration no. 1248, lr = 0.027682574402, average batch_loss = 0.12208503950697, Training Error = 0.041015625
Iteration no. 1249, lr = 0.027682574402, average batch_loss = 0.1194211299551, Training Error = 0.037109375
Iteration no. 1250, lr = 0.027682574402, average batch_loss = 0.12093794687998, Training Error = 0.044921875
Testing... average test_loss = 0.12669549623666, average test_pred_err = 0.041
Iteration no. 1251, lr = 0.027682574402, average batch_loss = 0.13071290091639, Training Error = 0.052734375
Iteration no. 1252, lr = 0.027682574402, average batch_loss = 0.13788591576171, Training Error = 0.052734375
Iteration no. 1253, lr = 0.027682574402, average batch_loss = 0.12531172762652, Training Error = 0.04296875
Iteration no. 1254, lr = 0.027682574402, average batch_loss = 0.12811106807939, Training Error = 0.044921875
Iteration no. 1255, lr = 0.027682574402, average batch_loss = 0.13206990260522, Training Error = 0.041015625
Iteration no. 1256, lr = 0.027682574402, average batch_loss = 0.14869739656935, Training Error = 0.0546875
Iteration no. 1257, lr = 0.027682574402, average batch_loss = 0.11749711381245, Training Error = 0.04296875
Iteration no. 1258, lr = 0.027682574402, average batch_loss = 0.11425115691074, Training Error = 0.033203125
Iteration no. 1259, lr = 0.027682574402, average batch_loss = 0.14022897737232, Training Error = 0.046875
Iteration no. 1260, lr = 0.027682574402, average batch_loss = 0.094177415683804, Training Error = 0.025390625
Iteration no. 1261, lr = 0.027682574402, average batch_loss = 0.11596474402717, Training Error = 0.037109375
Iteration no. 1262, lr = 0.027682574402, average batch_loss = 0.12329530589329, Training Error = 0.05078125
Iteration no. 1263, lr = 0.027682574402, average batch_loss = 0.12301656666653, Training Error = 0.044921875
Iteration no. 1264, lr = 0.027682574402, average batch_loss = 0.13293350123617, Training Error = 0.052734375
Iteration no. 1265, lr = 0.027682574402, average batch_loss = 0.12254673920738, Training Error = 0.037109375
Iteration no. 1266, lr = 0.027682574402, average batch_loss = 0.12327156220444, Training Error = 0.04296875
Iteration no. 1267, lr = 0.027682574402, average batch_loss = 0.14412295522097, Training Error = 0.064453125
Iteration no. 1268, lr = 0.027682574402, average batch_loss = 0.14099053097837, Training Error = 0.05859375
Iteration no. 1269, lr = 0.027682574402, average batch_loss = 0.11497558379621, Training Error = 0.041015625
Iteration no. 1270, lr = 0.027682574402, average batch_loss = 0.11145678933329, Training Error = 0.03515625
Iteration no. 1271, lr = 0.027682574402, average batch_loss = 0.13241635558659, Training Error = 0.060546875
Iteration no. 1272, lr = 0.027682574402, average batch_loss = 0.13481052527593, Training Error = 0.05078125
Iteration no. 1273, lr = 0.027682574402, average batch_loss = 0.13181392521723, Training Error = 0.05078125
Iteration no. 1274, lr = 0.027682574402, average batch_loss = 0.14851912664832, Training Error = 0.0625
Iteration no. 1275, lr = 0.027682574402, average batch_loss = 0.1169018033433, Training Error = 0.046875
Iteration no. 1276, lr = 0.027682574402, average batch_loss = 0.11817444826062, Training Error = 0.04296875
Iteration no. 1277, lr = 0.027682574402, average batch_loss = 0.11740913210192, Training Error = 0.04296875
Iteration no. 1278, lr = 0.027682574402, average batch_loss = 0.12174581393222, Training Error = 0.046875
Iteration no. 1279, lr = 0.027682574402, average batch_loss = 0.14359522255728, Training Error = 0.0546875
Iteration no. 1280, lr = 0.027682574402, average batch_loss = 0.14550157861972, Training Error = 0.0546875
Iteration no. 1281, lr = 0.027682574402, average batch_loss = 0.10336006685812, Training Error = 0.02734375
Iteration no. 1282, lr = 0.027682574402, average batch_loss = 0.10509088531233, Training Error = 0.03125
Iteration no. 1283, lr = 0.027682574402, average batch_loss = 0.13384079247072, Training Error = 0.05078125
Iteration no. 1284, lr = 0.027682574402, average batch_loss = 0.113401853254, Training Error = 0.0390625
Iteration no. 1285, lr = 0.027682574402, average batch_loss = 0.15087138393857, Training Error = 0.05859375
Iteration no. 1286, lr = 0.027682574402, average batch_loss = 0.13499648698387, Training Error = 0.04296875
Iteration no. 1287, lr = 0.027682574402, average batch_loss = 0.12051944222726, Training Error = 0.044921875
Iteration no. 1288, lr = 0.027682574402, average batch_loss = 0.14949207232287, Training Error = 0.05859375
Iteration no. 1289, lr = 0.027682574402, average batch_loss = 0.12986695477041, Training Error = 0.046875
Iteration no. 1290, lr = 0.027682574402, average batch_loss = 0.13837090430813, Training Error = 0.0546875
Iteration no. 1291, lr = 0.027682574402, average batch_loss = 0.12931026400749, Training Error = 0.048828125
Iteration no. 1292, lr = 0.027682574402, average batch_loss = 0.12063264838613, Training Error = 0.0390625
Iteration no. 1293, lr = 0.027682574402, average batch_loss = 0.14687539429658, Training Error = 0.0546875
Iteration no. 1294, lr = 0.027682574402, average batch_loss = 0.13078106288807, Training Error = 0.041015625
Iteration no. 1295, lr = 0.027682574402, average batch_loss = 0.12650376855102, Training Error = 0.041015625
Iteration no. 1296, lr = 0.027682574402, average batch_loss = 0.12407830793681, Training Error = 0.046875
Iteration no. 1297, lr = 0.027682574402, average batch_loss = 0.13302243322178, Training Error = 0.052734375
Iteration no. 1298, lr = 0.027682574402, average batch_loss = 0.10706670525862, Training Error = 0.033203125
Iteration no. 1299, lr = 0.027682574402, average batch_loss = 0.12990573479281, Training Error = 0.044921875
Iteration no. 1300, lr = 0.027682574402, average batch_loss = 0.12237266747742, Training Error = 0.0390625
Testing... average test_loss = 0.13829567196908, average test_pred_err = 0.051
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 1301, lr = 0.0193778020814, average batch_loss = 0.13202486501525, Training Error = 0.05078125
Iteration no. 1302, lr = 0.0193778020814, average batch_loss = 0.11988246148968, Training Error = 0.044921875
Iteration no. 1303, lr = 0.0193778020814, average batch_loss = 0.12990544400002, Training Error = 0.046875
Iteration no. 1304, lr = 0.0193778020814, average batch_loss = 0.1420123822836, Training Error = 0.0546875
Iteration no. 1305, lr = 0.0193778020814, average batch_loss = 0.12651563028247, Training Error = 0.046875
Iteration no. 1306, lr = 0.0193778020814, average batch_loss = 0.15850236541751, Training Error = 0.068359375
Iteration no. 1307, lr = 0.0193778020814, average batch_loss = 0.13633015417811, Training Error = 0.05078125
Iteration no. 1308, lr = 0.0193778020814, average batch_loss = 0.13310145542856, Training Error = 0.048828125
Iteration no. 1309, lr = 0.0193778020814, average batch_loss = 0.14150132905828, Training Error = 0.048828125
Iteration no. 1310, lr = 0.0193778020814, average batch_loss = 0.13224380664868, Training Error = 0.044921875
Iteration no. 1311, lr = 0.0193778020814, average batch_loss = 0.14445993510397, Training Error = 0.052734375
Iteration no. 1312, lr = 0.0193778020814, average batch_loss = 0.11810554489518, Training Error = 0.037109375
Iteration no. 1313, lr = 0.0193778020814, average batch_loss = 0.14375011264689, Training Error = 0.0546875
Iteration no. 1314, lr = 0.0193778020814, average batch_loss = 0.12396869425055, Training Error = 0.044921875
Iteration no. 1315, lr = 0.0193778020814, average batch_loss = 0.10670665278032, Training Error = 0.033203125
Iteration no. 1316, lr = 0.0193778020814, average batch_loss = 0.10680656402809, Training Error = 0.02734375
Iteration no. 1317, lr = 0.0193778020814, average batch_loss = 0.11138548908858, Training Error = 0.03125
Iteration no. 1318, lr = 0.0193778020814, average batch_loss = 0.12739039452347, Training Error = 0.05078125
Iteration no. 1319, lr = 0.0193778020814, average batch_loss = 0.13605638867089, Training Error = 0.0546875
Iteration no. 1320, lr = 0.0193778020814, average batch_loss = 0.1083701177615, Training Error = 0.033203125
Iteration no. 1321, lr = 0.0193778020814, average batch_loss = 0.12918595362868, Training Error = 0.044921875
Iteration no. 1322, lr = 0.0193778020814, average batch_loss = 0.1253947133628, Training Error = 0.044921875
Iteration no. 1323, lr = 0.0193778020814, average batch_loss = 0.11973492344925, Training Error = 0.044921875
Iteration no. 1324, lr = 0.0193778020814, average batch_loss = 0.13663134085373, Training Error = 0.044921875
Iteration no. 1325, lr = 0.0193778020814, average batch_loss = 0.12471299891799, Training Error = 0.048828125
Iteration no. 1326, lr = 0.0193778020814, average batch_loss = 0.135134545454, Training Error = 0.056640625
Iteration no. 1327, lr = 0.0193778020814, average batch_loss = 0.14317000881311, Training Error = 0.048828125
Iteration no. 1328, lr = 0.0193778020814, average batch_loss = 0.14704513147349, Training Error = 0.06640625
Iteration no. 1329, lr = 0.0193778020814, average batch_loss = 0.1245182004076, Training Error = 0.041015625
Iteration no. 1330, lr = 0.0193778020814, average batch_loss = 0.11694124003624, Training Error = 0.03515625
Iteration no. 1331, lr = 0.0193778020814, average batch_loss = 0.12934807648421, Training Error = 0.048828125
Iteration no. 1332, lr = 0.0193778020814, average batch_loss = 0.13564034908354, Training Error = 0.048828125
Iteration no. 1333, lr = 0.0193778020814, average batch_loss = 0.11338755536217, Training Error = 0.044921875
Iteration no. 1334, lr = 0.0193778020814, average batch_loss = 0.14574714031389, Training Error = 0.056640625
Iteration no. 1335, lr = 0.0193778020814, average batch_loss = 0.13508728601518, Training Error = 0.044921875
Iteration no. 1336, lr = 0.0193778020814, average batch_loss = 0.1240688324938, Training Error = 0.041015625
Iteration no. 1337, lr = 0.0193778020814, average batch_loss = 0.14640139999903, Training Error = 0.05859375
Iteration no. 1338, lr = 0.0193778020814, average batch_loss = 0.12672464008803, Training Error = 0.048828125
Iteration no. 1339, lr = 0.0193778020814, average batch_loss = 0.11956218897105, Training Error = 0.033203125
Iteration no. 1340, lr = 0.0193778020814, average batch_loss = 0.14666356341286, Training Error = 0.052734375
Iteration no. 1341, lr = 0.0193778020814, average batch_loss = 0.10250806898243, Training Error = 0.029296875
Iteration no. 1342, lr = 0.0193778020814, average batch_loss = 0.13657568232717, Training Error = 0.056640625
Iteration no. 1343, lr = 0.0193778020814, average batch_loss = 0.150390395839, Training Error = 0.064453125
Iteration no. 1344, lr = 0.0193778020814, average batch_loss = 0.12338820137638, Training Error = 0.04296875
Iteration no. 1345, lr = 0.0193778020814, average batch_loss = 0.1127910246542, Training Error = 0.0390625
Iteration no. 1346, lr = 0.0193778020814, average batch_loss = 0.13367827239368, Training Error = 0.056640625
Iteration no. 1347, lr = 0.0193778020814, average batch_loss = 0.12699581747366, Training Error = 0.044921875
Iteration no. 1348, lr = 0.0193778020814, average batch_loss = 0.11404954408115, Training Error = 0.03515625
Iteration no. 1349, lr = 0.0193778020814, average batch_loss = 0.11557126138659, Training Error = 0.037109375
Iteration no. 1350, lr = 0.0193778020814, average batch_loss = 0.12574914833198, Training Error = 0.046875
Testing... average test_loss = 0.12530337728005, average test_pred_err = 0.045
Iteration no. 1351, lr = 0.0193778020814, average batch_loss = 0.12479956696209, Training Error = 0.041015625
Iteration no. 1352, lr = 0.0193778020814, average batch_loss = 0.1314253039404, Training Error = 0.044921875
Iteration no. 1353, lr = 0.0193778020814, average batch_loss = 0.13266651336865, Training Error = 0.04296875
Iteration no. 1354, lr = 0.0193778020814, average batch_loss = 0.11141602206563, Training Error = 0.041015625
Iteration no. 1355, lr = 0.0193778020814, average batch_loss = 0.10315698422395, Training Error = 0.0234375
Iteration no. 1356, lr = 0.0193778020814, average batch_loss = 0.11715881349295, Training Error = 0.0390625
Iteration no. 1357, lr = 0.0193778020814, average batch_loss = 0.15244909325717, Training Error = 0.056640625
Iteration no. 1358, lr = 0.0193778020814, average batch_loss = 0.11565435384111, Training Error = 0.041015625
Iteration no. 1359, lr = 0.0193778020814, average batch_loss = 0.12998233642607, Training Error = 0.041015625
Iteration no. 1360, lr = 0.0193778020814, average batch_loss = 0.12674868562732, Training Error = 0.04296875
Iteration no. 1361, lr = 0.0193778020814, average batch_loss = 0.13744315742556, Training Error = 0.05078125
Iteration no. 1362, lr = 0.0193778020814, average batch_loss = 0.13336092623562, Training Error = 0.046875
Iteration no. 1363, lr = 0.0193778020814, average batch_loss = 0.13693697874232, Training Error = 0.056640625
Iteration no. 1364, lr = 0.0193778020814, average batch_loss = 0.11987400769762, Training Error = 0.04296875
Iteration no. 1365, lr = 0.0193778020814, average batch_loss = 0.13188206214713, Training Error = 0.046875
Iteration no. 1366, lr = 0.0193778020814, average batch_loss = 0.13297426124301, Training Error = 0.046875
Iteration no. 1367, lr = 0.0193778020814, average batch_loss = 0.11192792521218, Training Error = 0.033203125
Iteration no. 1368, lr = 0.0193778020814, average batch_loss = 0.11609623354283, Training Error = 0.041015625
Iteration no. 1369, lr = 0.0193778020814, average batch_loss = 0.11577710592013, Training Error = 0.0390625
Iteration no. 1370, lr = 0.0193778020814, average batch_loss = 0.11600420906063, Training Error = 0.037109375
Iteration no. 1371, lr = 0.0193778020814, average batch_loss = 0.13018169481394, Training Error = 0.048828125
Iteration no. 1372, lr = 0.0193778020814, average batch_loss = 0.1199245623962, Training Error = 0.046875
Iteration no. 1373, lr = 0.0193778020814, average batch_loss = 0.11599596180857, Training Error = 0.041015625
Iteration no. 1374, lr = 0.0193778020814, average batch_loss = 0.12374141682345, Training Error = 0.052734375
Iteration no. 1375, lr = 0.0193778020814, average batch_loss = 0.12877983802457, Training Error = 0.05078125
Iteration no. 1376, lr = 0.0193778020814, average batch_loss = 0.10216691168449, Training Error = 0.02734375
Iteration no. 1377, lr = 0.0193778020814, average batch_loss = 0.16268984475186, Training Error = 0.06640625
Iteration no. 1378, lr = 0.0193778020814, average batch_loss = 0.1162705158649, Training Error = 0.037109375
Iteration no. 1379, lr = 0.0193778020814, average batch_loss = 0.11372617872982, Training Error = 0.041015625
Iteration no. 1380, lr = 0.0193778020814, average batch_loss = 0.13856082400337, Training Error = 0.0546875
Iteration no. 1381, lr = 0.0193778020814, average batch_loss = 0.1144717866859, Training Error = 0.037109375
Iteration no. 1382, lr = 0.0193778020814, average batch_loss = 0.13628355981041, Training Error = 0.046875
Iteration no. 1383, lr = 0.0193778020814, average batch_loss = 0.13710353415794, Training Error = 0.048828125
Iteration no. 1384, lr = 0.0193778020814, average batch_loss = 0.1260573875754, Training Error = 0.044921875
Iteration no. 1385, lr = 0.0193778020814, average batch_loss = 0.12096040173711, Training Error = 0.0390625
Iteration no. 1386, lr = 0.0193778020814, average batch_loss = 0.13156896953087, Training Error = 0.048828125
Iteration no. 1387, lr = 0.0193778020814, average batch_loss = 0.11560674491484, Training Error = 0.041015625
Iteration no. 1388, lr = 0.0193778020814, average batch_loss = 0.11217066434504, Training Error = 0.044921875
Iteration no. 1389, lr = 0.0193778020814, average batch_loss = 0.13361405126095, Training Error = 0.05078125
Iteration no. 1390, lr = 0.0193778020814, average batch_loss = 0.13725032044816, Training Error = 0.0546875
Iteration no. 1391, lr = 0.0193778020814, average batch_loss = 0.13779412904871, Training Error = 0.05078125
Iteration no. 1392, lr = 0.0193778020814, average batch_loss = 0.12964216168808, Training Error = 0.041015625
Iteration no. 1393, lr = 0.0193778020814, average batch_loss = 0.12268429547154, Training Error = 0.0390625
Iteration no. 1394, lr = 0.0193778020814, average batch_loss = 0.13966314364394, Training Error = 0.05078125
Iteration no. 1395, lr = 0.0193778020814, average batch_loss = 0.1366506645275, Training Error = 0.048828125
Iteration no. 1396, lr = 0.0193778020814, average batch_loss = 0.14943091105488, Training Error = 0.0625
Iteration no. 1397, lr = 0.0193778020814, average batch_loss = 0.10478387203192, Training Error = 0.029296875
Iteration no. 1398, lr = 0.0193778020814, average batch_loss = 0.14586854292137, Training Error = 0.05859375
Iteration no. 1399, lr = 0.0193778020814, average batch_loss = 0.12511765490141, Training Error = 0.0390625
Iteration no. 1400, lr = 0.0193778020814, average batch_loss = 0.12049367617181, Training Error = 0.044921875
Testing... average test_loss = 0.11015309541506, average test_pred_err = 0.037
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 1401, lr = 0.01356446145698, average batch_loss = 0.11796616018146, Training Error = 0.041015625
Iteration no. 1402, lr = 0.01356446145698, average batch_loss = 0.10885803658237, Training Error = 0.037109375
Iteration no. 1403, lr = 0.01356446145698, average batch_loss = 0.10413569866334, Training Error = 0.029296875
Iteration no. 1404, lr = 0.01356446145698, average batch_loss = 0.12897719330385, Training Error = 0.044921875
Iteration no. 1405, lr = 0.01356446145698, average batch_loss = 0.13739358635586, Training Error = 0.044921875
Iteration no. 1406, lr = 0.01356446145698, average batch_loss = 0.13155817163502, Training Error = 0.046875
Iteration no. 1407, lr = 0.01356446145698, average batch_loss = 0.14499349978905, Training Error = 0.0546875
Iteration no. 1408, lr = 0.01356446145698, average batch_loss = 0.10689983638501, Training Error = 0.029296875
Iteration no. 1409, lr = 0.01356446145698, average batch_loss = 0.14672554013862, Training Error = 0.060546875
Iteration no. 1410, lr = 0.01356446145698, average batch_loss = 0.13344811501851, Training Error = 0.046875
Iteration no. 1411, lr = 0.01356446145698, average batch_loss = 0.13660708091308, Training Error = 0.046875
Iteration no. 1412, lr = 0.01356446145698, average batch_loss = 0.11070432290338, Training Error = 0.0390625
Iteration no. 1413, lr = 0.01356446145698, average batch_loss = 0.12451168038595, Training Error = 0.044921875
Iteration no. 1414, lr = 0.01356446145698, average batch_loss = 0.15777141775407, Training Error = 0.060546875
Iteration no. 1415, lr = 0.01356446145698, average batch_loss = 0.14852761659838, Training Error = 0.060546875
Iteration no. 1416, lr = 0.01356446145698, average batch_loss = 0.12553646978368, Training Error = 0.0390625
Iteration no. 1417, lr = 0.01356446145698, average batch_loss = 0.14672660752975, Training Error = 0.05078125
Iteration no. 1418, lr = 0.01356446145698, average batch_loss = 0.13544372515227, Training Error = 0.04296875
Iteration no. 1419, lr = 0.01356446145698, average batch_loss = 0.10392948764423, Training Error = 0.025390625
Iteration no. 1420, lr = 0.01356446145698, average batch_loss = 0.12059371921155, Training Error = 0.0390625
Iteration no. 1421, lr = 0.01356446145698, average batch_loss = 0.10816745189227, Training Error = 0.03515625
Iteration no. 1422, lr = 0.01356446145698, average batch_loss = 0.11910004064153, Training Error = 0.041015625
Iteration no. 1423, lr = 0.01356446145698, average batch_loss = 0.15045647219996, Training Error = 0.060546875
Iteration no. 1424, lr = 0.01356446145698, average batch_loss = 0.13107841532697, Training Error = 0.05078125
Iteration no. 1425, lr = 0.01356446145698, average batch_loss = 0.12930017781908, Training Error = 0.04296875
Iteration no. 1426, lr = 0.01356446145698, average batch_loss = 0.12747153420606, Training Error = 0.0390625
Iteration no. 1427, lr = 0.01356446145698, average batch_loss = 0.1137284041133, Training Error = 0.02734375
Iteration no. 1428, lr = 0.01356446145698, average batch_loss = 0.11693475559472, Training Error = 0.033203125
Iteration no. 1429, lr = 0.01356446145698, average batch_loss = 0.13024669423221, Training Error = 0.05078125
Iteration no. 1430, lr = 0.01356446145698, average batch_loss = 0.10032769977319, Training Error = 0.029296875
Iteration no. 1431, lr = 0.01356446145698, average batch_loss = 0.12113185690976, Training Error = 0.041015625
Iteration no. 1432, lr = 0.01356446145698, average batch_loss = 0.11398715660833, Training Error = 0.03515625
Iteration no. 1433, lr = 0.01356446145698, average batch_loss = 0.11948433926093, Training Error = 0.03125
Iteration no. 1434, lr = 0.01356446145698, average batch_loss = 0.13902508377897, Training Error = 0.048828125
Iteration no. 1435, lr = 0.01356446145698, average batch_loss = 0.14812415287821, Training Error = 0.056640625
Iteration no. 1436, lr = 0.01356446145698, average batch_loss = 0.11205437018819, Training Error = 0.033203125
Iteration no. 1437, lr = 0.01356446145698, average batch_loss = 0.13094413712249, Training Error = 0.05078125
Iteration no. 1438, lr = 0.01356446145698, average batch_loss = 0.11875119932072, Training Error = 0.044921875
Iteration no. 1439, lr = 0.01356446145698, average batch_loss = 0.11288512514364, Training Error = 0.029296875
Iteration no. 1440, lr = 0.01356446145698, average batch_loss = 0.12907052576531, Training Error = 0.048828125
Iteration no. 1441, lr = 0.01356446145698, average batch_loss = 0.12582530734332, Training Error = 0.044921875
Iteration no. 1442, lr = 0.01356446145698, average batch_loss = 0.12389117386692, Training Error = 0.0390625
Iteration no. 1443, lr = 0.01356446145698, average batch_loss = 0.12555653747069, Training Error = 0.046875
Iteration no. 1444, lr = 0.01356446145698, average batch_loss = 0.12273924979744, Training Error = 0.04296875
Iteration no. 1445, lr = 0.01356446145698, average batch_loss = 0.13749707201843, Training Error = 0.048828125
Iteration no. 1446, lr = 0.01356446145698, average batch_loss = 0.11378162194578, Training Error = 0.03515625
Iteration no. 1447, lr = 0.01356446145698, average batch_loss = 0.13096724999984, Training Error = 0.05078125
Iteration no. 1448, lr = 0.01356446145698, average batch_loss = 0.10960475758434, Training Error = 0.037109375
Iteration no. 1449, lr = 0.01356446145698, average batch_loss = 0.10177725196634, Training Error = 0.0234375
Iteration no. 1450, lr = 0.01356446145698, average batch_loss = 0.11735029213867, Training Error = 0.041015625
Testing... average test_loss = 0.130787711912, average test_pred_err = 0.046
Iteration no. 1451, lr = 0.01356446145698, average batch_loss = 0.13653848230831, Training Error = 0.05078125
Iteration no. 1452, lr = 0.01356446145698, average batch_loss = 0.14913231131767, Training Error = 0.068359375
Iteration no. 1453, lr = 0.01356446145698, average batch_loss = 0.12139229884354, Training Error = 0.04296875
Iteration no. 1454, lr = 0.01356446145698, average batch_loss = 0.12529436649745, Training Error = 0.04296875
Iteration no. 1455, lr = 0.01356446145698, average batch_loss = 0.1252087249731, Training Error = 0.05078125
Iteration no. 1456, lr = 0.01356446145698, average batch_loss = 0.13128654491551, Training Error = 0.046875
Iteration no. 1457, lr = 0.01356446145698, average batch_loss = 0.12516886346489, Training Error = 0.037109375
Iteration no. 1458, lr = 0.01356446145698, average batch_loss = 0.11101505710188, Training Error = 0.0390625
Iteration no. 1459, lr = 0.01356446145698, average batch_loss = 0.12984190990464, Training Error = 0.0546875
Iteration no. 1460, lr = 0.01356446145698, average batch_loss = 0.11761106076527, Training Error = 0.03515625
Iteration no. 1461, lr = 0.01356446145698, average batch_loss = 0.12192304926355, Training Error = 0.044921875
Iteration no. 1462, lr = 0.01356446145698, average batch_loss = 0.12431787213981, Training Error = 0.046875
Iteration no. 1463, lr = 0.01356446145698, average batch_loss = 0.11947751367402, Training Error = 0.037109375
Iteration no. 1464, lr = 0.01356446145698, average batch_loss = 0.15538033046429, Training Error = 0.064453125
Iteration no. 1465, lr = 0.01356446145698, average batch_loss = 0.13833920812672, Training Error = 0.046875
Iteration no. 1466, lr = 0.01356446145698, average batch_loss = 0.14966222593149, Training Error = 0.056640625
Iteration no. 1467, lr = 0.01356446145698, average batch_loss = 0.12096591518028, Training Error = 0.046875
Iteration no. 1468, lr = 0.01356446145698, average batch_loss = 0.13115924310384, Training Error = 0.046875
Iteration no. 1469, lr = 0.01356446145698, average batch_loss = 0.12104062206931, Training Error = 0.037109375
Iteration no. 1470, lr = 0.01356446145698, average batch_loss = 0.11242416018864, Training Error = 0.0390625
Iteration no. 1471, lr = 0.01356446145698, average batch_loss = 0.12522064024519, Training Error = 0.048828125
Iteration no. 1472, lr = 0.01356446145698, average batch_loss = 0.14345340487434, Training Error = 0.052734375
Iteration no. 1473, lr = 0.01356446145698, average batch_loss = 0.133171061941, Training Error = 0.048828125
Iteration no. 1474, lr = 0.01356446145698, average batch_loss = 0.12947380155734, Training Error = 0.046875
Iteration no. 1475, lr = 0.01356446145698, average batch_loss = 0.12220262071047, Training Error = 0.04296875
Iteration no. 1476, lr = 0.01356446145698, average batch_loss = 0.12721716433945, Training Error = 0.046875
Iteration no. 1477, lr = 0.01356446145698, average batch_loss = 0.13573179133344, Training Error = 0.046875
Iteration no. 1478, lr = 0.01356446145698, average batch_loss = 0.13067970015999, Training Error = 0.044921875
Iteration no. 1479, lr = 0.01356446145698, average batch_loss = 0.13170626029303, Training Error = 0.05078125
Iteration no. 1480, lr = 0.01356446145698, average batch_loss = 0.12846838472123, Training Error = 0.037109375
Iteration no. 1481, lr = 0.01356446145698, average batch_loss = 0.12629171565356, Training Error = 0.044921875
Iteration no. 1482, lr = 0.01356446145698, average batch_loss = 0.10630727377673, Training Error = 0.033203125
Iteration no. 1483, lr = 0.01356446145698, average batch_loss = 0.14510419470566, Training Error = 0.048828125
Iteration no. 1484, lr = 0.01356446145698, average batch_loss = 0.12915868421186, Training Error = 0.05078125
Iteration no. 1485, lr = 0.01356446145698, average batch_loss = 0.11243838315431, Training Error = 0.033203125
Iteration no. 1486, lr = 0.01356446145698, average batch_loss = 0.1314747460503, Training Error = 0.052734375
Iteration no. 1487, lr = 0.01356446145698, average batch_loss = 0.11891915796832, Training Error = 0.0390625
Iteration no. 1488, lr = 0.01356446145698, average batch_loss = 0.11682453541709, Training Error = 0.041015625
Iteration no. 1489, lr = 0.01356446145698, average batch_loss = 0.11494359085385, Training Error = 0.03515625
Iteration no. 1490, lr = 0.01356446145698, average batch_loss = 0.12367005619328, Training Error = 0.046875
Iteration no. 1491, lr = 0.01356446145698, average batch_loss = 0.10668076506241, Training Error = 0.0234375
Iteration no. 1492, lr = 0.01356446145698, average batch_loss = 0.12987661833641, Training Error = 0.05078125
Iteration no. 1493, lr = 0.01356446145698, average batch_loss = 0.1106179105246, Training Error = 0.041015625
Iteration no. 1494, lr = 0.01356446145698, average batch_loss = 0.16223087607043, Training Error = 0.0703125
Iteration no. 1495, lr = 0.01356446145698, average batch_loss = 0.11348289143099, Training Error = 0.0390625
Iteration no. 1496, lr = 0.01356446145698, average batch_loss = 0.14714912190088, Training Error = 0.0546875
Iteration no. 1497, lr = 0.01356446145698, average batch_loss = 0.1347547971582, Training Error = 0.048828125
Iteration no. 1498, lr = 0.01356446145698, average batch_loss = 0.13351846560787, Training Error = 0.0546875
Iteration no. 1499, lr = 0.01356446145698, average batch_loss = 0.12501552838573, Training Error = 0.04296875
Iteration no. 1500, lr = 0.01356446145698, average batch_loss = 0.12683954885908, Training Error = 0.046875
Testing... average test_loss = 0.11547298562214, average test_pred_err = 0.033
Snapshotting B_model... done
Snapshotting C_model... done
Iteration no. 1501, lr = 0.009495123019886, average batch_loss = 0.11850637498983, Training Error = 0.0390625
Iteration no. 1502, lr = 0.009495123019886, average batch_loss = 0.12791078110745, Training Error = 0.046875
Iteration no. 1503, lr = 0.009495123019886, average batch_loss = 0.11560823651336, Training Error = 0.04296875
Iteration no. 1504, lr = 0.009495123019886, average batch_loss = 0.14481264885721, Training Error = 0.06640625
