Loading pretrained model... done
Testing... average test_loss = 6.8737384969377, average test_pred_err = 0.456
Iteration no. 1, lr = 2, attribute lr =0.02, average batch_loss = 8.2180102632527, Training Error = 0.494140625
Iteration no. 2, lr = 2, attribute lr =0.02, average batch_loss = 5.088813400174, Training Error = 0.546875
Iteration no. 3, lr = 2, attribute lr =0.02, average batch_loss = 1.5999645886911, Training Error = 0.49609375
Iteration no. 4, lr = 2, attribute lr =0.02, average batch_loss = 2.1799362263749, Training Error = 0.490234375
Iteration no. 5, lr = 2, attribute lr =0.02, average batch_loss = 1.7560472236752, Training Error = 0.490234375
Iteration no. 6, lr = 2, attribute lr =0.02, average batch_loss = 1.2149376861166, Training Error = 0.5078125
Iteration no. 7, lr = 2, attribute lr =0.02, average batch_loss = 1.3072991738473, Training Error = 0.47265625
Iteration no. 8, lr = 2, attribute lr =0.02, average batch_loss = 1.2475439957914, Training Error = 0.525390625
Iteration no. 9, lr = 2, attribute lr =0.02, average batch_loss = 1.3892398924499, Training Error = 0.484375
Iteration no. 10, lr = 2, attribute lr =0.02, average batch_loss = 0.94015452675893, Training Error = 0.439453125
Iteration no. 11, lr = 2, attribute lr =0.02, average batch_loss = 0.97719351671361, Training Error = 0.458984375
Iteration no. 12, lr = 2, attribute lr =0.02, average batch_loss = 0.88643733484565, Training Error = 0.46875
Iteration no. 13, lr = 2, attribute lr =0.02, average batch_loss = 1.0421606032151, Training Error = 0.474609375
Iteration no. 14, lr = 2, attribute lr =0.02, average batch_loss = 1.0768274869687, Training Error = 0.53125
Iteration no. 15, lr = 2, attribute lr =0.02, average batch_loss = 1.221407620496, Training Error = 0.49609375
Iteration no. 16, lr = 2, attribute lr =0.02, average batch_loss = 0.81072808681378, Training Error = 0.458984375
Iteration no. 17, lr = 2, attribute lr =0.02, average batch_loss = 0.80629994845649, Training Error = 0.4140625
Iteration no. 18, lr = 2, attribute lr =0.02, average batch_loss = 0.78480123367955, Training Error = 0.451171875
Iteration no. 19, lr = 2, attribute lr =0.02, average batch_loss = 0.93190368664287, Training Error = 0.48046875
Iteration no. 20, lr = 2, attribute lr =0.02, average batch_loss = 0.97535065263514, Training Error = 0.482421875
Iteration no. 21, lr = 2, attribute lr =0.02, average batch_loss = 0.95024166859366, Training Error = 0.48046875
Iteration no. 22, lr = 2, attribute lr =0.02, average batch_loss = 0.76807565955925, Training Error = 0.43359375
Iteration no. 23, lr = 2, attribute lr =0.02, average batch_loss = 0.79811281020914, Training Error = 0.435546875
Iteration no. 24, lr = 2, attribute lr =0.02, average batch_loss = 0.77065652820459, Training Error = 0.478515625
Iteration no. 25, lr = 2, attribute lr =0.02, average batch_loss = 0.76090899587925, Training Error = 0.42578125
Iteration no. 26, lr = 2, attribute lr =0.02, average batch_loss = 0.71549647987039, Training Error = 0.416015625
Iteration no. 27, lr = 2, attribute lr =0.02, average batch_loss = 0.7323966958121, Training Error = 0.443359375
Iteration no. 28, lr = 2, attribute lr =0.02, average batch_loss = 0.71162622009614, Training Error = 0.421875
Iteration no. 29, lr = 2, attribute lr =0.02, average batch_loss = 0.70002147299833, Training Error = 0.4375
Iteration no. 30, lr = 2, attribute lr =0.02, average batch_loss = 0.69115337352641, Training Error = 0.4375
Iteration no. 31, lr = 2, attribute lr =0.02, average batch_loss = 0.78988834141925, Training Error = 0.509765625
Iteration no. 32, lr = 2, attribute lr =0.02, average batch_loss = 0.84582246785924, Training Error = 0.4765625
Iteration no. 33, lr = 2, attribute lr =0.02, average batch_loss = 0.87443933770948, Training Error = 0.515625
Iteration no. 34, lr = 2, attribute lr =0.02, average batch_loss = 0.83361985595512, Training Error = 0.509765625
Iteration no. 35, lr = 2, attribute lr =0.02, average batch_loss = 0.76992573900746, Training Error = 0.453125
Iteration no. 36, lr = 2, attribute lr =0.02, average batch_loss = 0.70543130473527, Training Error = 0.419921875
Iteration no. 37, lr = 2, attribute lr =0.02, average batch_loss = 0.69429656810854, Training Error = 0.421875
Iteration no. 38, lr = 2, attribute lr =0.02, average batch_loss = 0.65929463960653, Training Error = 0.375
Iteration no. 39, lr = 2, attribute lr =0.02, average batch_loss = 0.63226187364073, Training Error = 0.373046875
Iteration no. 40, lr = 2, attribute lr =0.02, average batch_loss = 0.66336952944778, Training Error = 0.392578125
Iteration no. 41, lr = 2, attribute lr =0.02, average batch_loss = 0.668963324567, Training Error = 0.39453125
Iteration no. 42, lr = 2, attribute lr =0.02, average batch_loss = 0.68254278638375, Training Error = 0.404296875
Iteration no. 43, lr = 2, attribute lr =0.02, average batch_loss = 0.59300839546457, Training Error = 0.3203125
Iteration no. 44, lr = 2, attribute lr =0.02, average batch_loss = 0.63244220267908, Training Error = 0.384765625
Iteration no. 45, lr = 2, attribute lr =0.02, average batch_loss = 0.63257290635126, Training Error = 0.33984375
Iteration no. 46, lr = 2, attribute lr =0.02, average batch_loss = 0.65152825811958, Training Error = 0.37890625
Iteration no. 47, lr = 2, attribute lr =0.02, average batch_loss = 0.61718951412333, Training Error = 0.33984375
Iteration no. 48, lr = 2, attribute lr =0.02, average batch_loss = 0.64492808454799, Training Error = 0.37890625
Iteration no. 49, lr = 2, attribute lr =0.02, average batch_loss = 0.64309014473235, Training Error = 0.384765625
Iteration no. 50, lr = 2, attribute lr =0.02, average batch_loss = 0.65781972428625, Training Error = 0.400390625
Testing... average test_loss = 0.72403621871645, average test_pred_err = 0.488
Iteration no. 51, lr = 2, attribute lr =0.02, average batch_loss = 0.64671093571333, Training Error = 0.39453125
Iteration no. 52, lr = 2, attribute lr =0.02, average batch_loss = 0.66359345848639, Training Error = 0.37109375
Iteration no. 53, lr = 2, attribute lr =0.02, average batch_loss = 0.65969581528765, Training Error = 0.390625
Iteration no. 54, lr = 2, attribute lr =0.02, average batch_loss = 0.6712915012909, Training Error = 0.416015625
Iteration no. 55, lr = 2, attribute lr =0.02, average batch_loss = 0.62613396879271, Training Error = 0.369140625
Iteration no. 56, lr = 2, attribute lr =0.02, average batch_loss = 0.65417252634889, Training Error = 0.400390625
Iteration no. 57, lr = 2, attribute lr =0.02, average batch_loss = 0.64071356916833, Training Error = 0.380859375
Iteration no. 58, lr = 2, attribute lr =0.02, average batch_loss = 0.63439654785984, Training Error = 0.373046875
Iteration no. 59, lr = 2, attribute lr =0.02, average batch_loss = 0.63389431586571, Training Error = 0.353515625
Iteration no. 60, lr = 2, attribute lr =0.02, average batch_loss = 0.63207830724961, Training Error = 0.337890625
Iteration no. 61, lr = 2, attribute lr =0.02, average batch_loss = 0.63616050182156, Training Error = 0.37109375
Iteration no. 62, lr = 2, attribute lr =0.02, average batch_loss = 0.64618198747768, Training Error = 0.380859375
Iteration no. 63, lr = 2, attribute lr =0.02, average batch_loss = 0.6200036962703, Training Error = 0.345703125
Iteration no. 64, lr = 2, attribute lr =0.02, average batch_loss = 0.64583360192136, Training Error = 0.3828125
Iteration no. 65, lr = 2, attribute lr =0.02, average batch_loss = 0.64800492982355, Training Error = 0.376953125
Iteration no. 66, lr = 2, attribute lr =0.02, average batch_loss = 0.67630987636411, Training Error = 0.400390625
Iteration no. 67, lr = 2, attribute lr =0.02, average batch_loss = 0.62114412381112, Training Error = 0.349609375
Iteration no. 68, lr = 2, attribute lr =0.02, average batch_loss = 0.63701004291749, Training Error = 0.3671875
Iteration no. 69, lr = 2, attribute lr =0.02, average batch_loss = 0.62782032993936, Training Error = 0.373046875
Iteration no. 70, lr = 2, attribute lr =0.02, average batch_loss = 0.60927849220709, Training Error = 0.33984375
Iteration no. 71, lr = 2, attribute lr =0.02, average batch_loss = 0.63294455166474, Training Error = 0.353515625
Iteration no. 72, lr = 2, attribute lr =0.02, average batch_loss = 0.63955899482968, Training Error = 0.37890625
Iteration no. 73, lr = 2, attribute lr =0.02, average batch_loss = 0.6289756802928, Training Error = 0.345703125
Iteration no. 74, lr = 2, attribute lr =0.02, average batch_loss = 0.62529573886818, Training Error = 0.359375
Iteration no. 75, lr = 2, attribute lr =0.02, average batch_loss = 0.62831252264619, Training Error = 0.357421875
Iteration no. 76, lr = 2, attribute lr =0.02, average batch_loss = 0.64854788803714, Training Error = 0.396484375
Iteration no. 77, lr = 2, attribute lr =0.02, average batch_loss = 0.63140237775857, Training Error = 0.365234375
Iteration no. 78, lr = 2, attribute lr =0.02, average batch_loss = 0.60290012106803, Training Error = 0.3203125
Iteration no. 79, lr = 2, attribute lr =0.02, average batch_loss = 0.60305616243774, Training Error = 0.330078125
Iteration no. 80, lr = 2, attribute lr =0.02, average batch_loss = 0.62334084167173, Training Error = 0.37890625
Iteration no. 81, lr = 2, attribute lr =0.02, average batch_loss = 0.61300033938773, Training Error = 0.34375
Iteration no. 82, lr = 2, attribute lr =0.02, average batch_loss = 0.61602915848441, Training Error = 0.345703125
Iteration no. 83, lr = 2, attribute lr =0.02, average batch_loss = 0.62339378653454, Training Error = 0.357421875
Iteration no. 84, lr = 2, attribute lr =0.02, average batch_loss = 0.61266342745645, Training Error = 0.34765625
Iteration no. 85, lr = 2, attribute lr =0.02, average batch_loss = 0.59698976693144, Training Error = 0.31640625
Iteration no. 86, lr = 2, attribute lr =0.02, average batch_loss = 0.61779772289731, Training Error = 0.35546875
Iteration no. 87, lr = 2, attribute lr =0.02, average batch_loss = 0.63966431789321, Training Error = 0.36328125
Iteration no. 88, lr = 2, attribute lr =0.02, average batch_loss = 0.65933133740254, Training Error = 0.376953125
Iteration no. 89, lr = 2, attribute lr =0.02, average batch_loss = 0.6268452942077, Training Error = 0.361328125
Iteration no. 90, lr = 2, attribute lr =0.02, average batch_loss = 0.62510053946594, Training Error = 0.349609375
Iteration no. 91, lr = 2, attribute lr =0.02, average batch_loss = 0.61181969092961, Training Error = 0.359375
Iteration no. 92, lr = 2, attribute lr =0.02, average batch_loss = 0.60971224759583, Training Error = 0.328125
Iteration no. 93, lr = 2, attribute lr =0.02, average batch_loss = 0.58306060487518, Training Error = 0.3046875
Iteration no. 94, lr = 2, attribute lr =0.02, average batch_loss = 0.60919702866756, Training Error = 0.33203125
Iteration no. 95, lr = 2, attribute lr =0.02, average batch_loss = 0.62551798476715, Training Error = 0.341796875
Iteration no. 96, lr = 2, attribute lr =0.02, average batch_loss = 0.59798828186293, Training Error = 0.330078125
Iteration no. 97, lr = 2, attribute lr =0.02, average batch_loss = 0.61019200982292, Training Error = 0.337890625
Iteration no. 98, lr = 2, attribute lr =0.02, average batch_loss = 0.6383746555749, Training Error = 0.36328125
Iteration no. 99, lr = 2, attribute lr =0.02, average batch_loss = 0.5877384838298, Training Error = 0.328125
Iteration no. 100, lr = 2, attribute lr =0.02, average batch_loss = 0.59565026526119, Training Error = 0.326171875
Testing... average test_loss = 0.73570808835249, average test_pred_err = 0.482
Snapshotting C_model... done
Iteration no. 101, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60751388312478, Training Error = 0.3203125
Iteration no. 102, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59738226429878, Training Error = 0.3359375
Iteration no. 103, lr = 1.4, attribute lr =0.02, average batch_loss = 0.62414198326239, Training Error = 0.369140625
Iteration no. 104, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60947668156171, Training Error = 0.345703125
Iteration no. 105, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57674294761925, Training Error = 0.3046875
Iteration no. 106, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60038388131074, Training Error = 0.33984375
Iteration no. 107, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57804337525434, Training Error = 0.31640625
Iteration no. 108, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60147957107365, Training Error = 0.330078125
Iteration no. 109, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59400864551345, Training Error = 0.3125
Iteration no. 110, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5923462471158, Training Error = 0.3203125
Iteration no. 111, lr = 1.4, attribute lr =0.02, average batch_loss = 0.63184618292029, Training Error = 0.369140625
Iteration no. 112, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60437228212979, Training Error = 0.34375
Iteration no. 113, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57500162057897, Training Error = 0.3125
Iteration no. 114, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59847959935137, Training Error = 0.3203125
Iteration no. 115, lr = 1.4, attribute lr =0.02, average batch_loss = 0.56429680166019, Training Error = 0.306640625
Iteration no. 116, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57642589785313, Training Error = 0.3203125
Iteration no. 117, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59826813505889, Training Error = 0.310546875
Iteration no. 118, lr = 1.4, attribute lr =0.02, average batch_loss = 0.62356317858394, Training Error = 0.353515625
Iteration no. 119, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59442873388568, Training Error = 0.330078125
Iteration no. 120, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58826060507593, Training Error = 0.30859375
Iteration no. 121, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60337299896605, Training Error = 0.333984375
Iteration no. 122, lr = 1.4, attribute lr =0.02, average batch_loss = 0.570723111815, Training Error = 0.2890625
Iteration no. 123, lr = 1.4, attribute lr =0.02, average batch_loss = 0.61076313452409, Training Error = 0.341796875
Iteration no. 124, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60158214299441, Training Error = 0.326171875
Iteration no. 125, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59534495227018, Training Error = 0.32421875
Iteration no. 126, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60851378252526, Training Error = 0.345703125
Iteration no. 127, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59297835625299, Training Error = 0.326171875
Iteration no. 128, lr = 1.4, attribute lr =0.02, average batch_loss = 0.61056648987853, Training Error = 0.349609375
Iteration no. 129, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58782100704526, Training Error = 0.314453125
Iteration no. 130, lr = 1.4, attribute lr =0.02, average batch_loss = 0.61953431542397, Training Error = 0.333984375
Iteration no. 131, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58561696126622, Training Error = 0.294921875
Iteration no. 132, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5975229264814, Training Error = 0.333984375
Iteration no. 133, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5933403790304, Training Error = 0.310546875
Iteration no. 134, lr = 1.4, attribute lr =0.02, average batch_loss = 0.6110350247803, Training Error = 0.36328125
Iteration no. 135, lr = 1.4, attribute lr =0.02, average batch_loss = 0.61020519228846, Training Error = 0.36328125
Iteration no. 136, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58350180130956, Training Error = 0.310546875
Iteration no. 137, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57247706488837, Training Error = 0.310546875
Iteration no. 138, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60627299042376, Training Error = 0.337890625
Iteration no. 139, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60293686525151, Training Error = 0.3359375
Iteration no. 140, lr = 1.4, attribute lr =0.02, average batch_loss = 0.61664530253406, Training Error = 0.34375
Iteration no. 141, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58490542747937, Training Error = 0.330078125
Iteration no. 142, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57017809314267, Training Error = 0.296875
Iteration no. 143, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59631944357926, Training Error = 0.3515625
Iteration no. 144, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58280437700663, Training Error = 0.32421875
Iteration no. 145, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58678142383878, Training Error = 0.341796875
Iteration no. 146, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57179244250262, Training Error = 0.31640625
Iteration no. 147, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58985037486753, Training Error = 0.326171875
Iteration no. 148, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5751144198381, Training Error = 0.30859375
Iteration no. 149, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59268077610264, Training Error = 0.322265625
Iteration no. 150, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57417870088611, Training Error = 0.298828125
Testing... average test_loss = 0.77817548617927, average test_pred_err = 0.471
Iteration no. 151, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59171824518051, Training Error = 0.318359375
Iteration no. 152, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57761763552541, Training Error = 0.27734375
Iteration no. 153, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57676571406364, Training Error = 0.3125
Iteration no. 154, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57390180238849, Training Error = 0.30859375
Iteration no. 155, lr = 1.4, attribute lr =0.02, average batch_loss = 0.56081460479343, Training Error = 0.287109375
Iteration no. 156, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5741548469685, Training Error = 0.318359375
Iteration no. 157, lr = 1.4, attribute lr =0.02, average batch_loss = 0.60526991875164, Training Error = 0.33984375
Iteration no. 158, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54638788731378, Training Error = 0.263671875
Iteration no. 159, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57550279443793, Training Error = 0.3203125
Iteration no. 160, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58046758451291, Training Error = 0.328125
Iteration no. 161, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59607404189592, Training Error = 0.330078125
Iteration no. 162, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54630670174738, Training Error = 0.29296875
Iteration no. 163, lr = 1.4, attribute lr =0.02, average batch_loss = 0.56644575679304, Training Error = 0.302734375
Iteration no. 164, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55886073854487, Training Error = 0.2734375
Iteration no. 165, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58371589804943, Training Error = 0.3046875
Iteration no. 166, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57686358079809, Training Error = 0.3125
Iteration no. 167, lr = 1.4, attribute lr =0.02, average batch_loss = 0.59554998736151, Training Error = 0.31640625
Iteration no. 168, lr = 1.4, attribute lr =0.02, average batch_loss = 0.56941941980054, Training Error = 0.30859375
Iteration no. 169, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57290152656107, Training Error = 0.3046875
Iteration no. 170, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54919413490562, Training Error = 0.28515625
Iteration no. 171, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55183688734289, Training Error = 0.2734375
Iteration no. 172, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55426466817801, Training Error = 0.271484375
Iteration no. 173, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5719091665576, Training Error = 0.30078125
Iteration no. 174, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57779034493221, Training Error = 0.31640625
Iteration no. 175, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55431997237368, Training Error = 0.267578125
Iteration no. 176, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55827363619289, Training Error = 0.275390625
Iteration no. 177, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5497176032507, Training Error = 0.2734375
Iteration no. 178, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57577436339375, Training Error = 0.3125
Iteration no. 179, lr = 1.4, attribute lr =0.02, average batch_loss = 0.52809542065438, Training Error = 0.26171875
Iteration no. 180, lr = 1.4, attribute lr =0.02, average batch_loss = 0.53694984930005, Training Error = 0.298828125
Iteration no. 181, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55635732297431, Training Error = 0.322265625
Iteration no. 182, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55747378658768, Training Error = 0.27734375
Iteration no. 183, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58914560238187, Training Error = 0.33203125
Iteration no. 184, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58360445646593, Training Error = 0.306640625
Iteration no. 185, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5603094388947, Training Error = 0.3046875
Iteration no. 186, lr = 1.4, attribute lr =0.02, average batch_loss = 0.5544946795367, Training Error = 0.296875
Iteration no. 187, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54128499934748, Training Error = 0.28515625
Iteration no. 188, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57227132567496, Training Error = 0.287109375
Iteration no. 189, lr = 1.4, attribute lr =0.02, average batch_loss = 0.56590217332358, Training Error = 0.296875
Iteration no. 190, lr = 1.4, attribute lr =0.02, average batch_loss = 0.58306196532932, Training Error = 0.32421875
Iteration no. 191, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57192431577662, Training Error = 0.318359375
Iteration no. 192, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54278805941202, Training Error = 0.3046875
Iteration no. 193, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54553334397237, Training Error = 0.283203125
Iteration no. 194, lr = 1.4, attribute lr =0.02, average batch_loss = 0.51768678710816, Training Error = 0.263671875
Iteration no. 195, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57464413601721, Training Error = 0.310546875
Iteration no. 196, lr = 1.4, attribute lr =0.02, average batch_loss = 0.56594960810428, Training Error = 0.287109375
Iteration no. 197, lr = 1.4, attribute lr =0.02, average batch_loss = 0.54943824439466, Training Error = 0.30078125
Iteration no. 198, lr = 1.4, attribute lr =0.02, average batch_loss = 0.550007658013, Training Error = 0.2890625
Iteration no. 199, lr = 1.4, attribute lr =0.02, average batch_loss = 0.55960148867805, Training Error = 0.298828125
Iteration no. 200, lr = 1.4, attribute lr =0.02, average batch_loss = 0.57189629063419, Training Error = 0.291015625
Testing... average test_loss = 0.8284637706416, average test_pred_err = 0.471
Snapshotting C_model... done
Iteration no. 201, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54529717418817, Training Error = 0.283203125
Iteration no. 202, lr = 0.98, attribute lr =0.02, average batch_loss = 0.57213939756374, Training Error = 0.3046875
Iteration no. 203, lr = 0.98, attribute lr =0.02, average batch_loss = 0.57807999956668, Training Error = 0.3125
Iteration no. 204, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54469788054872, Training Error = 0.26953125
Iteration no. 205, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55045648014238, Training Error = 0.271484375
Iteration no. 206, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5379899697528, Training Error = 0.263671875
Iteration no. 207, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54914745035502, Training Error = 0.298828125
Iteration no. 208, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54781973416896, Training Error = 0.298828125
Iteration no. 209, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5302695567875, Training Error = 0.287109375
Iteration no. 210, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5511080779304, Training Error = 0.306640625
Iteration no. 211, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54613077758367, Training Error = 0.27734375
Iteration no. 212, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56954486846222, Training Error = 0.3203125
Iteration no. 213, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53057440782676, Training Error = 0.28515625
Iteration no. 214, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54674770970097, Training Error = 0.28125
Iteration no. 215, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56875210750801, Training Error = 0.31640625
Iteration no. 216, lr = 0.98, attribute lr =0.02, average batch_loss = 0.58035869541622, Training Error = 0.31640625
Iteration no. 217, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56850467959081, Training Error = 0.294921875
Iteration no. 218, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5349371183439, Training Error = 0.271484375
Iteration no. 219, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53451568767707, Training Error = 0.30078125
Iteration no. 220, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54902175653639, Training Error = 0.29296875
Iteration no. 221, lr = 0.98, attribute lr =0.02, average batch_loss = 0.61667502501461, Training Error = 0.34765625
Iteration no. 222, lr = 0.98, attribute lr =0.02, average batch_loss = 0.50174267345566, Training Error = 0.22265625
Iteration no. 223, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53411583214724, Training Error = 0.27734375
Iteration no. 224, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55404979995384, Training Error = 0.294921875
Iteration no. 225, lr = 0.98, attribute lr =0.02, average batch_loss = 0.59216695516531, Training Error = 0.306640625
Iteration no. 226, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51910752685715, Training Error = 0.275390625
Iteration no. 227, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54295974541337, Training Error = 0.2734375
Iteration no. 228, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51477248535835, Training Error = 0.2734375
Iteration no. 229, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55872295299205, Training Error = 0.306640625
Iteration no. 230, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5650274118779, Training Error = 0.287109375
Iteration no. 231, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54057361517265, Training Error = 0.287109375
Iteration no. 232, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52734102242338, Training Error = 0.26953125
Iteration no. 233, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53403593672276, Training Error = 0.267578125
Iteration no. 234, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55655489156582, Training Error = 0.287109375
Iteration no. 235, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5451283483375, Training Error = 0.283203125
Iteration no. 236, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56472326257728, Training Error = 0.298828125
Iteration no. 237, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5782901234266, Training Error = 0.33203125
Iteration no. 238, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55510397151591, Training Error = 0.30859375
Iteration no. 239, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51930834636982, Training Error = 0.26953125
Iteration no. 240, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51957094286195, Training Error = 0.265625
Iteration no. 241, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53655130122217, Training Error = 0.26953125
Iteration no. 242, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52459375460491, Training Error = 0.263671875
Iteration no. 243, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52875322431125, Training Error = 0.275390625
Iteration no. 244, lr = 0.98, attribute lr =0.02, average batch_loss = 0.50312069501326, Training Error = 0.25390625
Iteration no. 245, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5552779331743, Training Error = 0.302734375
Iteration no. 246, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56757961587016, Training Error = 0.318359375
Iteration no. 247, lr = 0.98, attribute lr =0.02, average batch_loss = 0.50539128954466, Training Error = 0.255859375
Iteration no. 248, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54795310164081, Training Error = 0.279296875
Iteration no. 249, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56132888006739, Training Error = 0.291015625
Iteration no. 250, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51343995969466, Training Error = 0.25
Testing... average test_loss = 0.83938068168978, average test_pred_err = 0.453
Iteration no. 251, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53827555345829, Training Error = 0.26953125
Iteration no. 252, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52367341898867, Training Error = 0.267578125
Iteration no. 253, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52339388517167, Training Error = 0.279296875
Iteration no. 254, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53718980017198, Training Error = 0.25390625
Iteration no. 255, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52713211119451, Training Error = 0.26953125
Iteration no. 256, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54513871563317, Training Error = 0.291015625
Iteration no. 257, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54765280378723, Training Error = 0.302734375
Iteration no. 258, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54715469231493, Training Error = 0.283203125
Iteration no. 259, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55144883191096, Training Error = 0.298828125
Iteration no. 260, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54525735029926, Training Error = 0.279296875
Iteration no. 261, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51024266206742, Training Error = 0.265625
Iteration no. 262, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51700303806183, Training Error = 0.259765625
Iteration no. 263, lr = 0.98, attribute lr =0.02, average batch_loss = 0.57346517187487, Training Error = 0.3046875
Iteration no. 264, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54985736186064, Training Error = 0.283203125
Iteration no. 265, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52132735722889, Training Error = 0.2421875
Iteration no. 266, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52173905336756, Training Error = 0.263671875
Iteration no. 267, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55087286598443, Training Error = 0.296875
Iteration no. 268, lr = 0.98, attribute lr =0.02, average batch_loss = 0.50293346246923, Training Error = 0.25
Iteration no. 269, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55718196769364, Training Error = 0.271484375
Iteration no. 270, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56627016381973, Training Error = 0.31640625
Iteration no. 271, lr = 0.98, attribute lr =0.02, average batch_loss = 0.57158966677209, Training Error = 0.314453125
Iteration no. 272, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54754205307693, Training Error = 0.28515625
Iteration no. 273, lr = 0.98, attribute lr =0.02, average batch_loss = 0.54387683840215, Training Error = 0.29296875
Iteration no. 274, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52275510568483, Training Error = 0.251953125
Iteration no. 275, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56348571458691, Training Error = 0.30078125
Iteration no. 276, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52520941651079, Training Error = 0.279296875
Iteration no. 277, lr = 0.98, attribute lr =0.02, average batch_loss = 0.50310670956411, Training Error = 0.248046875
Iteration no. 278, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5154131179908, Training Error = 0.25390625
Iteration no. 279, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5330191876451, Training Error = 0.2890625
Iteration no. 280, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52437476792766, Training Error = 0.275390625
Iteration no. 281, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51885919231045, Training Error = 0.267578125
Iteration no. 282, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51832975974266, Training Error = 0.255859375
Iteration no. 283, lr = 0.98, attribute lr =0.02, average batch_loss = 0.49517597212272, Training Error = 0.23046875
Iteration no. 284, lr = 0.98, attribute lr =0.02, average batch_loss = 0.46061862099103, Training Error = 0.22265625
Iteration no. 285, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52303481542344, Training Error = 0.248046875
Iteration no. 286, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52118973884372, Training Error = 0.27734375
Iteration no. 287, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53206509752675, Training Error = 0.267578125
Iteration no. 288, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53242104551802, Training Error = 0.283203125
Iteration no. 289, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53114695557048, Training Error = 0.302734375
Iteration no. 290, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53374627590176, Training Error = 0.283203125
Iteration no. 291, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5122882107346, Training Error = 0.28515625
Iteration no. 292, lr = 0.98, attribute lr =0.02, average batch_loss = 0.529802148894, Training Error = 0.2578125
Iteration no. 293, lr = 0.98, attribute lr =0.02, average batch_loss = 0.51470028824778, Training Error = 0.25
Iteration no. 294, lr = 0.98, attribute lr =0.02, average batch_loss = 0.55809574924671, Training Error = 0.30078125
Iteration no. 295, lr = 0.98, attribute lr =0.02, average batch_loss = 0.5289781453859, Training Error = 0.26953125
Iteration no. 296, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53234968427692, Training Error = 0.267578125
Iteration no. 297, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52626155838854, Training Error = 0.279296875
Iteration no. 298, lr = 0.98, attribute lr =0.02, average batch_loss = 0.52358818434998, Training Error = 0.259765625
Iteration no. 299, lr = 0.98, attribute lr =0.02, average batch_loss = 0.53975327116549, Training Error = 0.275390625
Iteration no. 300, lr = 0.98, attribute lr =0.02, average batch_loss = 0.56286303746788, Training Error = 0.28515625
Testing... average test_loss = 0.80550270308347, average test_pred_err = 0.446
Snapshotting C_model... done
Iteration no. 301, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53025626283354, Training Error = 0.279296875
Iteration no. 302, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50542248412426, Training Error = 0.244140625
Iteration no. 303, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53180464849818, Training Error = 0.291015625
Iteration no. 304, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50993552321366, Training Error = 0.244140625
Iteration no. 305, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5099468475166, Training Error = 0.263671875
Iteration no. 306, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5190342458439, Training Error = 0.27734375
Iteration no. 307, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51779223871829, Training Error = 0.25390625
Iteration no. 308, lr = 0.686, attribute lr =0.02, average batch_loss = 0.54484530894244, Training Error = 0.296875
Iteration no. 309, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50716482578879, Training Error = 0.23828125
Iteration no. 310, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53191323871251, Training Error = 0.271484375
Iteration no. 311, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51121801366775, Training Error = 0.259765625
Iteration no. 312, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49093103643334, Training Error = 0.2421875
Iteration no. 313, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50937117659352, Training Error = 0.26171875
Iteration no. 314, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50394512488569, Training Error = 0.25
Iteration no. 315, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53004247886596, Training Error = 0.275390625
Iteration no. 316, lr = 0.686, attribute lr =0.02, average batch_loss = 0.55666717749741, Training Error = 0.279296875
Iteration no. 317, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53573616813814, Training Error = 0.265625
Iteration no. 318, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51974922826638, Training Error = 0.248046875
Iteration no. 319, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5307964787482, Training Error = 0.26953125
Iteration no. 320, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50353322666426, Training Error = 0.251953125
Iteration no. 321, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49615388571324, Training Error = 0.244140625
Iteration no. 322, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50214339995804, Training Error = 0.263671875
Iteration no. 323, lr = 0.686, attribute lr =0.02, average batch_loss = 0.47977505894273, Training Error = 0.23828125
Iteration no. 324, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50533422536602, Training Error = 0.251953125
Iteration no. 325, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49678297025481, Training Error = 0.248046875
Iteration no. 326, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51001416130066, Training Error = 0.26171875
Iteration no. 327, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5026643594038, Training Error = 0.25
Iteration no. 328, lr = 0.686, attribute lr =0.02, average batch_loss = 0.54347263164101, Training Error = 0.279296875
Iteration no. 329, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53196246455041, Training Error = 0.28125
Iteration no. 330, lr = 0.686, attribute lr =0.02, average batch_loss = 0.52683276990555, Training Error = 0.27734375
Iteration no. 331, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5313917814675, Training Error = 0.2890625
Iteration no. 332, lr = 0.686, attribute lr =0.02, average batch_loss = 0.55261678733789, Training Error = 0.302734375
Iteration no. 333, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51443313132893, Training Error = 0.28515625
Iteration no. 334, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49000045116476, Training Error = 0.228515625
Iteration no. 335, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50757400139797, Training Error = 0.25
Iteration no. 336, lr = 0.686, attribute lr =0.02, average batch_loss = 0.4875962502809, Training Error = 0.2421875
Iteration no. 337, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53819701134682, Training Error = 0.2734375
Iteration no. 338, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51112582099824, Training Error = 0.283203125
Iteration no. 339, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51752650701335, Training Error = 0.275390625
Iteration no. 340, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49706846022335, Training Error = 0.240234375
Iteration no. 341, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50931686853091, Training Error = 0.255859375
Iteration no. 342, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49493813511787, Training Error = 0.2265625
Iteration no. 343, lr = 0.686, attribute lr =0.02, average batch_loss = 0.4945144738423, Training Error = 0.25390625
Iteration no. 344, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50869993263874, Training Error = 0.248046875
Iteration no. 345, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50964667040623, Training Error = 0.2421875
Iteration no. 346, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50852669665721, Training Error = 0.2421875
Iteration no. 347, lr = 0.686, attribute lr =0.02, average batch_loss = 0.52200676372425, Training Error = 0.263671875
Iteration no. 348, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49252096736691, Training Error = 0.23828125
Iteration no. 349, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53996029030301, Training Error = 0.3046875
Iteration no. 350, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50336901075431, Training Error = 0.24609375
Testing... average test_loss = 0.88490447580685, average test_pred_err = 0.451
Iteration no. 351, lr = 0.686, attribute lr =0.02, average batch_loss = 0.47613433094025, Training Error = 0.220703125
Iteration no. 352, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48949073934655, Training Error = 0.24609375
Iteration no. 353, lr = 0.686, attribute lr =0.02, average batch_loss = 0.4908782702784, Training Error = 0.22265625
Iteration no. 354, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48963379052436, Training Error = 0.234375
Iteration no. 355, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51673912808257, Training Error = 0.26953125
Iteration no. 356, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51021761511728, Training Error = 0.251953125
Iteration no. 357, lr = 0.686, attribute lr =0.02, average batch_loss = 0.52537538143492, Training Error = 0.279296875
Iteration no. 358, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53909616942368, Training Error = 0.287109375
Iteration no. 359, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49922801686271, Training Error = 0.25390625
Iteration no. 360, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5059737940819, Training Error = 0.248046875
Iteration no. 361, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51442278922778, Training Error = 0.25
Iteration no. 362, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50857510345666, Training Error = 0.259765625
Iteration no. 363, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48076877816268, Training Error = 0.22265625
Iteration no. 364, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48292148476381, Training Error = 0.23828125
Iteration no. 365, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5128913346857, Training Error = 0.265625
Iteration no. 366, lr = 0.686, attribute lr =0.02, average batch_loss = 0.52812941165824, Training Error = 0.28125
Iteration no. 367, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51443701596903, Training Error = 0.267578125
Iteration no. 368, lr = 0.686, attribute lr =0.02, average batch_loss = 0.47965271435779, Training Error = 0.248046875
Iteration no. 369, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48402915149786, Training Error = 0.21875
Iteration no. 370, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51420621704712, Training Error = 0.26171875
Iteration no. 371, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48432607821881, Training Error = 0.240234375
Iteration no. 372, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53050299751586, Training Error = 0.27734375
Iteration no. 373, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49929856322541, Training Error = 0.255859375
Iteration no. 374, lr = 0.686, attribute lr =0.02, average batch_loss = 0.52575300426592, Training Error = 0.271484375
Iteration no. 375, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48706523113837, Training Error = 0.2265625
Iteration no. 376, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51519696117094, Training Error = 0.267578125
Iteration no. 377, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53303756242956, Training Error = 0.255859375
Iteration no. 378, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50531248241878, Training Error = 0.240234375
Iteration no. 379, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49211702503054, Training Error = 0.23828125
Iteration no. 380, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51848993884722, Training Error = 0.267578125
Iteration no. 381, lr = 0.686, attribute lr =0.02, average batch_loss = 0.4748020871381, Training Error = 0.220703125
Iteration no. 382, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53223904153027, Training Error = 0.263671875
Iteration no. 383, lr = 0.686, attribute lr =0.02, average batch_loss = 0.45082113756153, Training Error = 0.197265625
Iteration no. 384, lr = 0.686, attribute lr =0.02, average batch_loss = 0.5334564788368, Training Error = 0.279296875
Iteration no. 385, lr = 0.686, attribute lr =0.02, average batch_loss = 0.53037921620817, Training Error = 0.259765625
Iteration no. 386, lr = 0.686, attribute lr =0.02, average batch_loss = 0.52228076454134, Training Error = 0.25390625
Iteration no. 387, lr = 0.686, attribute lr =0.02, average batch_loss = 0.51464676338492, Training Error = 0.279296875
Iteration no. 388, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50287456514806, Training Error = 0.2578125
Iteration no. 389, lr = 0.686, attribute lr =0.02, average batch_loss = 0.47362257667907, Training Error = 0.23046875
Iteration no. 390, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49047980955654, Training Error = 0.248046875
Iteration no. 391, lr = 0.686, attribute lr =0.02, average batch_loss = 0.47416420968108, Training Error = 0.236328125
Iteration no. 392, lr = 0.686, attribute lr =0.02, average batch_loss = 0.50088592568168, Training Error = 0.267578125
Iteration no. 393, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49129638473881, Training Error = 0.2578125
Iteration no. 394, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48600263567388, Training Error = 0.205078125
Iteration no. 395, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48334114743362, Training Error = 0.244140625
Iteration no. 396, lr = 0.686, attribute lr =0.02, average batch_loss = 0.48487255457752, Training Error = 0.23046875
Iteration no. 397, lr = 0.686, attribute lr =0.02, average batch_loss = 0.46203068353741, Training Error = 0.205078125
Iteration no. 398, lr = 0.686, attribute lr =0.02, average batch_loss = 0.49418244018122, Training Error = 0.234375
Iteration no. 399, lr = 0.686, attribute lr =0.02, average batch_loss = 0.45165199137405, Training Error = 0.216796875
Iteration no. 400, lr = 0.686, attribute lr =0.02, average batch_loss = 0.46228741094491, Training Error = 0.228515625
Testing... average test_loss = 0.97166528062812, average test_pred_err = 0.478
Snapshotting C_model... done
Iteration no. 401, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49122159064389, Training Error = 0.24609375
Iteration no. 402, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50269791840958, Training Error = 0.24609375
Iteration no. 403, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.45695259040519, Training Error = 0.232421875
Iteration no. 404, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4775139216795, Training Error = 0.23046875
Iteration no. 405, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.51045631424305, Training Error = 0.26171875
Iteration no. 406, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.5143946828459, Training Error = 0.25390625
Iteration no. 407, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49918132581762, Training Error = 0.248046875
Iteration no. 408, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50981270318467, Training Error = 0.251953125
Iteration no. 409, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47285112780337, Training Error = 0.23046875
Iteration no. 410, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48856296903536, Training Error = 0.25390625
Iteration no. 411, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.54659064092896, Training Error = 0.29296875
Iteration no. 412, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49370024330052, Training Error = 0.240234375
Iteration no. 413, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46403756584626, Training Error = 0.224609375
Iteration no. 414, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48359542080001, Training Error = 0.24609375
Iteration no. 415, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.52026715799418, Training Error = 0.26171875
Iteration no. 416, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.52202358077779, Training Error = 0.275390625
Iteration no. 417, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48163739751749, Training Error = 0.21875
Iteration no. 418, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4990306951368, Training Error = 0.24609375
Iteration no. 419, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4774213822607, Training Error = 0.216796875
Iteration no. 420, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.52530918709898, Training Error = 0.259765625
Iteration no. 421, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49701279022173, Training Error = 0.259765625
Iteration no. 422, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.51646450320227, Training Error = 0.259765625
Iteration no. 423, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46172859837714, Training Error = 0.220703125
Iteration no. 424, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49162137739388, Training Error = 0.24609375
Iteration no. 425, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4767108554565, Training Error = 0.234375
Iteration no. 426, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.490544313295, Training Error = 0.24609375
Iteration no. 427, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49241435206689, Training Error = 0.248046875
Iteration no. 428, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.52639999036846, Training Error = 0.26953125
Iteration no. 429, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4970194172419, Training Error = 0.22265625
Iteration no. 430, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47656524443268, Training Error = 0.22265625
Iteration no. 431, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.53812365152485, Training Error = 0.279296875
Iteration no. 432, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47133452755371, Training Error = 0.2265625
Iteration no. 433, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47052120607196, Training Error = 0.205078125
Iteration no. 434, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47279552822415, Training Error = 0.244140625
Iteration no. 435, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46666834978457, Training Error = 0.224609375
Iteration no. 436, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46671539987359, Training Error = 0.2265625
Iteration no. 437, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47957920979513, Training Error = 0.2265625
Iteration no. 438, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48673538668609, Training Error = 0.255859375
Iteration no. 439, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46575431032894, Training Error = 0.228515625
Iteration no. 440, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50517791390454, Training Error = 0.26171875
Iteration no. 441, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46562167475403, Training Error = 0.216796875
Iteration no. 442, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48571941941174, Training Error = 0.23828125
Iteration no. 443, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48903507996448, Training Error = 0.216796875
Iteration no. 444, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.43726132406202, Training Error = 0.193359375
Iteration no. 445, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49072165688758, Training Error = 0.244140625
Iteration no. 446, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.51639781279112, Training Error = 0.28515625
Iteration no. 447, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50595609018416, Training Error = 0.248046875
Iteration no. 448, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48936951423454, Training Error = 0.248046875
Iteration no. 449, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48780027179893, Training Error = 0.244140625
Iteration no. 450, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.44598555827428, Training Error = 0.19921875
Testing... average test_loss = 0.9497486040578, average test_pred_err = 0.446
Iteration no. 451, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50475803482001, Training Error = 0.23828125
Iteration no. 452, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50408119678086, Training Error = 0.267578125
Iteration no. 453, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50348313792478, Training Error = 0.263671875
Iteration no. 454, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49019878354498, Training Error = 0.234375
Iteration no. 455, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47609944155103, Training Error = 0.224609375
Iteration no. 456, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48156009124576, Training Error = 0.236328125
Iteration no. 457, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50148498308983, Training Error = 0.248046875
Iteration no. 458, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50280953824642, Training Error = 0.25
Iteration no. 459, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48478863531473, Training Error = 0.21484375
Iteration no. 460, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4913772893679, Training Error = 0.24609375
Iteration no. 461, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.500085341957, Training Error = 0.26171875
Iteration no. 462, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50182290167692, Training Error = 0.24609375
Iteration no. 463, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47284692672658, Training Error = 0.224609375
Iteration no. 464, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49009533102945, Training Error = 0.251953125
Iteration no. 465, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49061513132889, Training Error = 0.2421875
Iteration no. 466, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47456908056253, Training Error = 0.244140625
Iteration no. 467, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46440330558, Training Error = 0.22265625
Iteration no. 468, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.50926678182555, Training Error = 0.2578125
Iteration no. 469, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.51427335467289, Training Error = 0.267578125
Iteration no. 470, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.5340965953452, Training Error = 0.26171875
Iteration no. 471, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48559194049022, Training Error = 0.23828125
Iteration no. 472, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.51437508645976, Training Error = 0.25
Iteration no. 473, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49221846657256, Training Error = 0.248046875
Iteration no. 474, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46015438210456, Training Error = 0.220703125
Iteration no. 475, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.51625471341394, Training Error = 0.271484375
Iteration no. 476, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.43074420560307, Training Error = 0.166015625
Iteration no. 477, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46593642674441, Training Error = 0.22265625
Iteration no. 478, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.44512962496945, Training Error = 0.197265625
Iteration no. 479, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.45924199136513, Training Error = 0.208984375
Iteration no. 480, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48487976499348, Training Error = 0.232421875
Iteration no. 481, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49936983442663, Training Error = 0.2578125
Iteration no. 482, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48121799442796, Training Error = 0.234375
Iteration no. 483, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4766125656955, Training Error = 0.23828125
Iteration no. 484, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.45558216882478, Training Error = 0.2109375
Iteration no. 485, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46394391801053, Training Error = 0.208984375
Iteration no. 486, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46005964428724, Training Error = 0.21484375
Iteration no. 487, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47574220167032, Training Error = 0.23828125
Iteration no. 488, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48918071848838, Training Error = 0.2265625
Iteration no. 489, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49019200024777, Training Error = 0.25390625
Iteration no. 490, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48782420245537, Training Error = 0.25390625
Iteration no. 491, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48462643671784, Training Error = 0.234375
Iteration no. 492, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49009463513942, Training Error = 0.236328125
Iteration no. 493, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47623982208148, Training Error = 0.23828125
Iteration no. 494, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.49312983255189, Training Error = 0.248046875
Iteration no. 495, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.48309360262724, Training Error = 0.25390625
Iteration no. 496, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.46976112758744, Training Error = 0.23046875
Iteration no. 497, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.47159812598368, Training Error = 0.240234375
Iteration no. 498, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.4437023754853, Training Error = 0.20703125
Iteration no. 499, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.43463037610756, Training Error = 0.181640625
Iteration no. 500, lr = 0.4802, attribute lr =0.02, average batch_loss = 0.45780544534533, Training Error = 0.212890625
Testing... average test_loss = 0.9638776764576, average test_pred_err = 0.446
Snapshotting C_model... done
Iteration no. 501, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.51660824734227, Training Error = 0.265625
Iteration no. 502, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48246847260872, Training Error = 0.2421875
Iteration no. 503, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45396281981074, Training Error = 0.21484375
Iteration no. 504, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46473875986829, Training Error = 0.20703125
Iteration no. 505, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46151902912797, Training Error = 0.216796875
Iteration no. 506, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.49782553123875, Training Error = 0.251953125
Iteration no. 507, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.500892682218, Training Error = 0.244140625
Iteration no. 508, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46261808791555, Training Error = 0.220703125
Iteration no. 509, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.50444866795815, Training Error = 0.236328125
Iteration no. 510, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48514428695025, Training Error = 0.2265625
Iteration no. 511, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45985236293706, Training Error = 0.234375
Iteration no. 512, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.49726124013787, Training Error = 0.267578125
Iteration no. 513, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47288527822886, Training Error = 0.240234375
Iteration no. 514, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48610650047331, Training Error = 0.248046875
Iteration no. 515, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44951328817571, Training Error = 0.21484375
Iteration no. 516, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44203152144634, Training Error = 0.2109375
Iteration no. 517, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.43016324994947, Training Error = 0.203125
Iteration no. 518, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4574469909244, Training Error = 0.224609375
Iteration no. 519, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48001748523246, Training Error = 0.23828125
Iteration no. 520, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48482963994481, Training Error = 0.2578125
Iteration no. 521, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45730573869908, Training Error = 0.2109375
Iteration no. 522, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46153022047155, Training Error = 0.23828125
Iteration no. 523, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47552666465881, Training Error = 0.244140625
Iteration no. 524, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45659460030656, Training Error = 0.23046875
Iteration no. 525, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.50546670850337, Training Error = 0.259765625
Iteration no. 526, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.50160058531152, Training Error = 0.248046875
Iteration no. 527, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.49092563220265, Training Error = 0.240234375
Iteration no. 528, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44283822539717, Training Error = 0.2109375
Iteration no. 529, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4885645828668, Training Error = 0.25
Iteration no. 530, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48633664533243, Training Error = 0.228515625
Iteration no. 531, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.474349844218, Training Error = 0.248046875
Iteration no. 532, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46366607010504, Training Error = 0.228515625
Iteration no. 533, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47244065801837, Training Error = 0.23046875
Iteration no. 534, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4868995819592, Training Error = 0.244140625
Iteration no. 535, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46999281211736, Training Error = 0.24609375
Iteration no. 536, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46752402548753, Training Error = 0.2265625
Iteration no. 537, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47473013317645, Training Error = 0.244140625
Iteration no. 538, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45240742501417, Training Error = 0.224609375
Iteration no. 539, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48414646507713, Training Error = 0.23046875
Iteration no. 540, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.50935166278703, Training Error = 0.263671875
Iteration no. 541, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4796644692177, Training Error = 0.224609375
Iteration no. 542, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47845073964972, Training Error = 0.2421875
Iteration no. 543, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44889599850744, Training Error = 0.228515625
Iteration no. 544, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47063638968219, Training Error = 0.224609375
Iteration no. 545, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48236890521637, Training Error = 0.22265625
Iteration no. 546, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44218642322318, Training Error = 0.19921875
Iteration no. 547, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44719774444804, Training Error = 0.1953125
Iteration no. 548, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47319584648585, Training Error = 0.24609375
Iteration no. 549, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44648953144835, Training Error = 0.212890625
Iteration no. 550, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47153485171128, Training Error = 0.22265625
Testing... average test_loss = 0.96580281701088, average test_pred_err = 0.464
Iteration no. 551, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47168920752801, Training Error = 0.2421875
Iteration no. 552, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44233720444307, Training Error = 0.20703125
Iteration no. 553, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.43630366137472, Training Error = 0.19140625
Iteration no. 554, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4623563918483, Training Error = 0.21875
Iteration no. 555, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.43868192143856, Training Error = 0.216796875
Iteration no. 556, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47375089230329, Training Error = 0.22265625
Iteration no. 557, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4927440891941, Training Error = 0.25390625
Iteration no. 558, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48014894938292, Training Error = 0.23828125
Iteration no. 559, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4542556369838, Training Error = 0.201171875
Iteration no. 560, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4885374615438, Training Error = 0.21875
Iteration no. 561, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45025618305824, Training Error = 0.216796875
Iteration no. 562, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44747199009537, Training Error = 0.2109375
Iteration no. 563, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48203665485213, Training Error = 0.232421875
Iteration no. 564, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.480799073599, Training Error = 0.24609375
Iteration no. 565, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4655922207003, Training Error = 0.234375
Iteration no. 566, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46855871099108, Training Error = 0.220703125
Iteration no. 567, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44758334215144, Training Error = 0.216796875
Iteration no. 568, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44820227003426, Training Error = 0.2265625
Iteration no. 569, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46913414809731, Training Error = 0.2265625
Iteration no. 570, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4357795813102, Training Error = 0.2109375
Iteration no. 571, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.42542777017444, Training Error = 0.19140625
Iteration no. 572, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46403817382048, Training Error = 0.22265625
Iteration no. 573, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.496124566041, Training Error = 0.2421875
Iteration no. 574, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44179791053115, Training Error = 0.1953125
Iteration no. 575, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.42605420392032, Training Error = 0.201171875
Iteration no. 576, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46675037915902, Training Error = 0.224609375
Iteration no. 577, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45003071199686, Training Error = 0.2265625
Iteration no. 578, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44223143036618, Training Error = 0.20703125
Iteration no. 579, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.42139093997398, Training Error = 0.197265625
Iteration no. 580, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.42526318017257, Training Error = 0.197265625
Iteration no. 581, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46194942415634, Training Error = 0.23046875
Iteration no. 582, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.42217323965935, Training Error = 0.1796875
Iteration no. 583, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46191607873673, Training Error = 0.2265625
Iteration no. 584, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.49821471481527, Training Error = 0.26171875
Iteration no. 585, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.48422886520706, Training Error = 0.22265625
Iteration no. 586, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.4708360601202, Training Error = 0.224609375
Iteration no. 587, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47206733607865, Training Error = 0.220703125
Iteration no. 588, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.50190273131326, Training Error = 0.234375
Iteration no. 589, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45571009420743, Training Error = 0.22265625
Iteration no. 590, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45591470395581, Training Error = 0.21484375
Iteration no. 591, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47422052489793, Training Error = 0.236328125
Iteration no. 592, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45465317729807, Training Error = 0.2109375
Iteration no. 593, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44724413943046, Training Error = 0.212890625
Iteration no. 594, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46275758094323, Training Error = 0.220703125
Iteration no. 595, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44242072902212, Training Error = 0.197265625
Iteration no. 596, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.47239163165951, Training Error = 0.2421875
Iteration no. 597, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.44397650539132, Training Error = 0.205078125
Iteration no. 598, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.45184791501148, Training Error = 0.201171875
Iteration no. 599, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.46667311409988, Training Error = 0.220703125
Iteration no. 600, lr = 0.33614, attribute lr =0.02, average batch_loss = 0.49451881101365, Training Error = 0.25390625
Testing... average test_loss = 1.0671344820139, average test_pred_err = 0.468
Snapshotting C_model... done
Iteration no. 601, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4781666742904, Training Error = 0.224609375
Iteration no. 602, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4762675065171, Training Error = 0.2265625
Iteration no. 603, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44241244294435, Training Error = 0.2265625
Iteration no. 604, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45995988666582, Training Error = 0.22265625
Iteration no. 605, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42005338261459, Training Error = 0.181640625
Iteration no. 606, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43855964280932, Training Error = 0.203125
Iteration no. 607, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47481075446205, Training Error = 0.248046875
Iteration no. 608, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.48762783835173, Training Error = 0.25390625
Iteration no. 609, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45558753989255, Training Error = 0.2109375
Iteration no. 610, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44129669448969, Training Error = 0.19140625
Iteration no. 611, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46650961679093, Training Error = 0.234375
Iteration no. 612, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43490066144747, Training Error = 0.205078125
Iteration no. 613, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44859823886707, Training Error = 0.212890625
Iteration no. 614, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46050442777088, Training Error = 0.228515625
Iteration no. 615, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44622575684133, Training Error = 0.20703125
Iteration no. 616, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4421428919715, Training Error = 0.203125
Iteration no. 617, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4749631644342, Training Error = 0.2265625
Iteration no. 618, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44003296461314, Training Error = 0.201171875
Iteration no. 619, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44766282066406, Training Error = 0.21484375
Iteration no. 620, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44930814076002, Training Error = 0.197265625
Iteration no. 621, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46948328907053, Training Error = 0.2265625
Iteration no. 622, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.48655752787822, Training Error = 0.25390625
Iteration no. 623, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43876251141513, Training Error = 0.19921875
Iteration no. 624, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47362681034834, Training Error = 0.2421875
Iteration no. 625, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45924514474762, Training Error = 0.228515625
Iteration no. 626, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47297419509494, Training Error = 0.234375
Iteration no. 627, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43392846526787, Training Error = 0.19921875
Iteration no. 628, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46389356995198, Training Error = 0.228515625
Iteration no. 629, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45428314576178, Training Error = 0.20703125
Iteration no. 630, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.48950419519447, Training Error = 0.23046875
Iteration no. 631, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.41789550041919, Training Error = 0.181640625
Iteration no. 632, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44830384244309, Training Error = 0.22265625
Iteration no. 633, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46762906988194, Training Error = 0.220703125
Iteration no. 634, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43719047305077, Training Error = 0.203125
Iteration no. 635, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.50099304729841, Training Error = 0.267578125
Iteration no. 636, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46945975482411, Training Error = 0.220703125
Iteration no. 637, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45331184554396, Training Error = 0.232421875
Iteration no. 638, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44933433621164, Training Error = 0.212890625
Iteration no. 639, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44561565898894, Training Error = 0.228515625
Iteration no. 640, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45153039750999, Training Error = 0.212890625
Iteration no. 641, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43038754313705, Training Error = 0.19921875
Iteration no. 642, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46726077097353, Training Error = 0.21484375
Iteration no. 643, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42573452223055, Training Error = 0.201171875
Iteration no. 644, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.40635973677421, Training Error = 0.201171875
Iteration no. 645, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.41936038877962, Training Error = 0.19921875
Iteration no. 646, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43480953354063, Training Error = 0.208984375
Iteration no. 647, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4968617627502, Training Error = 0.244140625
Iteration no. 648, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46632715628001, Training Error = 0.25
Iteration no. 649, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.48058664829909, Training Error = 0.248046875
Iteration no. 650, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4150385392864, Training Error = 0.1875
Testing... average test_loss = 1.0271727360571, average test_pred_err = 0.448
Iteration no. 651, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.49179375246132, Training Error = 0.23828125
Iteration no. 652, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47410573619928, Training Error = 0.23828125
Iteration no. 653, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47734465972718, Training Error = 0.244140625
Iteration no. 654, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4463223429132, Training Error = 0.216796875
Iteration no. 655, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43630524554313, Training Error = 0.203125
Iteration no. 656, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46472241417528, Training Error = 0.22265625
Iteration no. 657, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.48050625486044, Training Error = 0.2265625
Iteration no. 658, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.468517254388, Training Error = 0.228515625
Iteration no. 659, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4248984365313, Training Error = 0.1796875
Iteration no. 660, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.41669039920153, Training Error = 0.19140625
Iteration no. 661, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43573749100518, Training Error = 0.203125
Iteration no. 662, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46710433627944, Training Error = 0.234375
Iteration no. 663, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44854303397826, Training Error = 0.2265625
Iteration no. 664, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45729709032196, Training Error = 0.21484375
Iteration no. 665, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4079420960114, Training Error = 0.181640625
Iteration no. 666, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.40581800975699, Training Error = 0.171875
Iteration no. 667, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43531698732367, Training Error = 0.205078125
Iteration no. 668, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42381376187555, Training Error = 0.193359375
Iteration no. 669, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42850472926934, Training Error = 0.201171875
Iteration no. 670, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.41988034005891, Training Error = 0.197265625
Iteration no. 671, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.46867720488321, Training Error = 0.224609375
Iteration no. 672, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.408043220059, Training Error = 0.19921875
Iteration no. 673, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45501996078372, Training Error = 0.208984375
Iteration no. 674, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43088350116377, Training Error = 0.203125
Iteration no. 675, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47200164945798, Training Error = 0.23046875
Iteration no. 676, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4432330085306, Training Error = 0.21484375
Iteration no. 677, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44358050230842, Training Error = 0.19921875
Iteration no. 678, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44964876049686, Training Error = 0.21875
Iteration no. 679, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44749708337563, Training Error = 0.197265625
Iteration no. 680, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4244763365914, Training Error = 0.19140625
Iteration no. 681, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.427268397877, Training Error = 0.205078125
Iteration no. 682, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45918344008131, Training Error = 0.22265625
Iteration no. 683, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.40850063179206, Training Error = 0.1796875
Iteration no. 684, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45625653600777, Training Error = 0.197265625
Iteration no. 685, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.4289269562617, Training Error = 0.203125
Iteration no. 686, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.44624619081299, Training Error = 0.20703125
Iteration no. 687, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.51337820498767, Training Error = 0.244140625
Iteration no. 688, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.41593410465603, Training Error = 0.181640625
Iteration no. 689, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42258057609467, Training Error = 0.193359375
Iteration no. 690, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.49776496747662, Training Error = 0.234375
Iteration no. 691, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47157958677883, Training Error = 0.220703125
Iteration no. 692, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43349982030174, Training Error = 0.2109375
Iteration no. 693, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.45909484960503, Training Error = 0.20703125
Iteration no. 694, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.47288729675889, Training Error = 0.228515625
Iteration no. 695, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43329540082097, Training Error = 0.19921875
Iteration no. 696, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42856604146184, Training Error = 0.201171875
Iteration no. 697, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.42593233598472, Training Error = 0.205078125
Iteration no. 698, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43001254937446, Training Error = 0.21484375
Iteration no. 699, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43216927237689, Training Error = 0.19921875
Iteration no. 700, lr = 0.235298, attribute lr =0.02, average batch_loss = 0.43173326649949, Training Error = 0.20703125
Testing... average test_loss = 1.156456551139, average test_pred_err = 0.479
Snapshotting C_model... done
Iteration no. 701, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.4485876633736, Training Error = 0.2265625
Iteration no. 702, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.45471592749751, Training Error = 0.203125
Iteration no. 703, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42922836955147, Training Error = 0.197265625
Iteration no. 704, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43217446140512, Training Error = 0.201171875
Iteration no. 705, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.47041774702016, Training Error = 0.232421875
Iteration no. 706, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.46382507435416, Training Error = 0.23046875
Iteration no. 707, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.46787286902925, Training Error = 0.21875
Iteration no. 708, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44976858814207, Training Error = 0.2109375
Iteration no. 709, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.45080934008548, Training Error = 0.2109375
Iteration no. 710, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44112867110177, Training Error = 0.203125
Iteration no. 711, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42548371869369, Training Error = 0.2109375
Iteration no. 712, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43514114500237, Training Error = 0.228515625
Iteration no. 713, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42304374643815, Training Error = 0.1796875
Iteration no. 714, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.49182187582415, Training Error = 0.23046875
Iteration no. 715, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.45282716171329, Training Error = 0.2109375
Iteration no. 716, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44250158095047, Training Error = 0.212890625
Iteration no. 717, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42903831086691, Training Error = 0.1953125
Iteration no. 718, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44144657785568, Training Error = 0.19921875
Iteration no. 719, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.47161517244941, Training Error = 0.23046875
Iteration no. 720, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.46286116098829, Training Error = 0.216796875
Iteration no. 721, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42949416166604, Training Error = 0.189453125
Iteration no. 722, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.41285090894354, Training Error = 0.189453125
Iteration no. 723, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.45052520775428, Training Error = 0.22265625
Iteration no. 724, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.47022292254071, Training Error = 0.220703125
Iteration no. 725, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.45019452159787, Training Error = 0.22265625
Iteration no. 726, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.48276501082336, Training Error = 0.2421875
Iteration no. 727, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.39294889065267, Training Error = 0.173828125
Iteration no. 728, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.47339551337702, Training Error = 0.2265625
Iteration no. 729, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43029511115922, Training Error = 0.203125
Iteration no. 730, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.410368845505, Training Error = 0.18359375
Iteration no. 731, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43647663302488, Training Error = 0.19140625
Iteration no. 732, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44051687720164, Training Error = 0.208984375
Iteration no. 733, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.3973191121673, Training Error = 0.169921875
Iteration no. 734, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44638344736295, Training Error = 0.19921875
Iteration no. 735, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.48224636654481, Training Error = 0.23046875
Iteration no. 736, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.40612869281962, Training Error = 0.208984375
Iteration no. 737, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.4500130783543, Training Error = 0.205078125
Iteration no. 738, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.48424356808706, Training Error = 0.2421875
Iteration no. 739, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42558775783272, Training Error = 0.189453125
Iteration no. 740, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.41803572060033, Training Error = 0.1875
Iteration no. 741, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44664097259885, Training Error = 0.220703125
Iteration no. 742, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.4564908774901, Training Error = 0.224609375
Iteration no. 743, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.41700859987866, Training Error = 0.20703125
Iteration no. 744, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.449721896171, Training Error = 0.212890625
Iteration no. 745, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44359743232745, Training Error = 0.205078125
Iteration no. 746, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42620982878034, Training Error = 0.181640625
Iteration no. 747, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.47053162832773, Training Error = 0.234375
Iteration no. 748, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.44808081344687, Training Error = 0.21484375
Iteration no. 749, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42013294614073, Training Error = 0.197265625
Iteration no. 750, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.46275222574506, Training Error = 0.212890625
Testing... average test_loss = 1.094504287965, average test_pred_err = 0.455
Iteration no. 751, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.4532118283224, Training Error = 0.22265625
Iteration no. 752, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.45444236644862, Training Error = 0.228515625
Iteration no. 753, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43683055780621, Training Error = 0.216796875
Iteration no. 754, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43204246782918, Training Error = 0.205078125
Iteration no. 755, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.47466819011901, Training Error = 0.248046875
Iteration no. 756, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.43119741116859, Training Error = 0.2109375
Iteration no. 757, lr = 0.1647086, attribute lr =0.02, average batch_loss = 0.42207190813774, Training Error = 0.205078125
