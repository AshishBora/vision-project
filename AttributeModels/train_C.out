Loading pretrained model... done
Training... 
Testing... average test_loss = 0.70890006185917, average test_pred_err = 0.513
Iteration no. 1, lr = 2, average batch_loss = 0.70828518304545, Training Error = 0.5
Iteration no. 2, lr = 2, average batch_loss = 0.69876353157006, Training Error = 0.501953125
Iteration no. 3, lr = 2, average batch_loss = 0.70982246254393, Training Error = 0.560546875
Iteration no. 4, lr = 2, average batch_loss = 0.69676108158396, Training Error = 0.421875
Iteration no. 5, lr = 2, average batch_loss = 0.70053792240636, Training Error = 0.56640625
Iteration no. 6, lr = 2, average batch_loss = 0.6975025022508, Training Error = 0.498046875
Iteration no. 7, lr = 2, average batch_loss = 0.69538146360112, Training Error = 0.529296875
Iteration no. 8, lr = 2, average batch_loss = 0.69236586837857, Training Error = 0.427734375
Iteration no. 9, lr = 2, average batch_loss = 0.69373541946139, Training Error = 0.408203125
Iteration no. 10, lr = 2, average batch_loss = 0.69847386024103, Training Error = 0.564453125
Iteration no. 11, lr = 2, average batch_loss = 0.69344161773927, Training Error = 0.40234375
Iteration no. 12, lr = 2, average batch_loss = 0.69877308334709, Training Error = 0.458984375
Iteration no. 13, lr = 2, average batch_loss = 0.69023253030625, Training Error = 0.43359375
Iteration no. 14, lr = 2, average batch_loss = 0.69508587121315, Training Error = 0.443359375
Iteration no. 15, lr = 2, average batch_loss = 0.69638376148069, Training Error = 0.498046875
Iteration no. 16, lr = 2, average batch_loss = 0.69269501973984, Training Error = 0.435546875
Iteration no. 17, lr = 2, average batch_loss = 0.69469382711816, Training Error = 0.498046875
Iteration no. 18, lr = 2, average batch_loss = 0.70212062560885, Training Error = 0.5
Iteration no. 19, lr = 2, average batch_loss = 0.69300798784519, Training Error = 0.474609375
Iteration no. 20, lr = 2, average batch_loss = 0.70157884964098, Training Error = 0.470703125
Iteration no. 21, lr = 2, average batch_loss = 0.70098482077786, Training Error = 0.498046875
Iteration no. 22, lr = 2, average batch_loss = 0.69505642243281, Training Error = 0.513671875
Iteration no. 23, lr = 2, average batch_loss = 0.69780264166067, Training Error = 0.49609375
Iteration no. 24, lr = 2, average batch_loss = 0.69278186320824, Training Error = 0.46875
Iteration no. 25, lr = 2, average batch_loss = 0.68688977071547, Training Error = 0.50390625
Iteration no. 26, lr = 2, average batch_loss = 0.70053687076567, Training Error = 0.529296875
Iteration no. 27, lr = 2, average batch_loss = 0.69058236363426, Training Error = 0.501953125
Iteration no. 28, lr = 2, average batch_loss = 0.68966693897178, Training Error = 0.421875
Iteration no. 29, lr = 2, average batch_loss = 0.68647066907383, Training Error = 0.443359375
Iteration no. 30, lr = 2, average batch_loss = 0.69223278054699, Training Error = 0.458984375
Iteration no. 31, lr = 2, average batch_loss = 0.6928426013037, Training Error = 0.494140625
Iteration no. 32, lr = 2, average batch_loss = 0.68060386780123, Training Error = 0.30859375
Iteration no. 33, lr = 2, average batch_loss = 0.67950152358692, Training Error = 0.369140625
Iteration no. 34, lr = 2, average batch_loss = 0.68364482104367, Training Error = 0.568359375
Iteration no. 35, lr = 2, average batch_loss = 0.6756479426928, Training Error = 0.40625
Iteration no. 36, lr = 2, average batch_loss = 0.66696949340442, Training Error = 0.421875
Iteration no. 37, lr = 2, average batch_loss = 0.67887086111783, Training Error = 0.53125
Iteration no. 38, lr = 2, average batch_loss = 0.65619569644082, Training Error = 0.2734375
Iteration no. 39, lr = 2, average batch_loss = 0.66136177224777, Training Error = 0.470703125
Iteration no. 40, lr = 2, average batch_loss = 0.66302036341061, Training Error = 0.578125
Iteration no. 41, lr = 2, average batch_loss = 0.67591654777321, Training Error = 0.5625
Iteration no. 42, lr = 2, average batch_loss = 0.66243846591238, Training Error = 0.2890625
Iteration no. 43, lr = 2, average batch_loss = 0.64751743395075, Training Error = 0.330078125
Iteration no. 44, lr = 2, average batch_loss = 0.62019227399034, Training Error = 0.28125
Iteration no. 45, lr = 2, average batch_loss = 0.59555424255411, Training Error = 0.201171875
Iteration no. 46, lr = 2, average batch_loss = 0.60392560813242, Training Error = 0.23828125
Iteration no. 47, lr = 2, average batch_loss = 0.82475681324879, Training Error = 0.58984375
Iteration no. 48, lr = 2, average batch_loss = 0.73770764319215, Training Error = 0.384765625
Iteration no. 49, lr = 2, average batch_loss = 0.73250925781024, Training Error = 0.65234375
Iteration no. 50, lr = 2, average batch_loss = 0.69597921369405, Training Error = 0.474609375
Testing... average test_loss = 0.58927261038037, average test_pred_err = 0.342
Iteration no. 51, lr = 2, average batch_loss = 0.57085366279131, Training Error = 0.32421875
Iteration no. 52, lr = 2, average batch_loss = 0.54584842271217, Training Error = 0.12109375
Iteration no. 53, lr = 2, average batch_loss = 0.55797268002338, Training Error = 0.1875
Iteration no. 54, lr = 2, average batch_loss = 0.79491685264894, Training Error = 0.62109375
Iteration no. 55, lr = 2, average batch_loss = 0.69650130618831, Training Error = 0.447265625
Iteration no. 56, lr = 2, average batch_loss = 0.61370493814667, Training Error = 0.30859375
Iteration no. 57, lr = 2, average batch_loss = 0.52015274790277, Training Error = 0.197265625
Iteration no. 58, lr = 2, average batch_loss = 0.54316984431736, Training Error = 0.3046875
Iteration no. 59, lr = 2, average batch_loss = 0.81634567091957, Training Error = 0.564453125
Iteration no. 60, lr = 2, average batch_loss = 0.72737689680229, Training Error = 0.373046875
Iteration no. 61, lr = 2, average batch_loss = 0.68073824294109, Training Error = 0.587890625
Iteration no. 62, lr = 2, average batch_loss = 0.57793884571835, Training Error = 0.2734375
Iteration no. 63, lr = 2, average batch_loss = 0.52960083025541, Training Error = 0.212890625
Iteration no. 64, lr = 2, average batch_loss = 0.53727624879016, Training Error = 0.326171875
Iteration no. 65, lr = 2, average batch_loss = 0.64477800031853, Training Error = 0.583984375
Iteration no. 66, lr = 2, average batch_loss = 0.63823360905408, Training Error = 0.396484375
Iteration no. 67, lr = 2, average batch_loss = 0.5068564175902, Training Error = 0.25
Iteration no. 68, lr = 2, average batch_loss = 0.49771845930527, Training Error = 0.19921875
Iteration no. 69, lr = 2, average batch_loss = 0.56289577595342, Training Error = 0.37890625
Iteration no. 70, lr = 2, average batch_loss = 0.81884682074043, Training Error = 0.5390625
Iteration no. 71, lr = 2, average batch_loss = 0.62341004894469, Training Error = 0.29296875
Iteration no. 72, lr = 2, average batch_loss = 0.60822180836899, Training Error = 0.25390625
Iteration no. 73, lr = 2, average batch_loss = 0.46440043811574, Training Error = 0.21875
Iteration no. 74, lr = 2, average batch_loss = 0.42499393565917, Training Error = 0.14453125
Iteration no. 75, lr = 2, average batch_loss = 0.41658638713547, Training Error = 0.130859375
Iteration no. 76, lr = 2, average batch_loss = 0.43615120190221, Training Error = 0.13671875
Iteration no. 77, lr = 2, average batch_loss = 0.57033904350797, Training Error = 0.318359375
Iteration no. 78, lr = 2, average batch_loss = 0.74059732638297, Training Error = 0.5078125
Iteration no. 79, lr = 2, average batch_loss = 0.71419275780165, Training Error = 0.322265625
Iteration no. 80, lr = 2, average batch_loss = 0.60168868589911, Training Error = 0.314453125
Iteration no. 81, lr = 2, average batch_loss = 0.50478951467106, Training Error = 0.23828125
Iteration no. 82, lr = 2, average batch_loss = 0.39946196667235, Training Error = 0.134765625
Iteration no. 83, lr = 2, average batch_loss = 0.45585760224204, Training Error = 0.158203125
Iteration no. 84, lr = 2, average batch_loss = 0.70914746000074, Training Error = 0.51953125
Iteration no. 85, lr = 2, average batch_loss = 0.81266908236305, Training Error = 0.697265625
Iteration no. 86, lr = 2, average batch_loss = 0.75655697800711, Training Error = 0.369140625
Iteration no. 87, lr = 2, average batch_loss = 0.44813386567628, Training Error = 0.189453125
Iteration no. 88, lr = 2, average batch_loss = 0.54394436884155, Training Error = 0.3984375
Iteration no. 89, lr = 2, average batch_loss = 0.67234733677355, Training Error = 0.478515625
Iteration no. 90, lr = 2, average batch_loss = 0.60216460778699, Training Error = 0.251953125
Iteration no. 91, lr = 2, average batch_loss = 0.46220107149607, Training Error = 0.22265625
Iteration no. 92, lr = 2, average batch_loss = 0.43093538343921, Training Error = 0.1953125
Iteration no. 93, lr = 2, average batch_loss = 0.40607983008747, Training Error = 0.173828125
Iteration no. 94, lr = 2, average batch_loss = 0.37243421542975, Training Error = 0.11328125
Iteration no. 95, lr = 2, average batch_loss = 0.48644006113649, Training Error = 0.1875
Iteration no. 96, lr = 2, average batch_loss = 0.88351111422974, Training Error = 0.57421875
Iteration no. 97, lr = 2, average batch_loss = 0.72525268274244, Training Error = 0.51953125
Iteration no. 98, lr = 2, average batch_loss = 0.67069456829205, Training Error = 0.345703125
Iteration no. 99, lr = 2, average batch_loss = 0.48683510234033, Training Error = 0.193359375
Iteration no. 100, lr = 2, average batch_loss = 0.46915000136023, Training Error = 0.173828125
Testing... average test_loss = 0.43615733970454, average test_pred_err = 0.168
Iteration no. 101, lr = 1.4, average batch_loss = 0.41679034555965, Training Error = 0.169921875
Iteration no. 102, lr = 1.4, average batch_loss = 0.39742280624187, Training Error = 0.138671875
Iteration no. 103, lr = 1.4, average batch_loss = 0.37459986061248, Training Error = 0.138671875
Iteration no. 104, lr = 1.4, average batch_loss = 0.41812390729929, Training Error = 0.169921875
Iteration no. 105, lr = 1.4, average batch_loss = 0.42808699213589, Training Error = 0.138671875
Iteration no. 106, lr = 1.4, average batch_loss = 0.3842918504577, Training Error = 0.154296875
Iteration no. 107, lr = 1.4, average batch_loss = 0.40761190404729, Training Error = 0.150390625
Iteration no. 108, lr = 1.4, average batch_loss = 0.4394972747884, Training Error = 0.212890625
Iteration no. 109, lr = 1.4, average batch_loss = 0.39348080867037, Training Error = 0.130859375
Iteration no. 110, lr = 1.4, average batch_loss = 0.42469579200751, Training Error = 0.1875
Iteration no. 111, lr = 1.4, average batch_loss = 0.42588913903314, Training Error = 0.162109375
Iteration no. 112, lr = 1.4, average batch_loss = 0.34753029412746, Training Error = 0.125
Iteration no. 113, lr = 1.4, average batch_loss = 0.36658906621771, Training Error = 0.162109375
Iteration no. 114, lr = 1.4, average batch_loss = 0.37067176843041, Training Error = 0.140625
Iteration no. 115, lr = 1.4, average batch_loss = 0.37645888252691, Training Error = 0.115234375
Iteration no. 116, lr = 1.4, average batch_loss = 0.47865547069471, Training Error = 0.24609375
Iteration no. 117, lr = 1.4, average batch_loss = 0.47965445500209, Training Error = 0.111328125
Iteration no. 118, lr = 1.4, average batch_loss = 0.43713941011454, Training Error = 0.201171875
Iteration no. 119, lr = 1.4, average batch_loss = 0.33692329806029, Training Error = 0.13671875
Iteration no. 120, lr = 1.4, average batch_loss = 0.3757757100016, Training Error = 0.15234375
Iteration no. 121, lr = 1.4, average batch_loss = 0.33186969651025, Training Error = 0.146484375
Iteration no. 122, lr = 1.4, average batch_loss = 0.3454274840962, Training Error = 0.1484375
Iteration no. 123, lr = 1.4, average batch_loss = 0.31978342622194, Training Error = 0.130859375
Iteration no. 124, lr = 1.4, average batch_loss = 0.40046288910918, Training Error = 0.18359375
Iteration no. 125, lr = 1.4, average batch_loss = 0.50278577807136, Training Error = 0.36328125
Iteration no. 126, lr = 1.4, average batch_loss = 0.46346107234998, Training Error = 0.208984375
Iteration no. 127, lr = 1.4, average batch_loss = 0.33280903375237, Training Error = 0.14453125
Iteration no. 128, lr = 1.4, average batch_loss = 0.3163393021806, Training Error = 0.1484375
Iteration no. 129, lr = 1.4, average batch_loss = 0.31037833624006, Training Error = 0.142578125
Iteration no. 130, lr = 1.4, average batch_loss = 0.28781398273746, Training Error = 0.1328125
Iteration no. 131, lr = 1.4, average batch_loss = 0.2735978328047, Training Error = 0.1171875
Iteration no. 132, lr = 1.4, average batch_loss = 0.26849888843086, Training Error = 0.10546875
Iteration no. 133, lr = 1.4, average batch_loss = 0.28233959446073, Training Error = 0.123046875
Iteration no. 134, lr = 1.4, average batch_loss = 0.24922281906687, Training Error = 0.09375
Iteration no. 135, lr = 1.4, average batch_loss = 0.25759689777327, Training Error = 0.087890625
Iteration no. 136, lr = 1.4, average batch_loss = 0.34517802295131, Training Error = 0.158203125
Iteration no. 137, lr = 1.4, average batch_loss = 0.49728659282898, Training Error = 0.32421875
Iteration no. 138, lr = 1.4, average batch_loss = 0.64857499141643, Training Error = 0.373046875
Iteration no. 139, lr = 1.4, average batch_loss = 0.55269606007718, Training Error = 0.310546875
Iteration no. 140, lr = 1.4, average batch_loss = 0.3683527393605, Training Error = 0.171875
Iteration no. 141, lr = 1.4, average batch_loss = 0.38934410636766, Training Error = 0.181640625
Iteration no. 142, lr = 1.4, average batch_loss = 0.26318369705143, Training Error = 0.125
Iteration no. 143, lr = 1.4, average batch_loss = 0.33128576063357, Training Error = 0.15625
Iteration no. 144, lr = 1.4, average batch_loss = 0.25810205017836, Training Error = 0.119140625
Iteration no. 145, lr = 1.4, average batch_loss = 0.25659063592446, Training Error = 0.09375
Iteration no. 146, lr = 1.4, average batch_loss = 0.35231106335359, Training Error = 0.177734375
Iteration no. 147, lr = 1.4, average batch_loss = 0.34487766434519, Training Error = 0.150390625
Iteration no. 148, lr = 1.4, average batch_loss = 0.31718640163554, Training Error = 0.15234375
Iteration no. 149, lr = 1.4, average batch_loss = 0.20748763457869, Training Error = 0.091796875
Iteration no. 150, lr = 1.4, average batch_loss = 0.22011311407583, Training Error = 0.07421875
Testing... average test_loss = 0.25636531233154, average test_pred_err = 0.091
Iteration no. 151, lr = 1.4, average batch_loss = 0.26968249015309, Training Error = 0.1171875
Iteration no. 152, lr = 1.4, average batch_loss = 0.24837028464489, Training Error = 0.109375
Iteration no. 153, lr = 1.4, average batch_loss = 0.24766182250746, Training Error = 0.095703125
Iteration no. 154, lr = 1.4, average batch_loss = 0.27354410670997, Training Error = 0.1015625
Iteration no. 155, lr = 1.4, average batch_loss = 0.36107848923115, Training Error = 0.185546875
Iteration no. 156, lr = 1.4, average batch_loss = 0.27842175975727, Training Error = 0.09765625
Iteration no. 157, lr = 1.4, average batch_loss = 0.29944298536541, Training Error = 0.134765625
Iteration no. 158, lr = 1.4, average batch_loss = 0.25958001507161, Training Error = 0.095703125
Iteration no. 159, lr = 1.4, average batch_loss = 0.32701221863761, Training Error = 0.173828125
Iteration no. 160, lr = 1.4, average batch_loss = 0.27619106547142, Training Error = 0.10546875
Iteration no. 161, lr = 1.4, average batch_loss = 0.30930513111944, Training Error = 0.158203125
Iteration no. 162, lr = 1.4, average batch_loss = 0.23931187805999, Training Error = 0.107421875
Iteration no. 163, lr = 1.4, average batch_loss = 0.28057370485784, Training Error = 0.107421875
Iteration no. 164, lr = 1.4, average batch_loss = 0.25334835752072, Training Error = 0.09375
Iteration no. 165, lr = 1.4, average batch_loss = 0.31900114043506, Training Error = 0.154296875
Iteration no. 166, lr = 1.4, average batch_loss = 0.21604687964942, Training Error = 0.080078125
Iteration no. 167, lr = 1.4, average batch_loss = 0.23901719135143, Training Error = 0.111328125
Iteration no. 168, lr = 1.4, average batch_loss = 0.21885739528245, Training Error = 0.080078125
Iteration no. 169, lr = 1.4, average batch_loss = 0.24797028979533, Training Error = 0.12109375
Iteration no. 170, lr = 1.4, average batch_loss = 0.25964160549737, Training Error = 0.119140625
Iteration no. 171, lr = 1.4, average batch_loss = 0.2803315497036, Training Error = 0.126953125
Iteration no. 172, lr = 1.4, average batch_loss = 0.2064146604596, Training Error = 0.091796875
Iteration no. 173, lr = 1.4, average batch_loss = 0.2194420574331, Training Error = 0.103515625
Iteration no. 174, lr = 1.4, average batch_loss = 0.25693643460439, Training Error = 0.107421875
Iteration no. 175, lr = 1.4, average batch_loss = 0.21899388382357, Training Error = 0.1015625
Iteration no. 176, lr = 1.4, average batch_loss = 0.23621376619284, Training Error = 0.091796875
Iteration no. 177, lr = 1.4, average batch_loss = 0.2543138480448, Training Error = 0.103515625
Iteration no. 178, lr = 1.4, average batch_loss = 0.24118790983485, Training Error = 0.123046875
Iteration no. 179, lr = 1.4, average batch_loss = 0.2761255758013, Training Error = 0.1328125
Iteration no. 180, lr = 1.4, average batch_loss = 0.270269767252, Training Error = 0.138671875
Iteration no. 181, lr = 1.4, average batch_loss = 0.27872647974496, Training Error = 0.138671875
Iteration no. 182, lr = 1.4, average batch_loss = 0.27626049785158, Training Error = 0.16796875
Iteration no. 183, lr = 1.4, average batch_loss = 0.42086348862727, Training Error = 0.24609375
Iteration no. 184, lr = 1.4, average batch_loss = 0.42337684089007, Training Error = 0.26171875
Iteration no. 185, lr = 1.4, average batch_loss = 0.31304399918599, Training Error = 0.158203125
Iteration no. 186, lr = 1.4, average batch_loss = 0.21627737707369, Training Error = 0.111328125
Iteration no. 187, lr = 1.4, average batch_loss = 0.32366189565279, Training Error = 0.13671875
Iteration no. 188, lr = 1.4, average batch_loss = 0.28572539952287, Training Error = 0.140625
Iteration no. 189, lr = 1.4, average batch_loss = 0.20838182883037, Training Error = 0.095703125
Iteration no. 190, lr = 1.4, average batch_loss = 0.18626297601322, Training Error = 0.078125
Iteration no. 191, lr = 1.4, average batch_loss = 0.17638366339376, Training Error = 0.060546875
Iteration no. 192, lr = 1.4, average batch_loss = 0.21224411237552, Training Error = 0.08203125
Iteration no. 193, lr = 1.4, average batch_loss = 0.19755936409408, Training Error = 0.0625
Iteration no. 194, lr = 1.4, average batch_loss = 0.19378204859842, Training Error = 0.091796875
Iteration no. 195, lr = 1.4, average batch_loss = 0.21953259418681, Training Error = 0.08984375
Iteration no. 196, lr = 1.4, average batch_loss = 0.21885007650953, Training Error = 0.10546875
Iteration no. 197, lr = 1.4, average batch_loss = 0.30549233662916, Training Error = 0.181640625
Iteration no. 198, lr = 1.4, average batch_loss = 0.30028326500314, Training Error = 0.185546875
Iteration no. 199, lr = 1.4, average batch_loss = 0.26845255675922, Training Error = 0.158203125
Iteration no. 200, lr = 1.4, average batch_loss = 0.27436956564228, Training Error = 0.107421875
Testing... average test_loss = 0.22342226750973, average test_pred_err = 0.108
Iteration no. 201, lr = 0.98, average batch_loss = 0.20424723480468, Training Error = 0.1015625
Iteration no. 202, lr = 0.98, average batch_loss = 0.14698080815112, Training Error = 0.072265625
Iteration no. 203, lr = 0.98, average batch_loss = 0.14704371708993, Training Error = 0.0546875
Iteration no. 204, lr = 0.98, average batch_loss = 0.18541953464589, Training Error = 0.087890625
Iteration no. 205, lr = 0.98, average batch_loss = 0.14410915321628, Training Error = 0.056640625
Iteration no. 206, lr = 0.98, average batch_loss = 0.17384936335086, Training Error = 0.083984375
Iteration no. 207, lr = 0.98, average batch_loss = 0.17076541316894, Training Error = 0.068359375
Iteration no. 208, lr = 0.98, average batch_loss = 0.20738134956725, Training Error = 0.091796875
Iteration no. 209, lr = 0.98, average batch_loss = 0.18037378799416, Training Error = 0.064453125
Iteration no. 210, lr = 0.98, average batch_loss = 0.20340173094963, Training Error = 0.091796875
Iteration no. 211, lr = 0.98, average batch_loss = 0.16782734789057, Training Error = 0.080078125
Iteration no. 212, lr = 0.98, average batch_loss = 0.19798099900763, Training Error = 0.08984375
Iteration no. 213, lr = 0.98, average batch_loss = 0.14561726542339, Training Error = 0.046875
Iteration no. 214, lr = 0.98, average batch_loss = 0.17602669424513, Training Error = 0.06640625
Iteration no. 215, lr = 0.98, average batch_loss = 0.17202318253828, Training Error = 0.05859375
Iteration no. 216, lr = 0.98, average batch_loss = 0.17038852188489, Training Error = 0.056640625
Iteration no. 217, lr = 0.98, average batch_loss = 0.16281934232628, Training Error = 0.048828125
Iteration no. 218, lr = 0.98, average batch_loss = 0.22380400987495, Training Error = 0.123046875
Iteration no. 219, lr = 0.98, average batch_loss = 0.16001961615936, Training Error = 0.060546875
Iteration no. 220, lr = 0.98, average batch_loss = 0.18588803566316, Training Error = 0.083984375
Iteration no. 221, lr = 0.98, average batch_loss = 0.15465277972234, Training Error = 0.04296875
Iteration no. 222, lr = 0.98, average batch_loss = 0.152942288102, Training Error = 0.06640625
Iteration no. 223, lr = 0.98, average batch_loss = 0.14340297050145, Training Error = 0.05078125
Iteration no. 224, lr = 0.98, average batch_loss = 0.17250632210482, Training Error = 0.068359375
Iteration no. 225, lr = 0.98, average batch_loss = 0.20612970029261, Training Error = 0.115234375
Iteration no. 226, lr = 0.98, average batch_loss = 0.21500763178254, Training Error = 0.115234375
Iteration no. 227, lr = 0.98, average batch_loss = 0.16533412081191, Training Error = 0.0703125
Iteration no. 228, lr = 0.98, average batch_loss = 0.15499215964676, Training Error = 0.0625
Iteration no. 229, lr = 0.98, average batch_loss = 0.16247312334207, Training Error = 0.0546875
Iteration no. 230, lr = 0.98, average batch_loss = 0.17388948884607, Training Error = 0.078125
Iteration no. 231, lr = 0.98, average batch_loss = 0.19484993254926, Training Error = 0.09765625
Iteration no. 232, lr = 0.98, average batch_loss = 0.14501694935241, Training Error = 0.05859375
Iteration no. 233, lr = 0.98, average batch_loss = 0.15118867395807, Training Error = 0.05859375
Iteration no. 234, lr = 0.98, average batch_loss = 0.18967788926781, Training Error = 0.08203125
Iteration no. 235, lr = 0.98, average batch_loss = 0.15397499844549, Training Error = 0.048828125
Iteration no. 236, lr = 0.98, average batch_loss = 0.15437539296275, Training Error = 0.068359375
Iteration no. 237, lr = 0.98, average batch_loss = 0.15769623181648, Training Error = 0.044921875
Iteration no. 238, lr = 0.98, average batch_loss = 0.16889858511106, Training Error = 0.072265625
Iteration no. 239, lr = 0.98, average batch_loss = 0.14733806905207, Training Error = 0.0390625
Iteration no. 240, lr = 0.98, average batch_loss = 0.1808199830181, Training Error = 0.091796875
Iteration no. 241, lr = 0.98, average batch_loss = 0.19135416820775, Training Error = 0.0859375
Iteration no. 242, lr = 0.98, average batch_loss = 0.18076614170375, Training Error = 0.087890625
Iteration no. 243, lr = 0.98, average batch_loss = 0.12234432700718, Training Error = 0.046875
Iteration no. 244, lr = 0.98, average batch_loss = 0.13585977443131, Training Error = 0.041015625
Iteration no. 245, lr = 0.98, average batch_loss = 0.1429733730616, Training Error = 0.052734375
Iteration no. 246, lr = 0.98, average batch_loss = 0.16430154307366, Training Error = 0.05859375
Iteration no. 247, lr = 0.98, average batch_loss = 0.18992322256667, Training Error = 0.09765625
Iteration no. 248, lr = 0.98, average batch_loss = 0.18089154478301, Training Error = 0.107421875
Iteration no. 249, lr = 0.98, average batch_loss = 0.16369342945047, Training Error = 0.052734375
Iteration no. 250, lr = 0.98, average batch_loss = 0.17692583216239, Training Error = 0.080078125
Testing... average test_loss = 0.15734272360839, average test_pred_err = 0.06
Iteration no. 251, lr = 0.98, average batch_loss = 0.13249685673067, Training Error = 0.044921875
Iteration no. 252, lr = 0.98, average batch_loss = 0.16082609003467, Training Error = 0.0703125
Iteration no. 253, lr = 0.98, average batch_loss = 0.15380491264329, Training Error = 0.076171875
Iteration no. 254, lr = 0.98, average batch_loss = 0.18420453182278, Training Error = 0.0703125
Iteration no. 255, lr = 0.98, average batch_loss = 0.17592037466203, Training Error = 0.0859375
Iteration no. 256, lr = 0.98, average batch_loss = 0.15816810433275, Training Error = 0.064453125
Iteration no. 257, lr = 0.98, average batch_loss = 0.14189984669515, Training Error = 0.048828125
Iteration no. 258, lr = 0.98, average batch_loss = 0.1424141162593, Training Error = 0.044921875
Iteration no. 259, lr = 0.98, average batch_loss = 0.15937403704161, Training Error = 0.0625
Iteration no. 260, lr = 0.98, average batch_loss = 0.16388460312111, Training Error = 0.068359375
Iteration no. 261, lr = 0.98, average batch_loss = 0.16870913093078, Training Error = 0.087890625
Iteration no. 262, lr = 0.98, average batch_loss = 0.13683306452185, Training Error = 0.056640625
Iteration no. 263, lr = 0.98, average batch_loss = 0.13851497327202, Training Error = 0.0546875
Iteration no. 264, lr = 0.98, average batch_loss = 0.15092906893085, Training Error = 0.072265625
Iteration no. 265, lr = 0.98, average batch_loss = 0.17381149153278, Training Error = 0.083984375
Iteration no. 266, lr = 0.98, average batch_loss = 0.1870808398989, Training Error = 0.080078125
Iteration no. 267, lr = 0.98, average batch_loss = 0.13077200413805, Training Error = 0.0546875
Iteration no. 268, lr = 0.98, average batch_loss = 0.16711385999883, Training Error = 0.078125
Iteration no. 269, lr = 0.98, average batch_loss = 0.15347344140831, Training Error = 0.07421875
Iteration no. 270, lr = 0.98, average batch_loss = 0.16218944313926, Training Error = 0.078125
Iteration no. 271, lr = 0.98, average batch_loss = 0.16215243885717, Training Error = 0.064453125
Iteration no. 272, lr = 0.98, average batch_loss = 0.17645192192186, Training Error = 0.0859375
Iteration no. 273, lr = 0.98, average batch_loss = 0.12855396134041, Training Error = 0.033203125
Iteration no. 274, lr = 0.98, average batch_loss = 0.14186187656589, Training Error = 0.052734375
Iteration no. 275, lr = 0.98, average batch_loss = 0.13495587507007, Training Error = 0.046875
Iteration no. 276, lr = 0.98, average batch_loss = 0.14047678359771, Training Error = 0.064453125
Iteration no. 277, lr = 0.98, average batch_loss = 0.13740942242028, Training Error = 0.046875
Iteration no. 278, lr = 0.98, average batch_loss = 0.16999448380498, Training Error = 0.08203125
Iteration no. 279, lr = 0.98, average batch_loss = 0.15854966372365, Training Error = 0.06640625
Iteration no. 280, lr = 0.98, average batch_loss = 0.13285952667449, Training Error = 0.068359375
Iteration no. 281, lr = 0.98, average batch_loss = 0.15048357638755, Training Error = 0.05859375
Iteration no. 282, lr = 0.98, average batch_loss = 0.12970253810138, Training Error = 0.052734375
Iteration no. 283, lr = 0.98, average batch_loss = 0.10472150619111, Training Error = 0.04296875
Iteration no. 284, lr = 0.98, average batch_loss = 0.139456218104, Training Error = 0.056640625
Iteration no. 285, lr = 0.98, average batch_loss = 0.17974415146465, Training Error = 0.099609375
Iteration no. 286, lr = 0.98, average batch_loss = 0.16639902871133, Training Error = 0.0859375
Iteration no. 287, lr = 0.98, average batch_loss = 0.12696995132011, Training Error = 0.04296875
Iteration no. 288, lr = 0.98, average batch_loss = 0.18655230889114, Training Error = 0.080078125
Iteration no. 289, lr = 0.98, average batch_loss = 0.2076587456699, Training Error = 0.12109375
Iteration no. 290, lr = 0.98, average batch_loss = 0.20891273892907, Training Error = 0.119140625
Iteration no. 291, lr = 0.98, average batch_loss = 0.1687867255799, Training Error = 0.0859375
Iteration no. 292, lr = 0.98, average batch_loss = 0.12877579872813, Training Error = 0.052734375
Iteration no. 293, lr = 0.98, average batch_loss = 0.13557886011985, Training Error = 0.064453125
Iteration no. 294, lr = 0.98, average batch_loss = 0.1274816832305, Training Error = 0.048828125
Iteration no. 295, lr = 0.98, average batch_loss = 0.1514874920258, Training Error = 0.0625
Iteration no. 296, lr = 0.98, average batch_loss = 0.17448926553912, Training Error = 0.08203125
Iteration no. 297, lr = 0.98, average batch_loss = 0.12734213676858, Training Error = 0.052734375
Iteration no. 298, lr = 0.98, average batch_loss = 0.16200979069347, Training Error = 0.07421875
Iteration no. 299, lr = 0.98, average batch_loss = 0.10824077084805, Training Error = 0.052734375
Iteration no. 300, lr = 0.98, average batch_loss = 0.12438238110093, Training Error = 0.04296875
Testing... average test_loss = 0.12433380369427, average test_pred_err = 0.057
Iteration no. 301, lr = 0.686, average batch_loss = 0.13812315687504, Training Error = 0.05859375
Iteration no. 302, lr = 0.686, average batch_loss = 0.12230096104758, Training Error = 0.05078125
Iteration no. 303, lr = 0.686, average batch_loss = 0.11044657470761, Training Error = 0.0234375
Iteration no. 304, lr = 0.686, average batch_loss = 0.14891822589942, Training Error = 0.060546875
Iteration no. 305, lr = 0.686, average batch_loss = 0.099685756113525, Training Error = 0.033203125
Iteration no. 306, lr = 0.686, average batch_loss = 0.14506414381963, Training Error = 0.05859375
Iteration no. 307, lr = 0.686, average batch_loss = 0.12270924910732, Training Error = 0.033203125
Iteration no. 308, lr = 0.686, average batch_loss = 0.12484506565651, Training Error = 0.052734375
Iteration no. 309, lr = 0.686, average batch_loss = 0.11937907394655, Training Error = 0.048828125
Iteration no. 310, lr = 0.686, average batch_loss = 0.13251627376763, Training Error = 0.060546875
Iteration no. 311, lr = 0.686, average batch_loss = 0.14301291369777, Training Error = 0.068359375
Iteration no. 312, lr = 0.686, average batch_loss = 0.13211061178241, Training Error = 0.056640625
Iteration no. 313, lr = 0.686, average batch_loss = 0.11561190356004, Training Error = 0.037109375
Iteration no. 314, lr = 0.686, average batch_loss = 0.10816297951814, Training Error = 0.0390625
Iteration no. 315, lr = 0.686, average batch_loss = 0.11644507854098, Training Error = 0.044921875
Iteration no. 316, lr = 0.686, average batch_loss = 0.12165557039897, Training Error = 0.033203125
Iteration no. 317, lr = 0.686, average batch_loss = 0.13035804553301, Training Error = 0.048828125
Iteration no. 318, lr = 0.686, average batch_loss = 0.14334811137832, Training Error = 0.068359375
Iteration no. 319, lr = 0.686, average batch_loss = 0.1126462505659, Training Error = 0.0390625
Iteration no. 320, lr = 0.686, average batch_loss = 0.10694168419494, Training Error = 0.03125
Iteration no. 321, lr = 0.686, average batch_loss = 0.11634135675544, Training Error = 0.03515625
Iteration no. 322, lr = 0.686, average batch_loss = 0.12378037022902, Training Error = 0.05078125
Iteration no. 323, lr = 0.686, average batch_loss = 0.11110938409372, Training Error = 0.029296875
Iteration no. 324, lr = 0.686, average batch_loss = 0.11368046213632, Training Error = 0.041015625
Iteration no. 325, lr = 0.686, average batch_loss = 0.11111410770428, Training Error = 0.046875
Iteration no. 326, lr = 0.686, average batch_loss = 0.10627492599643, Training Error = 0.037109375
Iteration no. 327, lr = 0.686, average batch_loss = 0.097672731779741, Training Error = 0.03125
Iteration no. 328, lr = 0.686, average batch_loss = 0.13199234470826, Training Error = 0.056640625
Iteration no. 329, lr = 0.686, average batch_loss = 0.10624789520667, Training Error = 0.029296875
Iteration no. 330, lr = 0.686, average batch_loss = 0.13807297151909, Training Error = 0.056640625
Iteration no. 331, lr = 0.686, average batch_loss = 0.12027889053584, Training Error = 0.041015625
Iteration no. 332, lr = 0.686, average batch_loss = 0.1456416616227, Training Error = 0.060546875
Iteration no. 333, lr = 0.686, average batch_loss = 0.14381085093585, Training Error = 0.0546875
Iteration no. 334, lr = 0.686, average batch_loss = 0.15453272289883, Training Error = 0.072265625
Iteration no. 335, lr = 0.686, average batch_loss = 0.1147324047901, Training Error = 0.0390625
Iteration no. 336, lr = 0.686, average batch_loss = 0.13059658933068, Training Error = 0.05859375
Iteration no. 337, lr = 0.686, average batch_loss = 0.12528867365534, Training Error = 0.05078125
Iteration no. 338, lr = 0.686, average batch_loss = 0.10040231265638, Training Error = 0.029296875
Iteration no. 339, lr = 0.686, average batch_loss = 0.12004394909825, Training Error = 0.052734375
Iteration no. 340, lr = 0.686, average batch_loss = 0.14157428053536, Training Error = 0.076171875
Iteration no. 341, lr = 0.686, average batch_loss = 0.1113057739832, Training Error = 0.0390625
Iteration no. 342, lr = 0.686, average batch_loss = 0.1313525897366, Training Error = 0.05859375
Iteration no. 343, lr = 0.686, average batch_loss = 0.10752052240058, Training Error = 0.037109375
Iteration no. 344, lr = 0.686, average batch_loss = 0.10079089685611, Training Error = 0.044921875
Iteration no. 345, lr = 0.686, average batch_loss = 0.10928712931975, Training Error = 0.033203125
Iteration no. 346, lr = 0.686, average batch_loss = 0.12159217417343, Training Error = 0.046875
Iteration no. 347, lr = 0.686, average batch_loss = 0.092898660621458, Training Error = 0.025390625
Iteration no. 348, lr = 0.686, average batch_loss = 0.1205486902442, Training Error = 0.052734375
Iteration no. 349, lr = 0.686, average batch_loss = 0.11098485076533, Training Error = 0.048828125
Iteration no. 350, lr = 0.686, average batch_loss = 0.12128331671299, Training Error = 0.056640625
Testing... average test_loss = 0.099465753231624, average test_pred_err = 0.031
Iteration no. 351, lr = 0.686, average batch_loss = 0.092786187012344, Training Error = 0.03125
Iteration no. 352, lr = 0.686, average batch_loss = 0.10416878886061, Training Error = 0.03515625
Iteration no. 353, lr = 0.686, average batch_loss = 0.11778457806102, Training Error = 0.044921875
Iteration no. 354, lr = 0.686, average batch_loss = 0.11852433735508, Training Error = 0.05078125
Iteration no. 355, lr = 0.686, average batch_loss = 0.10441787933105, Training Error = 0.041015625
Iteration no. 356, lr = 0.686, average batch_loss = 0.10200225768453, Training Error = 0.021484375
Iteration no. 357, lr = 0.686, average batch_loss = 0.11011620815857, Training Error = 0.04296875
Iteration no. 358, lr = 0.686, average batch_loss = 0.11138196587362, Training Error = 0.04296875
Iteration no. 359, lr = 0.686, average batch_loss = 0.11427341534839, Training Error = 0.046875
Iteration no. 360, lr = 0.686, average batch_loss = 0.12815801831532, Training Error = 0.0625
Iteration no. 361, lr = 0.686, average batch_loss = 0.11031919903631, Training Error = 0.046875
Iteration no. 362, lr = 0.686, average batch_loss = 0.12110129851993, Training Error = 0.044921875
Iteration no. 363, lr = 0.686, average batch_loss = 0.11034775072696, Training Error = 0.025390625
Iteration no. 364, lr = 0.686, average batch_loss = 0.091037151532934, Training Error = 0.013671875
Iteration no. 365, lr = 0.686, average batch_loss = 0.10974017281908, Training Error = 0.05078125
Iteration no. 366, lr = 0.686, average batch_loss = 0.10466872836011, Training Error = 0.044921875
Iteration no. 367, lr = 0.686, average batch_loss = 0.10466990335828, Training Error = 0.037109375
Iteration no. 368, lr = 0.686, average batch_loss = 0.14688454567206, Training Error = 0.0625
Iteration no. 369, lr = 0.686, average batch_loss = 0.1197828089177, Training Error = 0.044921875
Iteration no. 370, lr = 0.686, average batch_loss = 0.12425844992121, Training Error = 0.060546875
Iteration no. 371, lr = 0.686, average batch_loss = 0.11458766873401, Training Error = 0.03515625
Iteration no. 372, lr = 0.686, average batch_loss = 0.14261492170099, Training Error = 0.06640625
Iteration no. 373, lr = 0.686, average batch_loss = 0.097080052954515, Training Error = 0.033203125
Iteration no. 374, lr = 0.686, average batch_loss = 0.10212304414956, Training Error = 0.03515625
Iteration no. 375, lr = 0.686, average batch_loss = 0.096423278219035, Training Error = 0.037109375
Iteration no. 376, lr = 0.686, average batch_loss = 0.10631595489102, Training Error = 0.052734375
Iteration no. 377, lr = 0.686, average batch_loss = 0.089842058874636, Training Error = 0.03515625
Iteration no. 378, lr = 0.686, average batch_loss = 0.12073245054002, Training Error = 0.046875
Iteration no. 379, lr = 0.686, average batch_loss = 0.10458859400568, Training Error = 0.044921875
Iteration no. 380, lr = 0.686, average batch_loss = 0.11674099558282, Training Error = 0.056640625
Iteration no. 381, lr = 0.686, average batch_loss = 0.10663922990349, Training Error = 0.033203125
Iteration no. 382, lr = 0.686, average batch_loss = 0.11479023948328, Training Error = 0.046875
Iteration no. 383, lr = 0.686, average batch_loss = 0.12100030646586, Training Error = 0.0546875
Iteration no. 384, lr = 0.686, average batch_loss = 0.11523435120108, Training Error = 0.048828125
Iteration no. 385, lr = 0.686, average batch_loss = 0.10044150432436, Training Error = 0.03515625
Iteration no. 386, lr = 0.686, average batch_loss = 0.12163592579948, Training Error = 0.05078125
Iteration no. 387, lr = 0.686, average batch_loss = 0.10861345998749, Training Error = 0.0390625
Iteration no. 388, lr = 0.686, average batch_loss = 0.10017714444423, Training Error = 0.03125
Iteration no. 389, lr = 0.686, average batch_loss = 0.11640792371495, Training Error = 0.046875
Iteration no. 390, lr = 0.686, average batch_loss = 0.11762673052025, Training Error = 0.046875
Iteration no. 391, lr = 0.686, average batch_loss = 0.11101935691809, Training Error = 0.037109375
Iteration no. 392, lr = 0.686, average batch_loss = 0.12200054007346, Training Error = 0.05859375
Iteration no. 393, lr = 0.686, average batch_loss = 0.11406671255486, Training Error = 0.046875
Iteration no. 394, lr = 0.686, average batch_loss = 0.10515981754372, Training Error = 0.033203125
Iteration no. 395, lr = 0.686, average batch_loss = 0.11162809021917, Training Error = 0.044921875
Iteration no. 396, lr = 0.686, average batch_loss = 0.11216437040426, Training Error = 0.046875
Iteration no. 397, lr = 0.686, average batch_loss = 0.09337188573591, Training Error = 0.03515625
Iteration no. 398, lr = 0.686, average batch_loss = 0.10209533074621, Training Error = 0.0390625
Iteration no. 399, lr = 0.686, average batch_loss = 0.10213988391565, Training Error = 0.03515625
Iteration no. 400, lr = 0.686, average batch_loss = 0.10569203159802, Training Error = 0.02734375
Testing... average test_loss = 0.11680471813485, average test_pred_err = 0.045
Iteration no. 401, lr = 0.4802, average batch_loss = 0.11817852252798, Training Error = 0.044921875
Iteration no. 402, lr = 0.4802, average batch_loss = 0.09225703138622, Training Error = 0.025390625
Iteration no. 403, lr = 0.4802, average batch_loss = 0.095254743280981, Training Error = 0.033203125
Iteration no. 404, lr = 0.4802, average batch_loss = 0.10141765721113, Training Error = 0.03515625
Iteration no. 405, lr = 0.4802, average batch_loss = 0.10111334188857, Training Error = 0.048828125
Iteration no. 406, lr = 0.4802, average batch_loss = 0.099263701803462, Training Error = 0.02734375
Iteration no. 407, lr = 0.4802, average batch_loss = 0.10871230114856, Training Error = 0.02734375
Iteration no. 408, lr = 0.4802, average batch_loss = 0.10688024311532, Training Error = 0.03125
Iteration no. 409, lr = 0.4802, average batch_loss = 0.093052363601599, Training Error = 0.029296875
Iteration no. 410, lr = 0.4802, average batch_loss = 0.088300819578415, Training Error = 0.017578125
Iteration no. 411, lr = 0.4802, average batch_loss = 0.10508221831804, Training Error = 0.033203125
Iteration no. 412, lr = 0.4802, average batch_loss = 0.095604868844295, Training Error = 0.017578125
Iteration no. 413, lr = 0.4802, average batch_loss = 0.097160911632788, Training Error = 0.03515625
Iteration no. 414, lr = 0.4802, average batch_loss = 0.091189022715152, Training Error = 0.029296875
Iteration no. 415, lr = 0.4802, average batch_loss = 0.087861006104932, Training Error = 0.02734375
Iteration no. 416, lr = 0.4802, average batch_loss = 0.099451298186116, Training Error = 0.037109375
Iteration no. 417, lr = 0.4802, average batch_loss = 0.091447847532087, Training Error = 0.03515625
Iteration no. 418, lr = 0.4802, average batch_loss = 0.1220479400666, Training Error = 0.0390625
Iteration no. 419, lr = 0.4802, average batch_loss = 0.10344625221196, Training Error = 0.04296875
Iteration no. 420, lr = 0.4802, average batch_loss = 0.098180496288949, Training Error = 0.041015625
Iteration no. 421, lr = 0.4802, average batch_loss = 0.11814210264615, Training Error = 0.0390625
Iteration no. 422, lr = 0.4802, average batch_loss = 0.093326826394099, Training Error = 0.02734375
Iteration no. 423, lr = 0.4802, average batch_loss = 0.091494337148087, Training Error = 0.03515625
Iteration no. 424, lr = 0.4802, average batch_loss = 0.11309093893283, Training Error = 0.03515625
Iteration no. 425, lr = 0.4802, average batch_loss = 0.081585083757383, Training Error = 0.0234375
Iteration no. 426, lr = 0.4802, average batch_loss = 0.088686295553454, Training Error = 0.01953125
Iteration no. 427, lr = 0.4802, average batch_loss = 0.094150133042061, Training Error = 0.02734375
Iteration no. 428, lr = 0.4802, average batch_loss = 0.098534723843057, Training Error = 0.03515625
Iteration no. 429, lr = 0.4802, average batch_loss = 0.1029417395331, Training Error = 0.037109375
Iteration no. 430, lr = 0.4802, average batch_loss = 0.091288247188764, Training Error = 0.02734375
Iteration no. 431, lr = 0.4802, average batch_loss = 0.12227273800701, Training Error = 0.056640625
Iteration no. 432, lr = 0.4802, average batch_loss = 0.09690165580048, Training Error = 0.041015625
Iteration no. 433, lr = 0.4802, average batch_loss = 0.11786908766529, Training Error = 0.03515625
Iteration no. 434, lr = 0.4802, average batch_loss = 0.098709006435303, Training Error = 0.02734375
Iteration no. 435, lr = 0.4802, average batch_loss = 0.096268687295616, Training Error = 0.0390625
Iteration no. 436, lr = 0.4802, average batch_loss = 0.11211686233734, Training Error = 0.046875
Iteration no. 437, lr = 0.4802, average batch_loss = 0.13009741664317, Training Error = 0.0625
Iteration no. 438, lr = 0.4802, average batch_loss = 0.11348469595967, Training Error = 0.044921875
Iteration no. 439, lr = 0.4802, average batch_loss = 0.095589732072156, Training Error = 0.037109375
Iteration no. 440, lr = 0.4802, average batch_loss = 0.11087395215177, Training Error = 0.02734375
Iteration no. 441, lr = 0.4802, average batch_loss = 0.10136940350809, Training Error = 0.037109375
Iteration no. 442, lr = 0.4802, average batch_loss = 0.10460056564813, Training Error = 0.037109375
Iteration no. 443, lr = 0.4802, average batch_loss = 0.10710886397588, Training Error = 0.037109375
Iteration no. 444, lr = 0.4802, average batch_loss = 0.11899490603098, Training Error = 0.041015625
Iteration no. 445, lr = 0.4802, average batch_loss = 0.099367076415786, Training Error = 0.0390625
Iteration no. 446, lr = 0.4802, average batch_loss = 0.087691176070717, Training Error = 0.021484375
Iteration no. 447, lr = 0.4802, average batch_loss = 0.078231062119622, Training Error = 0.02734375
Iteration no. 448, lr = 0.4802, average batch_loss = 0.095734587682758, Training Error = 0.017578125
Iteration no. 449, lr = 0.4802, average batch_loss = 0.1201948255451, Training Error = 0.0546875
Iteration no. 450, lr = 0.4802, average batch_loss = 0.094951078175401, Training Error = 0.029296875
Testing... average test_loss = 0.098330954855669, average test_pred_err = 0.034
Iteration no. 451, lr = 0.4802, average batch_loss = 0.091973229255335, Training Error = 0.033203125
Iteration no. 452, lr = 0.4802, average batch_loss = 0.10260260218093, Training Error = 0.037109375
Iteration no. 453, lr = 0.4802, average batch_loss = 0.083468742920955, Training Error = 0.037109375
Iteration no. 454, lr = 0.4802, average batch_loss = 0.090902239855511, Training Error = 0.033203125
Iteration no. 455, lr = 0.4802, average batch_loss = 0.11592169058933, Training Error = 0.056640625
Iteration no. 456, lr = 0.4802, average batch_loss = 0.07533432769278, Training Error = 0.0078125
Iteration no. 457, lr = 0.4802, average batch_loss = 0.086532424294377, Training Error = 0.03125
Iteration no. 458, lr = 0.4802, average batch_loss = 0.097397686820344, Training Error = 0.02734375
Iteration no. 459, lr = 0.4802, average batch_loss = 0.10559658269671, Training Error = 0.048828125
Iteration no. 460, lr = 0.4802, average batch_loss = 0.093136251824145, Training Error = 0.029296875
Iteration no. 461, lr = 0.4802, average batch_loss = 0.097754961503517, Training Error = 0.029296875
Iteration no. 462, lr = 0.4802, average batch_loss = 0.095503241083475, Training Error = 0.03125
Iteration no. 463, lr = 0.4802, average batch_loss = 0.11759803762, Training Error = 0.041015625
Iteration no. 464, lr = 0.4802, average batch_loss = 0.092349698082709, Training Error = 0.02734375
Iteration no. 465, lr = 0.4802, average batch_loss = 0.088217135415671, Training Error = 0.029296875
Iteration no. 466, lr = 0.4802, average batch_loss = 0.1042863687232, Training Error = 0.033203125
Iteration no. 467, lr = 0.4802, average batch_loss = 0.09701888519921, Training Error = 0.025390625
Iteration no. 468, lr = 0.4802, average batch_loss = 0.077245794769983, Training Error = 0.01953125
Iteration no. 469, lr = 0.4802, average batch_loss = 0.11055646335184, Training Error = 0.0625
Iteration no. 470, lr = 0.4802, average batch_loss = 0.092293941090371, Training Error = 0.025390625
Iteration no. 471, lr = 0.4802, average batch_loss = 0.083416793250928, Training Error = 0.01953125
Iteration no. 472, lr = 0.4802, average batch_loss = 0.073050071566617, Training Error = 0.01953125
Iteration no. 473, lr = 0.4802, average batch_loss = 0.089335693856152, Training Error = 0.01953125
Iteration no. 474, lr = 0.4802, average batch_loss = 0.11656390726254, Training Error = 0.0546875
Iteration no. 475, lr = 0.4802, average batch_loss = 0.10160397121108, Training Error = 0.044921875
Iteration no. 476, lr = 0.4802, average batch_loss = 0.10212707579851, Training Error = 0.029296875
Iteration no. 477, lr = 0.4802, average batch_loss = 0.096622943834963, Training Error = 0.044921875
Iteration no. 478, lr = 0.4802, average batch_loss = 0.10332917595575, Training Error = 0.033203125
Iteration no. 479, lr = 0.4802, average batch_loss = 0.10105849049524, Training Error = 0.046875
Iteration no. 480, lr = 0.4802, average batch_loss = 0.083059066224161, Training Error = 0.02734375
Iteration no. 481, lr = 0.4802, average batch_loss = 0.080295261452304, Training Error = 0.025390625
Iteration no. 482, lr = 0.4802, average batch_loss = 0.097789579540678, Training Error = 0.037109375
Iteration no. 483, lr = 0.4802, average batch_loss = 0.094473163228845, Training Error = 0.037109375
Iteration no. 484, lr = 0.4802, average batch_loss = 0.079059513036779, Training Error = 0.015625
Iteration no. 485, lr = 0.4802, average batch_loss = 0.081265231287822, Training Error = 0.0234375
Iteration no. 486, lr = 0.4802, average batch_loss = 0.078503337621671, Training Error = 0.025390625
Iteration no. 487, lr = 0.4802, average batch_loss = 0.089920727018634, Training Error = 0.02734375
Iteration no. 488, lr = 0.4802, average batch_loss = 0.087915010626794, Training Error = 0.033203125
Iteration no. 489, lr = 0.4802, average batch_loss = 0.082853553815169, Training Error = 0.021484375
Iteration no. 490, lr = 0.4802, average batch_loss = 0.10070200578188, Training Error = 0.033203125
Iteration no. 491, lr = 0.4802, average batch_loss = 0.091790113396729, Training Error = 0.02734375
Iteration no. 492, lr = 0.4802, average batch_loss = 0.099616544748657, Training Error = 0.033203125
Iteration no. 493, lr = 0.4802, average batch_loss = 0.099847490497455, Training Error = 0.0390625
Iteration no. 494, lr = 0.4802, average batch_loss = 0.092204046174211, Training Error = 0.02734375
Iteration no. 495, lr = 0.4802, average batch_loss = 0.075004268866333, Training Error = 0.01953125
Iteration no. 496, lr = 0.4802, average batch_loss = 0.084763400358965, Training Error = 0.01953125
Iteration no. 497, lr = 0.4802, average batch_loss = 0.082852862127741, Training Error = 0.01953125
Iteration no. 498, lr = 0.4802, average batch_loss = 0.10121064933977, Training Error = 0.033203125
Iteration no. 499, lr = 0.4802, average batch_loss = 0.091166360211691, Training Error = 0.01953125
Iteration no. 500, lr = 0.4802, average batch_loss = 0.099210589814776, Training Error = 0.044921875
Testing... average test_loss = 0.08264963833868, average test_pred_err = 0.027
Iteration no. 501, lr = 0.33614, average batch_loss = 0.10560113031577, Training Error = 0.0390625
Iteration no. 502, lr = 0.33614, average batch_loss = 0.089340179806098, Training Error = 0.02734375
Iteration no. 503, lr = 0.33614, average batch_loss = 0.086346505727266, Training Error = 0.0234375
Iteration no. 504, lr = 0.33614, average batch_loss = 0.076693442138833, Training Error = 0.013671875
Iteration no. 505, lr = 0.33614, average batch_loss = 0.083570929058414, Training Error = 0.029296875
Iteration no. 506, lr = 0.33614, average batch_loss = 0.090682777822259, Training Error = 0.02734375
Iteration no. 507, lr = 0.33614, average batch_loss = 0.087436458758998, Training Error = 0.033203125
Iteration no. 508, lr = 0.33614, average batch_loss = 0.089494610633373, Training Error = 0.0234375
Iteration no. 509, lr = 0.33614, average batch_loss = 0.0938666999083, Training Error = 0.033203125
Iteration no. 510, lr = 0.33614, average batch_loss = 0.076644725628361, Training Error = 0.013671875
Iteration no. 511, lr = 0.33614, average batch_loss = 0.088714382629327, Training Error = 0.0234375
Iteration no. 512, lr = 0.33614, average batch_loss = 0.092921439097685, Training Error = 0.0390625
Iteration no. 513, lr = 0.33614, average batch_loss = 0.0815906758318, Training Error = 0.029296875
Iteration no. 514, lr = 0.33614, average batch_loss = 0.083667558047442, Training Error = 0.0234375
Iteration no. 515, lr = 0.33614, average batch_loss = 0.08781660894948, Training Error = 0.025390625
Iteration no. 516, lr = 0.33614, average batch_loss = 0.091783859790522, Training Error = 0.025390625
Iteration no. 517, lr = 0.33614, average batch_loss = 0.087356790489646, Training Error = 0.013671875
Iteration no. 518, lr = 0.33614, average batch_loss = 0.10409173037309, Training Error = 0.0234375
Iteration no. 519, lr = 0.33614, average batch_loss = 0.095963724599147, Training Error = 0.029296875
Iteration no. 520, lr = 0.33614, average batch_loss = 0.082403386664066, Training Error = 0.01953125
Iteration no. 521, lr = 0.33614, average batch_loss = 0.086481075292226, Training Error = 0.02734375
Iteration no. 522, lr = 0.33614, average batch_loss = 0.059541185922382, Training Error = 0.015625
Iteration no. 523, lr = 0.33614, average batch_loss = 0.084066978907843, Training Error = 0.029296875
Iteration no. 524, lr = 0.33614, average batch_loss = 0.073474332856605, Training Error = 0.01171875
Iteration no. 525, lr = 0.33614, average batch_loss = 0.081707459725632, Training Error = 0.029296875
Iteration no. 526, lr = 0.33614, average batch_loss = 0.093189836311272, Training Error = 0.037109375
Iteration no. 527, lr = 0.33614, average batch_loss = 0.085604559556084, Training Error = 0.02734375
Iteration no. 528, lr = 0.33614, average batch_loss = 0.10236976066812, Training Error = 0.0234375
Iteration no. 529, lr = 0.33614, average batch_loss = 0.091539540777411, Training Error = 0.02734375
Iteration no. 530, lr = 0.33614, average batch_loss = 0.093944145624693, Training Error = 0.025390625
Iteration no. 531, lr = 0.33614, average batch_loss = 0.089511574672212, Training Error = 0.017578125
Iteration no. 532, lr = 0.33614, average batch_loss = 0.068673371995975, Training Error = 0.025390625
Iteration no. 533, lr = 0.33614, average batch_loss = 0.077953526172155, Training Error = 0.0234375
Iteration no. 534, lr = 0.33614, average batch_loss = 0.087828811865036, Training Error = 0.015625
Iteration no. 535, lr = 0.33614, average batch_loss = 0.080633321652182, Training Error = 0.021484375
Iteration no. 536, lr = 0.33614, average batch_loss = 0.086749572003935, Training Error = 0.02734375
Iteration no. 537, lr = 0.33614, average batch_loss = 0.084425799883177, Training Error = 0.01953125
Iteration no. 538, lr = 0.33614, average batch_loss = 0.091395297302567, Training Error = 0.029296875
Iteration no. 539, lr = 0.33614, average batch_loss = 0.084647710001767, Training Error = 0.029296875
Iteration no. 540, lr = 0.33614, average batch_loss = 0.092101858146662, Training Error = 0.025390625
Iteration no. 541, lr = 0.33614, average batch_loss = 0.10760764593187, Training Error = 0.03125
Iteration no. 542, lr = 0.33614, average batch_loss = 0.091984390012417, Training Error = 0.029296875
Iteration no. 543, lr = 0.33614, average batch_loss = 0.063860137370256, Training Error = 0.01953125
Iteration no. 544, lr = 0.33614, average batch_loss = 0.083077406452457, Training Error = 0.021484375
Iteration no. 545, lr = 0.33614, average batch_loss = 0.078692890936575, Training Error = 0.013671875
Iteration no. 546, lr = 0.33614, average batch_loss = 0.075938579880452, Training Error = 0.017578125
Iteration no. 547, lr = 0.33614, average batch_loss = 0.093187198186603, Training Error = 0.03125
Iteration no. 548, lr = 0.33614, average batch_loss = 0.094349895619727, Training Error = 0.02734375
Iteration no. 549, lr = 0.33614, average batch_loss = 0.10045932645823, Training Error = 0.03125
Iteration no. 550, lr = 0.33614, average batch_loss = 0.09248115881118, Training Error = 0.03125
Testing... average test_loss = 0.079217438354762, average test_pred_err = 0.016
Iteration no. 551, lr = 0.33614, average batch_loss = 0.093407016028969, Training Error = 0.03125
Iteration no. 552, lr = 0.33614, average batch_loss = 0.076844287578006, Training Error = 0.01171875
Iteration no. 553, lr = 0.33614, average batch_loss = 0.088214492974365, Training Error = 0.03515625
Iteration no. 554, lr = 0.33614, average batch_loss = 0.079510303325424, Training Error = 0.0234375
Iteration no. 555, lr = 0.33614, average batch_loss = 0.070457349799423, Training Error = 0.015625
Iteration no. 556, lr = 0.33614, average batch_loss = 0.084792312990835, Training Error = 0.029296875
Iteration no. 557, lr = 0.33614, average batch_loss = 0.076268048757661, Training Error = 0.01171875
Iteration no. 558, lr = 0.33614, average batch_loss = 0.078203933304754, Training Error = 0.013671875
Iteration no. 559, lr = 0.33614, average batch_loss = 0.080205746093142, Training Error = 0.015625
Iteration no. 560, lr = 0.33614, average batch_loss = 0.078228560469306, Training Error = 0.015625
Iteration no. 561, lr = 0.33614, average batch_loss = 0.079364229525826, Training Error = 0.0234375
Iteration no. 562, lr = 0.33614, average batch_loss = 0.085999798934138, Training Error = 0.029296875
Iteration no. 563, lr = 0.33614, average batch_loss = 0.082135048337398, Training Error = 0.025390625
Iteration no. 564, lr = 0.33614, average batch_loss = 0.076319518320189, Training Error = 0.0234375
Iteration no. 565, lr = 0.33614, average batch_loss = 0.082252285042046, Training Error = 0.02734375
Iteration no. 566, lr = 0.33614, average batch_loss = 0.08449023244808, Training Error = 0.01953125
Iteration no. 567, lr = 0.33614, average batch_loss = 0.084112703225376, Training Error = 0.01953125
Iteration no. 568, lr = 0.33614, average batch_loss = 0.093885267557477, Training Error = 0.025390625
Iteration no. 569, lr = 0.33614, average batch_loss = 0.074400875119312, Training Error = 0.01953125
Iteration no. 570, lr = 0.33614, average batch_loss = 0.07648717309972, Training Error = 0.02734375
Iteration no. 571, lr = 0.33614, average batch_loss = 0.087404654988759, Training Error = 0.025390625
Iteration no. 572, lr = 0.33614, average batch_loss = 0.062302732947531, Training Error = 0.017578125
Iteration no. 573, lr = 0.33614, average batch_loss = 0.079695658499135, Training Error = 0.013671875
Iteration no. 574, lr = 0.33614, average batch_loss = 0.097197091576075, Training Error = 0.04296875
Iteration no. 575, lr = 0.33614, average batch_loss = 0.07742849922619, Training Error = 0.015625
Iteration no. 576, lr = 0.33614, average batch_loss = 0.08626951272924, Training Error = 0.017578125
Iteration no. 577, lr = 0.33614, average batch_loss = 0.10197637506325, Training Error = 0.033203125
Iteration no. 578, lr = 0.33614, average batch_loss = 0.090419550143342, Training Error = 0.02734375
Iteration no. 579, lr = 0.33614, average batch_loss = 0.085568274575272, Training Error = 0.025390625
Iteration no. 580, lr = 0.33614, average batch_loss = 0.090188260231902, Training Error = 0.025390625
Iteration no. 581, lr = 0.33614, average batch_loss = 0.097956704517277, Training Error = 0.02734375
Iteration no. 582, lr = 0.33614, average batch_loss = 0.099414626052975, Training Error = 0.048828125
Iteration no. 583, lr = 0.33614, average batch_loss = 0.083594580605417, Training Error = 0.029296875
Iteration no. 584, lr = 0.33614, average batch_loss = 0.068798666922416, Training Error = 0.017578125
Iteration no. 585, lr = 0.33614, average batch_loss = 0.062935064957688, Training Error = 0.015625
Iteration no. 586, lr = 0.33614, average batch_loss = 0.08011284128052, Training Error = 0.013671875
Iteration no. 587, lr = 0.33614, average batch_loss = 0.077490732612818, Training Error = 0.017578125
Iteration no. 588, lr = 0.33614, average batch_loss = 0.087997498953768, Training Error = 0.01953125
Iteration no. 589, lr = 0.33614, average batch_loss = 0.097167996703369, Training Error = 0.03515625
Iteration no. 590, lr = 0.33614, average batch_loss = 0.086236294467632, Training Error = 0.033203125
Iteration no. 591, lr = 0.33614, average batch_loss = 0.076487114397853, Training Error = 0.0078125
Iteration no. 592, lr = 0.33614, average batch_loss = 0.085724421190221, Training Error = 0.025390625
Iteration no. 593, lr = 0.33614, average batch_loss = 0.076790610269577, Training Error = 0.021484375
Iteration no. 594, lr = 0.33614, average batch_loss = 0.094325298586849, Training Error = 0.025390625
Iteration no. 595, lr = 0.33614, average batch_loss = 0.095500347639585, Training Error = 0.04296875
Iteration no. 596, lr = 0.33614, average batch_loss = 0.069313096650515, Training Error = 0.01171875
Iteration no. 597, lr = 0.33614, average batch_loss = 0.083407131399122, Training Error = 0.0234375
Iteration no. 598, lr = 0.33614, average batch_loss = 0.083250923870628, Training Error = 0.013671875
Iteration no. 599, lr = 0.33614, average batch_loss = 0.084660877851172, Training Error = 0.01953125
Iteration no. 600, lr = 0.33614, average batch_loss = 0.095292671025927, Training Error = 0.0390625
Testing... average test_loss = 0.084460465003576, average test_pred_err = 0.029
Iteration no. 601, lr = 0.235298, average batch_loss = 0.077987903981108, Training Error = 0.025390625
Iteration no. 602, lr = 0.235298, average batch_loss = 0.069360504583135, Training Error = 0.015625
Iteration no. 603, lr = 0.235298, average batch_loss = 0.071767161690305, Training Error = 0.01171875
Iteration no. 604, lr = 0.235298, average batch_loss = 0.082935449868051, Training Error = 0.02734375
Iteration no. 605, lr = 0.235298, average batch_loss = 0.071705556213724, Training Error = 0.013671875
Iteration no. 606, lr = 0.235298, average batch_loss = 0.07953719158325, Training Error = 0.025390625
Iteration no. 607, lr = 0.235298, average batch_loss = 0.08165151515261, Training Error = 0.013671875
Iteration no. 608, lr = 0.235298, average batch_loss = 0.077271333200378, Training Error = 0.01171875
Iteration no. 609, lr = 0.235298, average batch_loss = 0.089020816730458, Training Error = 0.015625
Iteration no. 610, lr = 0.235298, average batch_loss = 0.087514982517017, Training Error = 0.03125
Iteration no. 611, lr = 0.235298, average batch_loss = 0.079091378704031, Training Error = 0.02734375
Iteration no. 612, lr = 0.235298, average batch_loss = 0.087196737933267, Training Error = 0.015625
Iteration no. 613, lr = 0.235298, average batch_loss = 0.073293160866652, Training Error = 0.01953125
Iteration no. 614, lr = 0.235298, average batch_loss = 0.068889197875588, Training Error = 0.01953125
Iteration no. 615, lr = 0.235298, average batch_loss = 0.076405304615453, Training Error = 0.013671875
Iteration no. 616, lr = 0.235298, average batch_loss = 0.081659842091685, Training Error = 0.01953125
Iteration no. 617, lr = 0.235298, average batch_loss = 0.067797624952642, Training Error = 0.01171875
Iteration no. 618, lr = 0.235298, average batch_loss = 0.078524105734171, Training Error = 0.025390625
Iteration no. 619, lr = 0.235298, average batch_loss = 0.072583867724698, Training Error = 0.017578125
Iteration no. 620, lr = 0.235298, average batch_loss = 0.085378004752242, Training Error = 0.0390625
Iteration no. 621, lr = 0.235298, average batch_loss = 0.08528206916028, Training Error = 0.03125
Iteration no. 622, lr = 0.235298, average batch_loss = 0.072182775765647, Training Error = 0.017578125
Iteration no. 623, lr = 0.235298, average batch_loss = 0.078344995440938, Training Error = 0.01171875
Iteration no. 624, lr = 0.235298, average batch_loss = 0.068188855079933, Training Error = 0.015625
Iteration no. 625, lr = 0.235298, average batch_loss = 0.076258012728052, Training Error = 0.01953125
Iteration no. 626, lr = 0.235298, average batch_loss = 0.070765887565013, Training Error = 0.01171875
Iteration no. 627, lr = 0.235298, average batch_loss = 0.081542999396901, Training Error = 0.01171875
Iteration no. 628, lr = 0.235298, average batch_loss = 0.075741604331549, Training Error = 0.015625
Iteration no. 629, lr = 0.235298, average batch_loss = 0.08408228678442, Training Error = 0.029296875
Iteration no. 630, lr = 0.235298, average batch_loss = 0.081537449082503, Training Error = 0.0234375
Iteration no. 631, lr = 0.235298, average batch_loss = 0.082558855341048, Training Error = 0.02734375
Iteration no. 632, lr = 0.235298, average batch_loss = 0.090547760617384, Training Error = 0.029296875
Iteration no. 633, lr = 0.235298, average batch_loss = 0.069815585954622, Training Error = 0.017578125
Iteration no. 634, lr = 0.235298, average batch_loss = 0.07784454233704, Training Error = 0.01953125
Iteration no. 635, lr = 0.235298, average batch_loss = 0.086019472134461, Training Error = 0.025390625
Iteration no. 636, lr = 0.235298, average batch_loss = 0.088426004364636, Training Error = 0.029296875
Iteration no. 637, lr = 0.235298, average batch_loss = 0.075222084212561, Training Error = 0.01953125
Iteration no. 638, lr = 0.235298, average batch_loss = 0.077262435545574, Training Error = 0.02734375
Iteration no. 639, lr = 0.235298, average batch_loss = 0.081102922605492, Training Error = 0.021484375
Iteration no. 640, lr = 0.235298, average batch_loss = 0.075553509633396, Training Error = 0.01171875
Iteration no. 641, lr = 0.235298, average batch_loss = 0.074480030981759, Training Error = 0.015625
Iteration no. 642, lr = 0.235298, average batch_loss = 0.086254331851372, Training Error = 0.01953125
Iteration no. 643, lr = 0.235298, average batch_loss = 0.069452525716775, Training Error = 0.021484375
Iteration no. 644, lr = 0.235298, average batch_loss = 0.089997129819958, Training Error = 0.02734375
Iteration no. 645, lr = 0.235298, average batch_loss = 0.078122199216727, Training Error = 0.017578125
Iteration no. 646, lr = 0.235298, average batch_loss = 0.081828354067165, Training Error = 0.033203125
Iteration no. 647, lr = 0.235298, average batch_loss = 0.066243852262207, Training Error = 0.01953125
Iteration no. 648, lr = 0.235298, average batch_loss = 0.084771616053629, Training Error = 0.025390625
Iteration no. 649, lr = 0.235298, average batch_loss = 0.066435130744812, Training Error = 0.01171875
Iteration no. 650, lr = 0.235298, average batch_loss = 0.071137414521181, Training Error = 0.01953125
Testing... average test_loss = 0.073922610938153, average test_pred_err = 0.022
Iteration no. 651, lr = 0.235298, average batch_loss = 0.081433820983476, Training Error = 0.021484375
Iteration no. 652, lr = 0.235298, average batch_loss = 0.086204189792544, Training Error = 0.021484375
Iteration no. 653, lr = 0.235298, average batch_loss = 0.069751072328068, Training Error = 0.025390625
Iteration no. 654, lr = 0.235298, average batch_loss = 0.071497225276793, Training Error = 0.01171875
Iteration no. 655, lr = 0.235298, average batch_loss = 0.084592402685905, Training Error = 0.015625
Iteration no. 656, lr = 0.235298, average batch_loss = 0.067417313116402, Training Error = 0.01171875
Iteration no. 657, lr = 0.235298, average batch_loss = 0.068462976492049, Training Error = 0.01953125
Iteration no. 658, lr = 0.235298, average batch_loss = 0.064718475354339, Training Error = 0.013671875
Iteration no. 659, lr = 0.235298, average batch_loss = 0.083531719925248, Training Error = 0.021484375
Iteration no. 660, lr = 0.235298, average batch_loss = 0.073049732978561, Training Error = 0.015625
Iteration no. 661, lr = 0.235298, average batch_loss = 0.079090145939193, Training Error = 0.017578125
Iteration no. 662, lr = 0.235298, average batch_loss = 0.068631394911573, Training Error = 0.01953125
Iteration no. 663, lr = 0.235298, average batch_loss = 0.081859957262686, Training Error = 0.0234375
Iteration no. 664, lr = 0.235298, average batch_loss = 0.076292177255418, Training Error = 0.02734375
Iteration no. 665, lr = 0.235298, average batch_loss = 0.083379197073118, Training Error = 0.021484375
Iteration no. 666, lr = 0.235298, average batch_loss = 0.081803410590053, Training Error = 0.01171875
Iteration no. 667, lr = 0.235298, average batch_loss = 0.093518147801172, Training Error = 0.01953125
Iteration no. 668, lr = 0.235298, average batch_loss = 0.099026034125161, Training Error = 0.0234375
Iteration no. 669, lr = 0.235298, average batch_loss = 0.085039245079538, Training Error = 0.0234375
Iteration no. 670, lr = 0.235298, average batch_loss = 0.077269139850417, Training Error = 0.017578125
Iteration no. 671, lr = 0.235298, average batch_loss = 0.075042014868649, Training Error = 0.017578125
Iteration no. 672, lr = 0.235298, average batch_loss = 0.07104406905138, Training Error = 0.01953125
Iteration no. 673, lr = 0.235298, average batch_loss = 0.081861242521291, Training Error = 0.013671875
Iteration no. 674, lr = 0.235298, average batch_loss = 0.08353603514066, Training Error = 0.029296875
Iteration no. 675, lr = 0.235298, average batch_loss = 0.064577308684565, Training Error = 0.01171875
Iteration no. 676, lr = 0.235298, average batch_loss = 0.070381714022632, Training Error = 0.017578125
Iteration no. 677, lr = 0.235298, average batch_loss = 0.08284208991914, Training Error = 0.01953125
Iteration no. 678, lr = 0.235298, average batch_loss = 0.077626749504689, Training Error = 0.025390625
Iteration no. 679, lr = 0.235298, average batch_loss = 0.065623825325346, Training Error = 0.0234375
Iteration no. 680, lr = 0.235298, average batch_loss = 0.059987803697048, Training Error = 0.013671875
Iteration no. 681, lr = 0.235298, average batch_loss = 0.07181870281123, Training Error = 0.017578125
Iteration no. 682, lr = 0.235298, average batch_loss = 0.072129669187321, Training Error = 0.01171875
Iteration no. 683, lr = 0.235298, average batch_loss = 0.069308218538631, Training Error = 0.01171875
Iteration no. 684, lr = 0.235298, average batch_loss = 0.072874159102889, Training Error = 0.01171875
Iteration no. 685, lr = 0.235298, average batch_loss = 0.078067342311226, Training Error = 0.017578125
Iteration no. 686, lr = 0.235298, average batch_loss = 0.064687013220472, Training Error = 0.01171875
Iteration no. 687, lr = 0.235298, average batch_loss = 0.075089022050718, Training Error = 0.0234375
Iteration no. 688, lr = 0.235298, average batch_loss = 0.081544862514679, Training Error = 0.021484375
Iteration no. 689, lr = 0.235298, average batch_loss = 0.087941178159068, Training Error = 0.02734375
Iteration no. 690, lr = 0.235298, average batch_loss = 0.057677326409391, Training Error = 0.017578125
Iteration no. 691, lr = 0.235298, average batch_loss = 0.079924298582407, Training Error = 0.02734375
Iteration no. 692, lr = 0.235298, average batch_loss = 0.064559097702071, Training Error = 0.017578125
Iteration no. 693, lr = 0.235298, average batch_loss = 0.084423368368038, Training Error = 0.0234375
Iteration no. 694, lr = 0.235298, average batch_loss = 0.076376170678234, Training Error = 0.02734375
Iteration no. 695, lr = 0.235298, average batch_loss = 0.083020096386993, Training Error = 0.025390625
Iteration no. 696, lr = 0.235298, average batch_loss = 0.073439400817306, Training Error = 0.013671875
Iteration no. 697, lr = 0.235298, average batch_loss = 0.07612653853637, Training Error = 0.017578125
Iteration no. 698, lr = 0.235298, average batch_loss = 0.069032269181506, Training Error = 0.013671875
Iteration no. 699, lr = 0.235298, average batch_loss = 0.066860002864778, Training Error = 0.009765625
Iteration no. 700, lr = 0.235298, average batch_loss = 0.062347160527015, Training Error = 0.01171875
Testing... average test_loss = 0.075612344653128, average test_pred_err = 0.018
Iteration no. 701, lr = 0.1647086, average batch_loss = 0.088966955760797, Training Error = 0.0234375
Iteration no. 702, lr = 0.1647086, average batch_loss = 0.058121770951047, Training Error = 0.013671875
Iteration no. 703, lr = 0.1647086, average batch_loss = 0.078670664351386, Training Error = 0.021484375
Iteration no. 704, lr = 0.1647086, average batch_loss = 0.084628122384034, Training Error = 0.021484375
Iteration no. 705, lr = 0.1647086, average batch_loss = 0.062766615354609, Training Error = 0.009765625
Iteration no. 706, lr = 0.1647086, average batch_loss = 0.067407848154273, Training Error = 0.013671875
Iteration no. 707, lr = 0.1647086, average batch_loss = 0.070145694125762, Training Error = 0.013671875
Iteration no. 708, lr = 0.1647086, average batch_loss = 0.091430115775674, Training Error = 0.03125
Iteration no. 709, lr = 0.1647086, average batch_loss = 0.086052198911889, Training Error = 0.0234375
Iteration no. 710, lr = 0.1647086, average batch_loss = 0.068646161998174, Training Error = 0.013671875
Iteration no. 711, lr = 0.1647086, average batch_loss = 0.075295532099351, Training Error = 0.017578125
Iteration no. 712, lr = 0.1647086, average batch_loss = 0.071439041154532, Training Error = 0.01171875
Iteration no. 713, lr = 0.1647086, average batch_loss = 0.080483801516584, Training Error = 0.01953125
Iteration no. 714, lr = 0.1647086, average batch_loss = 0.065315241531774, Training Error = 0.015625
Iteration no. 715, lr = 0.1647086, average batch_loss = 0.065059282730058, Training Error = 0.017578125
Iteration no. 716, lr = 0.1647086, average batch_loss = 0.072399433651676, Training Error = 0.01953125
Iteration no. 717, lr = 0.1647086, average batch_loss = 0.081152437833422, Training Error = 0.0234375
Iteration no. 718, lr = 0.1647086, average batch_loss = 0.071373363956826, Training Error = 0.02734375
Iteration no. 719, lr = 0.1647086, average batch_loss = 0.062289656368131, Training Error = 0.015625
Iteration no. 720, lr = 0.1647086, average batch_loss = 0.077992980569465, Training Error = 0.025390625
Iteration no. 721, lr = 0.1647086, average batch_loss = 0.076137884880844, Training Error = 0.01953125
Iteration no. 722, lr = 0.1647086, average batch_loss = 0.072735186084187, Training Error = 0.005859375
Iteration no. 723, lr = 0.1647086, average batch_loss = 0.07701055896032, Training Error = 0.021484375
Iteration no. 724, lr = 0.1647086, average batch_loss = 0.080828002018316, Training Error = 0.02734375
Iteration no. 725, lr = 0.1647086, average batch_loss = 0.083802129818375, Training Error = 0.025390625
Iteration no. 726, lr = 0.1647086, average batch_loss = 0.079844671178374, Training Error = 0.025390625
Iteration no. 727, lr = 0.1647086, average batch_loss = 0.060863273645649, Training Error = 0.009765625
Iteration no. 728, lr = 0.1647086, average batch_loss = 0.063729570833601, Training Error = 0.01171875
Iteration no. 729, lr = 0.1647086, average batch_loss = 0.066084192643237, Training Error = 0.01171875
Iteration no. 730, lr = 0.1647086, average batch_loss = 0.071803707123173, Training Error = 0.009765625
Iteration no. 731, lr = 0.1647086, average batch_loss = 0.081427692779703, Training Error = 0.015625
Iteration no. 732, lr = 0.1647086, average batch_loss = 0.063686651905525, Training Error = 0.009765625
Iteration no. 733, lr = 0.1647086, average batch_loss = 0.091924509379576, Training Error = 0.02734375
Iteration no. 734, lr = 0.1647086, average batch_loss = 0.076702334540778, Training Error = 0.017578125
Iteration no. 735, lr = 0.1647086, average batch_loss = 0.081656357489836, Training Error = 0.01953125
Iteration no. 736, lr = 0.1647086, average batch_loss = 0.076922350859732, Training Error = 0.013671875
Iteration no. 737, lr = 0.1647086, average batch_loss = 0.079596047274998, Training Error = 0.01953125
Iteration no. 738, lr = 0.1647086, average batch_loss = 0.072219439989987, Training Error = 0.015625
Iteration no. 739, lr = 0.1647086, average batch_loss = 0.066821112337142, Training Error = 0.009765625
Iteration no. 740, lr = 0.1647086, average batch_loss = 0.065884782127094, Training Error = 0.017578125
Iteration no. 741, lr = 0.1647086, average batch_loss = 0.06737575294798, Training Error = 0.017578125
Iteration no. 742, lr = 0.1647086, average batch_loss = 0.075027687673821, Training Error = 0.025390625
Iteration no. 743, lr = 0.1647086, average batch_loss = 0.080261481199384, Training Error = 0.025390625
Iteration no. 744, lr = 0.1647086, average batch_loss = 0.069347323854554, Training Error = 0.021484375
Iteration no. 745, lr = 0.1647086, average batch_loss = 0.076917873773485, Training Error = 0.01953125
Iteration no. 746, lr = 0.1647086, average batch_loss = 0.063297140842113, Training Error = 0.017578125
Iteration no. 747, lr = 0.1647086, average batch_loss = 0.072290365579939, Training Error = 0.015625
Iteration no. 748, lr = 0.1647086, average batch_loss = 0.065789313797416, Training Error = 0.021484375
Iteration no. 749, lr = 0.1647086, average batch_loss = 0.068588700218401, Training Error = 0.025390625
Iteration no. 750, lr = 0.1647086, average batch_loss = 0.077661504691207, Training Error = 0.013671875
Testing... average test_loss = 0.07322576884868, average test_pred_err = 0.021
Iteration no. 751, lr = 0.1647086, average batch_loss = 0.066398434969604, Training Error = 0.005859375
Iteration no. 752, lr = 0.1647086, average batch_loss = 0.083289424377518, Training Error = 0.01953125
Iteration no. 753, lr = 0.1647086, average batch_loss = 0.072539677488417, Training Error = 0.015625
Iteration no. 754, lr = 0.1647086, average batch_loss = 0.068787635043448, Training Error = 0.01953125
Iteration no. 755, lr = 0.1647086, average batch_loss = 0.082236523162202, Training Error = 0.02734375
Iteration no. 756, lr = 0.1647086, average batch_loss = 0.076366639651278, Training Error = 0.0234375
Iteration no. 757, lr = 0.1647086, average batch_loss = 0.063349920203229, Training Error = 0.01171875
Iteration no. 758, lr = 0.1647086, average batch_loss = 0.074303229701782, Training Error = 0.01171875
Iteration no. 759, lr = 0.1647086, average batch_loss = 0.071255432589605, Training Error = 0.021484375
Iteration no. 760, lr = 0.1647086, average batch_loss = 0.066513533222991, Training Error = 0.015625
Iteration no. 761, lr = 0.1647086, average batch_loss = 0.068950921819638, Training Error = 0.01953125
Iteration no. 762, lr = 0.1647086, average batch_loss = 0.084046299297204, Training Error = 0.03125
Iteration no. 763, lr = 0.1647086, average batch_loss = 0.073187007441243, Training Error = 0.021484375
Iteration no. 764, lr = 0.1647086, average batch_loss = 0.070886211277257, Training Error = 0.021484375
Iteration no. 765, lr = 0.1647086, average batch_loss = 0.067207515061243, Training Error = 0.01953125
Iteration no. 766, lr = 0.1647086, average batch_loss = 0.067386447616118, Training Error = 0.005859375
Iteration no. 767, lr = 0.1647086, average batch_loss = 0.082061864320853, Training Error = 0.025390625
Iteration no. 768, lr = 0.1647086, average batch_loss = 0.066465287983868, Training Error = 0.013671875
Iteration no. 769, lr = 0.1647086, average batch_loss = 0.060215020674624, Training Error = 0.005859375
Iteration no. 770, lr = 0.1647086, average batch_loss = 0.063380163660634, Training Error = 0.017578125
Iteration no. 771, lr = 0.1647086, average batch_loss = 0.067974122269721, Training Error = 0.01171875
Iteration no. 772, lr = 0.1647086, average batch_loss = 0.070006619151261, Training Error = 0.017578125
Iteration no. 773, lr = 0.1647086, average batch_loss = 0.069644598918912, Training Error = 0.025390625
Iteration no. 774, lr = 0.1647086, average batch_loss = 0.071263739709558, Training Error = 0.015625
Iteration no. 775, lr = 0.1647086, average batch_loss = 0.070682753630714, Training Error = 0.01953125
Iteration no. 776, lr = 0.1647086, average batch_loss = 0.08202260119995, Training Error = 0.017578125
Iteration no. 777, lr = 0.1647086, average batch_loss = 0.072194643113577, Training Error = 0.009765625
Iteration no. 778, lr = 0.1647086, average batch_loss = 0.070924568597732, Training Error = 0.015625
Iteration no. 779, lr = 0.1647086, average batch_loss = 0.06445638888242, Training Error = 0.009765625
Iteration no. 780, lr = 0.1647086, average batch_loss = 0.084823054772956, Training Error = 0.0234375
Iteration no. 781, lr = 0.1647086, average batch_loss = 0.072057623674096, Training Error = 0.017578125
Iteration no. 782, lr = 0.1647086, average batch_loss = 0.07478535167785, Training Error = 0.02734375
Iteration no. 783, lr = 0.1647086, average batch_loss = 0.080090070944129, Training Error = 0.025390625
Iteration no. 784, lr = 0.1647086, average batch_loss = 0.070816480110401, Training Error = 0.021484375
Iteration no. 785, lr = 0.1647086, average batch_loss = 0.069118402156961, Training Error = 0.02734375
Iteration no. 786, lr = 0.1647086, average batch_loss = 0.067492062878553, Training Error = 0.013671875
Iteration no. 787, lr = 0.1647086, average batch_loss = 0.068869969219944, Training Error = 0.005859375
Iteration no. 788, lr = 0.1647086, average batch_loss = 0.076486526799224, Training Error = 0.025390625
Iteration no. 789, lr = 0.1647086, average batch_loss = 0.069538101477075, Training Error = 0.017578125
Iteration no. 790, lr = 0.1647086, average batch_loss = 0.063013034990058, Training Error = 0.015625
Iteration no. 791, lr = 0.1647086, average batch_loss = 0.077054015199149, Training Error = 0.015625
Iteration no. 792, lr = 0.1647086, average batch_loss = 0.077787191851741, Training Error = 0.017578125
Iteration no. 793, lr = 0.1647086, average batch_loss = 0.077028972816841, Training Error = 0.0234375
Iteration no. 794, lr = 0.1647086, average batch_loss = 0.063188463657781, Training Error = 0.009765625
Iteration no. 795, lr = 0.1647086, average batch_loss = 0.080017844725506, Training Error = 0.021484375
Iteration no. 796, lr = 0.1647086, average batch_loss = 0.067578461728804, Training Error = 0.015625
Iteration no. 797, lr = 0.1647086, average batch_loss = 0.076842745975216, Training Error = 0.021484375
Iteration no. 798, lr = 0.1647086, average batch_loss = 0.07326092122502, Training Error = 0.013671875
Iteration no. 799, lr = 0.1647086, average batch_loss = 0.071969474774593, Training Error = 0.015625
Iteration no. 800, lr = 0.1647086, average batch_loss = 0.070416144529509, Training Error = 0.013671875
Testing... average test_loss = 0.070599385543728, average test_pred_err = 0.016
Iteration no. 801, lr = 0.11529602, average batch_loss = 0.069352911200482, Training Error = 0.013671875
Iteration no. 802, lr = 0.11529602, average batch_loss = 0.065701436940881, Training Error = 0.01171875
Iteration no. 803, lr = 0.11529602, average batch_loss = 0.058178706298015, Training Error = 0.00390625
Iteration no. 804, lr = 0.11529602, average batch_loss = 0.071253423992943, Training Error = 0.015625
Iteration no. 805, lr = 0.11529602, average batch_loss = 0.068626830148744, Training Error = 0.021484375
Iteration no. 806, lr = 0.11529602, average batch_loss = 0.06328788239967, Training Error = 0.0078125
Iteration no. 807, lr = 0.11529602, average batch_loss = 0.064291052120903, Training Error = 0.01171875
Iteration no. 808, lr = 0.11529602, average batch_loss = 0.08063938746957, Training Error = 0.025390625
Iteration no. 809, lr = 0.11529602, average batch_loss = 0.082874991442638, Training Error = 0.01953125
Iteration no. 810, lr = 0.11529602, average batch_loss = 0.066727774240463, Training Error = 0.01953125
Iteration no. 811, lr = 0.11529602, average batch_loss = 0.082200792590839, Training Error = 0.021484375
Iteration no. 812, lr = 0.11529602, average batch_loss = 0.070539052621427, Training Error = 0.01953125
Iteration no. 813, lr = 0.11529602, average batch_loss = 0.07467313084803, Training Error = 0.0234375
Iteration no. 814, lr = 0.11529602, average batch_loss = 0.063367862613762, Training Error = 0.01171875
Iteration no. 815, lr = 0.11529602, average batch_loss = 0.074107137965873, Training Error = 0.01953125
Iteration no. 816, lr = 0.11529602, average batch_loss = 0.080382619856315, Training Error = 0.017578125
Iteration no. 817, lr = 0.11529602, average batch_loss = 0.066323547543217, Training Error = 0.015625
Iteration no. 818, lr = 0.11529602, average batch_loss = 0.083543952513368, Training Error = 0.021484375
Iteration no. 819, lr = 0.11529602, average batch_loss = 0.078620615529108, Training Error = 0.017578125
Iteration no. 820, lr = 0.11529602, average batch_loss = 0.07133377420665, Training Error = 0.013671875
Iteration no. 821, lr = 0.11529602, average batch_loss = 0.080092670941456, Training Error = 0.021484375
Iteration no. 822, lr = 0.11529602, average batch_loss = 0.069987466518957, Training Error = 0.015625
Iteration no. 823, lr = 0.11529602, average batch_loss = 0.08669016500844, Training Error = 0.01953125
Iteration no. 824, lr = 0.11529602, average batch_loss = 0.068718437831842, Training Error = 0.0078125
Iteration no. 825, lr = 0.11529602, average batch_loss = 0.055974465548215, Training Error = 0.005859375
Iteration no. 826, lr = 0.11529602, average batch_loss = 0.073468364988313, Training Error = 0.021484375
Iteration no. 827, lr = 0.11529602, average batch_loss = 0.0602230026065, Training Error = 0.0078125
Iteration no. 828, lr = 0.11529602, average batch_loss = 0.065949408342171, Training Error = 0.021484375
Iteration no. 829, lr = 0.11529602, average batch_loss = 0.076508974211309, Training Error = 0.013671875
Iteration no. 830, lr = 0.11529602, average batch_loss = 0.06824581659023, Training Error = 0.01171875
Iteration no. 831, lr = 0.11529602, average batch_loss = 0.07649496998483, Training Error = 0.0234375
Iteration no. 832, lr = 0.11529602, average batch_loss = 0.069118314600365, Training Error = 0.015625
Iteration no. 833, lr = 0.11529602, average batch_loss = 0.07231956174324, Training Error = 0.009765625
Iteration no. 834, lr = 0.11529602, average batch_loss = 0.068547678364875, Training Error = 0.017578125
Iteration no. 835, lr = 0.11529602, average batch_loss = 0.076807559606444, Training Error = 0.0078125
Iteration no. 836, lr = 0.11529602, average batch_loss = 0.078636892609978, Training Error = 0.017578125
Iteration no. 837, lr = 0.11529602, average batch_loss = 0.0749021059794, Training Error = 0.025390625
Iteration no. 838, lr = 0.11529602, average batch_loss = 0.068542516298655, Training Error = 0.0234375
Iteration no. 839, lr = 0.11529602, average batch_loss = 0.067019469719149, Training Error = 0.013671875
Iteration no. 840, lr = 0.11529602, average batch_loss = 0.073792642282106, Training Error = 0.01953125
Iteration no. 841, lr = 0.11529602, average batch_loss = 0.072914960088463, Training Error = 0.015625
Iteration no. 842, lr = 0.11529602, average batch_loss = 0.069729520280641, Training Error = 0.017578125
Iteration no. 843, lr = 0.11529602, average batch_loss = 0.072302303931342, Training Error = 0.017578125
Iteration no. 844, lr = 0.11529602, average batch_loss = 0.066726361756777, Training Error = 0.021484375
Iteration no. 845, lr = 0.11529602, average batch_loss = 0.07715451631831, Training Error = 0.025390625
Iteration no. 846, lr = 0.11529602, average batch_loss = 0.070767852727701, Training Error = 0.013671875
Iteration no. 847, lr = 0.11529602, average batch_loss = 0.076283867506873, Training Error = 0.021484375
Iteration no. 848, lr = 0.11529602, average batch_loss = 0.054363156897222, Training Error = 0.009765625
Iteration no. 849, lr = 0.11529602, average batch_loss = 0.057446919272168, Training Error = 0.009765625
Iteration no. 850, lr = 0.11529602, average batch_loss = 0.069878884327856, Training Error = 0.015625
Testing... average test_loss = 0.078797284208736, average test_pred_err = 0.016
Iteration no. 851, lr = 0.11529602, average batch_loss = 0.056918363294353, Training Error = 0.0078125
Iteration no. 852, lr = 0.11529602, average batch_loss = 0.076970315923656, Training Error = 0.01953125
Iteration no. 853, lr = 0.11529602, average batch_loss = 0.072462517561234, Training Error = 0.01953125
Iteration no. 854, lr = 0.11529602, average batch_loss = 0.067617636234883, Training Error = 0.015625
Iteration no. 855, lr = 0.11529602, average batch_loss = 0.070232047917665, Training Error = 0.0078125
Iteration no. 856, lr = 0.11529602, average batch_loss = 0.075646618784034, Training Error = 0.01953125
Iteration no. 857, lr = 0.11529602, average batch_loss = 0.076502567050192, Training Error = 0.017578125
Iteration no. 858, lr = 0.11529602, average batch_loss = 0.064611110365036, Training Error = 0.013671875
Iteration no. 859, lr = 0.11529602, average batch_loss = 0.07369581487474, Training Error = 0.013671875
Iteration no. 860, lr = 0.11529602, average batch_loss = 0.082978075219433, Training Error = 0.021484375
Iteration no. 861, lr = 0.11529602, average batch_loss = 0.062584275208595, Training Error = 0.01171875
Iteration no. 862, lr = 0.11529602, average batch_loss = 0.063685849915293, Training Error = 0.009765625
Iteration no. 863, lr = 0.11529602, average batch_loss = 0.06174479620653, Training Error = 0.013671875
Iteration no. 864, lr = 0.11529602, average batch_loss = 0.07355809262291, Training Error = 0.015625
Iteration no. 865, lr = 0.11529602, average batch_loss = 0.063472323927456, Training Error = 0.017578125
Iteration no. 866, lr = 0.11529602, average batch_loss = 0.060250322974137, Training Error = 0.015625
Iteration no. 867, lr = 0.11529602, average batch_loss = 0.063231392048341, Training Error = 0.017578125
Iteration no. 868, lr = 0.11529602, average batch_loss = 0.071528186328464, Training Error = 0.025390625
Iteration no. 869, lr = 0.11529602, average batch_loss = 0.054958662395976, Training Error = 0.00390625
Iteration no. 870, lr = 0.11529602, average batch_loss = 0.059960069210003, Training Error = 0.0078125
Iteration no. 871, lr = 0.11529602, average batch_loss = 0.07273243119126, Training Error = 0.01953125
Iteration no. 872, lr = 0.11529602, average batch_loss = 0.064021421248291, Training Error = 0.009765625
Iteration no. 873, lr = 0.11529602, average batch_loss = 0.077688907737548, Training Error = 0.021484375
Iteration no. 874, lr = 0.11529602, average batch_loss = 0.076530773426291, Training Error = 0.0234375
Iteration no. 875, lr = 0.11529602, average batch_loss = 0.065850288031381, Training Error = 0.01171875
Iteration no. 876, lr = 0.11529602, average batch_loss = 0.054731907654821, Training Error = 0.0078125
Iteration no. 877, lr = 0.11529602, average batch_loss = 0.064995618362236, Training Error = 0.0078125
Iteration no. 878, lr = 0.11529602, average batch_loss = 0.063943021406653, Training Error = 0.01171875
Iteration no. 879, lr = 0.11529602, average batch_loss = 0.068372696048694, Training Error = 0.01171875
Iteration no. 880, lr = 0.11529602, average batch_loss = 0.081716994425772, Training Error = 0.015625
Iteration no. 881, lr = 0.11529602, average batch_loss = 0.076192043882641, Training Error = 0.009765625
Iteration no. 882, lr = 0.11529602, average batch_loss = 0.070778479554945, Training Error = 0.01171875
Iteration no. 883, lr = 0.11529602, average batch_loss = 0.072519388368128, Training Error = 0.015625
Iteration no. 884, lr = 0.11529602, average batch_loss = 0.07153687817795, Training Error = 0.015625
Iteration no. 885, lr = 0.11529602, average batch_loss = 0.076317299548492, Training Error = 0.021484375
Iteration no. 886, lr = 0.11529602, average batch_loss = 0.074116778141555, Training Error = 0.02734375
Iteration no. 887, lr = 0.11529602, average batch_loss = 0.063596262111404, Training Error = 0.01953125
Iteration no. 888, lr = 0.11529602, average batch_loss = 0.069295168628158, Training Error = 0.01953125
Iteration no. 889, lr = 0.11529602, average batch_loss = 0.075602138109076, Training Error = 0.015625
Iteration no. 890, lr = 0.11529602, average batch_loss = 0.069425812712684, Training Error = 0.017578125
Iteration no. 891, lr = 0.11529602, average batch_loss = 0.062205273690926, Training Error = 0.009765625
Iteration no. 892, lr = 0.11529602, average batch_loss = 0.08477490645635, Training Error = 0.01953125
Iteration no. 893, lr = 0.11529602, average batch_loss = 0.066941192955306, Training Error = 0.021484375
Iteration no. 894, lr = 0.11529602, average batch_loss = 0.077073944196506, Training Error = 0.021484375
Iteration no. 895, lr = 0.11529602, average batch_loss = 0.056480683286773, Training Error = 0.0078125
Iteration no. 896, lr = 0.11529602, average batch_loss = 0.068368960837871, Training Error = 0.017578125
Iteration no. 897, lr = 0.11529602, average batch_loss = 0.078048116228399, Training Error = 0.015625
Iteration no. 898, lr = 0.11529602, average batch_loss = 0.05982285480246, Training Error = 0.009765625
Iteration no. 899, lr = 0.11529602, average batch_loss = 0.081124803976723, Training Error = 0.02734375
Iteration no. 900, lr = 0.11529602, average batch_loss = 0.068211539006138, Training Error = 0.013671875
Testing... average test_loss = 0.071393713514001, average test_pred_err = 0.017
Iteration no. 901, lr = 0.080707214, average batch_loss = 0.070470788855127, Training Error = 0.013671875
Iteration no. 902, lr = 0.080707214, average batch_loss = 0.062459463760104, Training Error = 0.009765625
Iteration no. 903, lr = 0.080707214, average batch_loss = 0.076730664804998, Training Error = 0.01953125
Iteration no. 904, lr = 0.080707214, average batch_loss = 0.081402596935548, Training Error = 0.01953125
Iteration no. 905, lr = 0.080707214, average batch_loss = 0.068513106572404, Training Error = 0.015625
Iteration no. 906, lr = 0.080707214, average batch_loss = 0.061489585718138, Training Error = 0.017578125
