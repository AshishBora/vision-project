Loading pretrained model... done
Testing... average test_loss = 7.1997453351198, average test_pred_err = 0.499
Iteration no. 1, lr = 2, attribute lr =0.02, average batch_loss = 8.0206241799606, Training Error = 0.48828125
Iteration no. 2, lr = 2, attribute lr =0.02, average batch_loss = 3.8511027182006, Training Error = 0.4921875
Iteration no. 3, lr = 2, attribute lr =0.02, average batch_loss = 1.6104330026476, Training Error = 0.5078125
Iteration no. 4, lr = 2, attribute lr =0.02, average batch_loss = 1.4692333424742, Training Error = 0.462890625
Iteration no. 5, lr = 2, attribute lr =0.02, average batch_loss = 1.6009583855164, Training Error = 0.5
Iteration no. 6, lr = 2, attribute lr =0.02, average batch_loss = 1.065855532193, Training Error = 0.447265625
Iteration no. 7, lr = 2, attribute lr =0.02, average batch_loss = 0.9786735076698, Training Error = 0.427734375
Iteration no. 8, lr = 2, attribute lr =0.02, average batch_loss = 1.0026356397938, Training Error = 0.494140625
Iteration no. 9, lr = 2, attribute lr =0.02, average batch_loss = 0.89592277827328, Training Error = 0.4375
Iteration no. 10, lr = 2, attribute lr =0.02, average batch_loss = 0.9160001769221, Training Error = 0.466796875
Iteration no. 11, lr = 2, attribute lr =0.02, average batch_loss = 1.0147040170731, Training Error = 0.48828125
Iteration no. 12, lr = 2, attribute lr =0.02, average batch_loss = 0.8227887888194, Training Error = 0.443359375
Iteration no. 13, lr = 2, attribute lr =0.02, average batch_loss = 0.87378398938183, Training Error = 0.46484375
Iteration no. 14, lr = 2, attribute lr =0.02, average batch_loss = 0.96157597951856, Training Error = 0.486328125
Iteration no. 15, lr = 2, attribute lr =0.02, average batch_loss = 1.1630202015659, Training Error = 0.4765625
Iteration no. 16, lr = 2, attribute lr =0.02, average batch_loss = 0.92288655901485, Training Error = 0.484375
Iteration no. 17, lr = 2, attribute lr =0.02, average batch_loss = 0.99907034049626, Training Error = 0.482421875
Iteration no. 18, lr = 2, attribute lr =0.02, average batch_loss = 0.77356981628273, Training Error = 0.435546875
Iteration no. 19, lr = 2, attribute lr =0.02, average batch_loss = 0.73817336630912, Training Error = 0.453125
Iteration no. 20, lr = 2, attribute lr =0.02, average batch_loss = 0.67441481580659, Training Error = 0.3984375
Iteration no. 21, lr = 2, attribute lr =0.02, average batch_loss = 0.70696695689069, Training Error = 0.431640625
Iteration no. 22, lr = 2, attribute lr =0.02, average batch_loss = 0.72585905608371, Training Error = 0.435546875
Iteration no. 23, lr = 2, attribute lr =0.02, average batch_loss = 0.7555818913571, Training Error = 0.4453125
Iteration no. 24, lr = 2, attribute lr =0.02, average batch_loss = 0.72065348098207, Training Error = 0.416015625
Iteration no. 25, lr = 2, attribute lr =0.02, average batch_loss = 0.7757900259726, Training Error = 0.435546875
Iteration no. 26, lr = 2, attribute lr =0.02, average batch_loss = 0.72808538613322, Training Error = 0.419921875
Iteration no. 27, lr = 2, attribute lr =0.02, average batch_loss = 0.71477009108217, Training Error = 0.427734375
Iteration no. 28, lr = 2, attribute lr =0.02, average batch_loss = 0.71846374176109, Training Error = 0.427734375
Iteration no. 29, lr = 2, attribute lr =0.02, average batch_loss = 0.73973772923606, Training Error = 0.44921875
Iteration no. 30, lr = 2, attribute lr =0.02, average batch_loss = 0.67353540796657, Training Error = 0.40625
Iteration no. 31, lr = 2, attribute lr =0.02, average batch_loss = 0.7184480516062, Training Error = 0.40625
Iteration no. 32, lr = 2, attribute lr =0.02, average batch_loss = 0.69443804610363, Training Error = 0.416015625
Iteration no. 33, lr = 2, attribute lr =0.02, average batch_loss = 0.7187693559387, Training Error = 0.404296875
Iteration no. 34, lr = 2, attribute lr =0.02, average batch_loss = 0.68580517858542, Training Error = 0.41015625
Iteration no. 35, lr = 2, attribute lr =0.02, average batch_loss = 0.721935651153, Training Error = 0.419921875
Iteration no. 36, lr = 2, attribute lr =0.02, average batch_loss = 0.68667820524777, Training Error = 0.416015625
Iteration no. 37, lr = 2, attribute lr =0.02, average batch_loss = 0.66354380113344, Training Error = 0.365234375
Iteration no. 38, lr = 2, attribute lr =0.02, average batch_loss = 0.67910910379558, Training Error = 0.400390625
Iteration no. 39, lr = 2, attribute lr =0.02, average batch_loss = 0.66959688959837, Training Error = 0.431640625
Iteration no. 40, lr = 2, attribute lr =0.02, average batch_loss = 0.65896595259671, Training Error = 0.376953125
Iteration no. 41, lr = 2, attribute lr =0.02, average batch_loss = 0.65879791252257, Training Error = 0.39453125
Iteration no. 42, lr = 2, attribute lr =0.02, average batch_loss = 0.65973417855079, Training Error = 0.392578125
Iteration no. 43, lr = 2, attribute lr =0.02, average batch_loss = 0.64726963599781, Training Error = 0.37890625
Iteration no. 44, lr = 2, attribute lr =0.02, average batch_loss = 0.62043553681066, Training Error = 0.33203125
Iteration no. 45, lr = 2, attribute lr =0.02, average batch_loss = 0.63581965375026, Training Error = 0.35546875
Iteration no. 46, lr = 2, attribute lr =0.02, average batch_loss = 0.6697265636418, Training Error = 0.4375
Iteration no. 47, lr = 2, attribute lr =0.02, average batch_loss = 0.63016461306221, Training Error = 0.34375
Iteration no. 48, lr = 2, attribute lr =0.02, average batch_loss = 0.65553440187005, Training Error = 0.38671875
Iteration no. 49, lr = 2, attribute lr =0.02, average batch_loss = 0.65258976087985, Training Error = 0.373046875
Iteration no. 50, lr = 2, attribute lr =0.02, average batch_loss = 0.64491885139241, Training Error = 0.3828125
Testing... average test_loss = 0.74119971738559, average test_pred_err = 0.508
Iteration no. 51, lr = 2, attribute lr =0.02, average batch_loss = 0.62562442419549, Training Error = 0.34765625
Iteration no. 52, lr = 2, attribute lr =0.02, average batch_loss = 0.63041508808766, Training Error = 0.376953125
Iteration no. 53, lr = 2, attribute lr =0.02, average batch_loss = 0.62303577427769, Training Error = 0.3515625
Iteration no. 54, lr = 2, attribute lr =0.02, average batch_loss = 0.62892459379832, Training Error = 0.369140625
Iteration no. 55, lr = 2, attribute lr =0.02, average batch_loss = 0.64426656401812, Training Error = 0.375
Iteration no. 56, lr = 2, attribute lr =0.02, average batch_loss = 0.61002808264348, Training Error = 0.337890625
Iteration no. 57, lr = 2, attribute lr =0.02, average batch_loss = 0.63212299548537, Training Error = 0.376953125
Iteration no. 58, lr = 2, attribute lr =0.02, average batch_loss = 0.64374345320677, Training Error = 0.37109375
Iteration no. 59, lr = 2, attribute lr =0.02, average batch_loss = 0.67191588205337, Training Error = 0.431640625
Iteration no. 60, lr = 2, attribute lr =0.02, average batch_loss = 0.65574189431965, Training Error = 0.376953125
Iteration no. 61, lr = 2, attribute lr =0.02, average batch_loss = 0.64801830010789, Training Error = 0.384765625
Iteration no. 62, lr = 2, attribute lr =0.02, average batch_loss = 0.63533907375114, Training Error = 0.361328125
Iteration no. 63, lr = 2, attribute lr =0.02, average batch_loss = 0.61288833287117, Training Error = 0.345703125
Iteration no. 64, lr = 2, attribute lr =0.02, average batch_loss = 0.61401153062699, Training Error = 0.33203125
Iteration no. 65, lr = 2, attribute lr =0.02, average batch_loss = 0.61880789785482, Training Error = 0.35546875
Iteration no. 66, lr = 2, attribute lr =0.02, average batch_loss = 0.62778798742617, Training Error = 0.361328125
Iteration no. 67, lr = 2, attribute lr =0.02, average batch_loss = 0.61986943080037, Training Error = 0.353515625
Iteration no. 68, lr = 2, attribute lr =0.02, average batch_loss = 0.63149840658143, Training Error = 0.373046875
Iteration no. 69, lr = 2, attribute lr =0.02, average batch_loss = 0.62459475677625, Training Error = 0.345703125
Iteration no. 70, lr = 2, attribute lr =0.02, average batch_loss = 0.61573392230541, Training Error = 0.353515625
Iteration no. 71, lr = 2, attribute lr =0.02, average batch_loss = 0.63480140325013, Training Error = 0.37109375
Iteration no. 72, lr = 2, attribute lr =0.02, average batch_loss = 0.63244350093859, Training Error = 0.390625
Iteration no. 73, lr = 2, attribute lr =0.02, average batch_loss = 0.62554494230595, Training Error = 0.361328125
Iteration no. 74, lr = 2, attribute lr =0.02, average batch_loss = 0.6054133108328, Training Error = 0.3359375
Iteration no. 75, lr = 2, attribute lr =0.02, average batch_loss = 0.63017694740691, Training Error = 0.375
Iteration no. 76, lr = 2, attribute lr =0.02, average batch_loss = 0.61651112414397, Training Error = 0.3359375
Iteration no. 77, lr = 2, attribute lr =0.02, average batch_loss = 0.5837209786613, Training Error = 0.322265625
Iteration no. 78, lr = 2, attribute lr =0.02, average batch_loss = 0.58515620160334, Training Error = 0.30078125
Iteration no. 79, lr = 2, attribute lr =0.02, average batch_loss = 0.61461214342411, Training Error = 0.337890625
Iteration no. 80, lr = 2, attribute lr =0.02, average batch_loss = 0.60130833403023, Training Error = 0.3125
Iteration no. 81, lr = 2, attribute lr =0.02, average batch_loss = 0.60122766392939, Training Error = 0.333984375
Iteration no. 82, lr = 2, attribute lr =0.02, average batch_loss = 0.59859843056709, Training Error = 0.337890625
Iteration no. 83, lr = 2, attribute lr =0.02, average batch_loss = 0.61468445770767, Training Error = 0.337890625
Iteration no. 84, lr = 2, attribute lr =0.02, average batch_loss = 0.60779290621409, Training Error = 0.33984375
Iteration no. 85, lr = 2, attribute lr =0.02, average batch_loss = 0.63228401819906, Training Error = 0.3515625
Iteration no. 86, lr = 2, attribute lr =0.02, average batch_loss = 0.62119355603406, Training Error = 0.37109375
Iteration no. 87, lr = 2, attribute lr =0.02, average batch_loss = 0.64476919676558, Training Error = 0.34765625
Iteration no. 88, lr = 2, attribute lr =0.02, average batch_loss = 0.61862872067758, Training Error = 0.345703125
Iteration no. 89, lr = 2, attribute lr =0.02, average batch_loss = 0.6107677938426, Training Error = 0.34375
Iteration no. 90, lr = 2, attribute lr =0.02, average batch_loss = 0.61267217896844, Training Error = 0.353515625
Iteration no. 91, lr = 2, attribute lr =0.02, average batch_loss = 0.60263943385795, Training Error = 0.345703125
Iteration no. 92, lr = 2, attribute lr =0.02, average batch_loss = 0.59672452640104, Training Error = 0.3359375
Iteration no. 93, lr = 2, attribute lr =0.02, average batch_loss = 0.63031542720349, Training Error = 0.3671875
Iteration no. 94, lr = 2, attribute lr =0.02, average batch_loss = 0.62537842237297, Training Error = 0.3515625
Iteration no. 95, lr = 2, attribute lr =0.02, average batch_loss = 0.61095580490634, Training Error = 0.361328125
Iteration no. 96, lr = 2, attribute lr =0.02, average batch_loss = 0.59246670555163, Training Error = 0.34765625
Iteration no. 97, lr = 2, attribute lr =0.02, average batch_loss = 0.63747683679943, Training Error = 0.38671875
Iteration no. 98, lr = 2, attribute lr =0.02, average batch_loss = 0.60322385012481, Training Error = 0.328125
Iteration no. 99, lr = 2, attribute lr =0.02, average batch_loss = 0.60683127739096, Training Error = 0.357421875
Iteration no. 100, lr = 2, attribute lr =0.02, average batch_loss = 0.59988841656498, Training Error = 0.330078125
Testing... average test_loss = 0.77013094712216, average test_pred_err = 0.484
Snapshotting C_model... done
Iteration no. 101, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57074933394859, Training Error = 0.2890625
Iteration no. 102, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61955616356665, Training Error = 0.361328125
Iteration no. 103, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59195015766433, Training Error = 0.310546875
Iteration no. 104, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59666378121483, Training Error = 0.3125
Iteration no. 105, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61848107973415, Training Error = 0.349609375
Iteration no. 106, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62104665224702, Training Error = 0.322265625
Iteration no. 107, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60798022175814, Training Error = 0.31640625
Iteration no. 108, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62634363696311, Training Error = 0.376953125
Iteration no. 109, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60019187492155, Training Error = 0.330078125
Iteration no. 110, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57387241139001, Training Error = 0.298828125
Iteration no. 111, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61241928957873, Training Error = 0.3515625
Iteration no. 112, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59716404162807, Training Error = 0.341796875
Iteration no. 113, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57824064016753, Training Error = 0.326171875
Iteration no. 114, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57310967067954, Training Error = 0.318359375
Iteration no. 115, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58275146012616, Training Error = 0.314453125
Iteration no. 116, lr = 1.4, attribute lr =0.014, average batch_loss = 0.65403935866191, Training Error = 0.380859375
Iteration no. 117, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58034371799361, Training Error = 0.314453125
Iteration no. 118, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59119312645308, Training Error = 0.3125
Iteration no. 119, lr = 1.4, attribute lr =0.014, average batch_loss = 0.6069877738452, Training Error = 0.33984375
Iteration no. 120, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57372432978602, Training Error = 0.29296875
Iteration no. 121, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58464341130525, Training Error = 0.330078125
Iteration no. 122, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57310111204006, Training Error = 0.322265625
Iteration no. 123, lr = 1.4, attribute lr =0.014, average batch_loss = 0.63313507643302, Training Error = 0.365234375
Iteration no. 124, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59739711735019, Training Error = 0.32421875
Iteration no. 125, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61102457605942, Training Error = 0.34765625
Iteration no. 126, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58622867526225, Training Error = 0.31640625
Iteration no. 127, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61329412598797, Training Error = 0.337890625
Iteration no. 128, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61729223434666, Training Error = 0.3515625
Iteration no. 129, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5776673776279, Training Error = 0.30078125
Iteration no. 130, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58297841314257, Training Error = 0.302734375
Iteration no. 131, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56106883288157, Training Error = 0.287109375
Iteration no. 132, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59216171301553, Training Error = 0.32421875
Iteration no. 133, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57666782159278, Training Error = 0.322265625
Iteration no. 134, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62110262905006, Training Error = 0.369140625
Iteration no. 135, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5545000274001, Training Error = 0.302734375
Iteration no. 136, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57246058188481, Training Error = 0.310546875
Iteration no. 137, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60300609183195, Training Error = 0.333984375
Iteration no. 138, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58014111728214, Training Error = 0.314453125
Iteration no. 139, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58728956676161, Training Error = 0.3359375
Iteration no. 140, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58402885323686, Training Error = 0.302734375
Iteration no. 141, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56830015285005, Training Error = 0.279296875
Iteration no. 142, lr = 1.4, attribute lr =0.014, average batch_loss = 0.63182313565882, Training Error = 0.361328125
Iteration no. 143, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58349548457972, Training Error = 0.30859375
Iteration no. 144, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57944472378809, Training Error = 0.310546875
Iteration no. 145, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57561681066076, Training Error = 0.298828125
Iteration no. 146, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57971826366351, Training Error = 0.31640625
Iteration no. 147, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58115649662312, Training Error = 0.3046875
Iteration no. 148, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58729017109155, Training Error = 0.298828125
Iteration no. 149, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57391470459615, Training Error = 0.296875
Iteration no. 150, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5866470103417, Training Error = 0.326171875
Testing... average test_loss = 0.81044480558306, average test_pred_err = 0.489
Iteration no. 151, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57285123767787, Training Error = 0.298828125
Iteration no. 152, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59218538550869, Training Error = 0.318359375
Iteration no. 153, lr = 1.4, attribute lr =0.014, average batch_loss = 0.61256341129829, Training Error = 0.337890625
Iteration no. 154, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59541998513453, Training Error = 0.3359375
Iteration no. 155, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56982938732192, Training Error = 0.296875
Iteration no. 156, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58248027004226, Training Error = 0.326171875
Iteration no. 157, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5903286748781, Training Error = 0.3046875
Iteration no. 158, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56489620108822, Training Error = 0.287109375
Iteration no. 159, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59496213072655, Training Error = 0.3359375
Iteration no. 160, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57642243049544, Training Error = 0.31640625
Iteration no. 161, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57661599454824, Training Error = 0.287109375
Iteration no. 162, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56897015118983, Training Error = 0.310546875
Iteration no. 163, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57630543827937, Training Error = 0.3046875
Iteration no. 164, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57219016052726, Training Error = 0.294921875
Iteration no. 165, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56306314811127, Training Error = 0.294921875
Iteration no. 166, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5783532816319, Training Error = 0.296875
Iteration no. 167, lr = 1.4, attribute lr =0.014, average batch_loss = 0.62737415229135, Training Error = 0.353515625
Iteration no. 168, lr = 1.4, attribute lr =0.014, average batch_loss = 0.53944299188691, Training Error = 0.2734375
Iteration no. 169, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55252865176313, Training Error = 0.294921875
Iteration no. 170, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5832547435648, Training Error = 0.29296875
Iteration no. 171, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60220973511423, Training Error = 0.33984375
Iteration no. 172, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56908317945306, Training Error = 0.306640625
Iteration no. 173, lr = 1.4, attribute lr =0.014, average batch_loss = 0.60929347111655, Training Error = 0.34765625
Iteration no. 174, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58215495226595, Training Error = 0.318359375
Iteration no. 175, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5688324279438, Training Error = 0.3125
Iteration no. 176, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57555286640101, Training Error = 0.33203125
Iteration no. 177, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57677414099103, Training Error = 0.3046875
Iteration no. 178, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56935411326801, Training Error = 0.302734375
Iteration no. 179, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56484367452888, Training Error = 0.302734375
Iteration no. 180, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55059342035625, Training Error = 0.283203125
Iteration no. 181, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56845554461337, Training Error = 0.30859375
Iteration no. 182, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56246449283865, Training Error = 0.29296875
Iteration no. 183, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5913500185344, Training Error = 0.3203125
Iteration no. 184, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59355867021374, Training Error = 0.32421875
Iteration no. 185, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57547555378127, Training Error = 0.31640625
Iteration no. 186, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58613880635264, Training Error = 0.326171875
Iteration no. 187, lr = 1.4, attribute lr =0.014, average batch_loss = 0.59485279929338, Training Error = 0.31640625
Iteration no. 188, lr = 1.4, attribute lr =0.014, average batch_loss = 0.58002086153811, Training Error = 0.3046875
Iteration no. 189, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56093988491051, Training Error = 0.298828125
Iteration no. 190, lr = 1.4, attribute lr =0.014, average batch_loss = 0.56158271683483, Training Error = 0.283203125
Iteration no. 191, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57705752428924, Training Error = 0.314453125
Iteration no. 192, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55864060258203, Training Error = 0.296875
Iteration no. 193, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57491195804244, Training Error = 0.3046875
Iteration no. 194, lr = 1.4, attribute lr =0.014, average batch_loss = 0.54431849229382, Training Error = 0.3046875
Iteration no. 195, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57747226194607, Training Error = 0.3125
Iteration no. 196, lr = 1.4, attribute lr =0.014, average batch_loss = 0.55695459389174, Training Error = 0.298828125
Iteration no. 197, lr = 1.4, attribute lr =0.014, average batch_loss = 0.57364596006206, Training Error = 0.306640625
Iteration no. 198, lr = 1.4, attribute lr =0.014, average batch_loss = 0.52458798015888, Training Error = 0.26953125
Iteration no. 199, lr = 1.4, attribute lr =0.014, average batch_loss = 0.53186822960127, Training Error = 0.265625
Iteration no. 200, lr = 1.4, attribute lr =0.014, average batch_loss = 0.5260540811887, Training Error = 0.248046875
Testing... average test_loss = 0.85415347615712, average test_pred_err = 0.495
Snapshotting C_model... done
Iteration no. 201, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56932081033989, Training Error = 0.30859375
Iteration no. 202, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57303936881421, Training Error = 0.294921875
Iteration no. 203, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56217902677817, Training Error = 0.306640625
Iteration no. 204, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55707248346569, Training Error = 0.2890625
Iteration no. 205, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55539948064907, Training Error = 0.275390625
Iteration no. 206, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54467287563903, Training Error = 0.298828125
Iteration no. 207, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56669448070656, Training Error = 0.306640625
Iteration no. 208, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52719954625105, Training Error = 0.236328125
Iteration no. 209, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56823734033245, Training Error = 0.3046875
Iteration no. 210, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58101827629255, Training Error = 0.328125
Iteration no. 211, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55781667238769, Training Error = 0.294921875
Iteration no. 212, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56180612472112, Training Error = 0.322265625
Iteration no. 213, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54209439652706, Training Error = 0.2578125
Iteration no. 214, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5613433368436, Training Error = 0.29296875
Iteration no. 215, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57481668935173, Training Error = 0.30859375
Iteration no. 216, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57283162139644, Training Error = 0.30859375
Iteration no. 217, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55693738804659, Training Error = 0.29296875
Iteration no. 218, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57202056998869, Training Error = 0.3046875
Iteration no. 219, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.547262089679, Training Error = 0.291015625
Iteration no. 220, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55146949468072, Training Error = 0.279296875
Iteration no. 221, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54166613076256, Training Error = 0.275390625
Iteration no. 222, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56800368055962, Training Error = 0.294921875
Iteration no. 223, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58863917827562, Training Error = 0.314453125
Iteration no. 224, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56613771369117, Training Error = 0.28515625
Iteration no. 225, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.60438180833083, Training Error = 0.34375
Iteration no. 226, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57316891477372, Training Error = 0.306640625
Iteration no. 227, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52781902309305, Training Error = 0.271484375
Iteration no. 228, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57462627826237, Training Error = 0.3125
Iteration no. 229, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56230777010359, Training Error = 0.302734375
Iteration no. 230, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53109684386165, Training Error = 0.2578125
Iteration no. 231, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56645044962806, Training Error = 0.32421875
Iteration no. 232, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5417621669849, Training Error = 0.298828125
Iteration no. 233, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.61025666608346, Training Error = 0.328125
Iteration no. 234, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56935912678491, Training Error = 0.30859375
Iteration no. 235, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52669118407451, Training Error = 0.248046875
Iteration no. 236, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.60058088632382, Training Error = 0.32421875
Iteration no. 237, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58942550243675, Training Error = 0.31640625
Iteration no. 238, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56141868883587, Training Error = 0.283203125
Iteration no. 239, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53707719503353, Training Error = 0.27734375
Iteration no. 240, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57832127401432, Training Error = 0.298828125
Iteration no. 241, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58079163514655, Training Error = 0.3125
Iteration no. 242, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57035475566842, Training Error = 0.291015625
Iteration no. 243, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54699632412878, Training Error = 0.27734375
Iteration no. 244, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56381602143396, Training Error = 0.296875
Iteration no. 245, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53397979540266, Training Error = 0.263671875
Iteration no. 246, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54329552945718, Training Error = 0.28125
Iteration no. 247, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53970079477378, Training Error = 0.28125
Iteration no. 248, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56928970727657, Training Error = 0.296875
Iteration no. 249, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54821572386034, Training Error = 0.267578125
Iteration no. 250, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54880273931855, Training Error = 0.2734375
Testing... average test_loss = 0.80687537531845, average test_pred_err = 0.463
Iteration no. 251, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58397392007532, Training Error = 0.32421875
Iteration no. 252, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53066903280451, Training Error = 0.255859375
Iteration no. 253, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57958412168263, Training Error = 0.322265625
Iteration no. 254, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55268675479371, Training Error = 0.294921875
Iteration no. 255, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53310405960782, Training Error = 0.265625
Iteration no. 256, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54262960678438, Training Error = 0.259765625
Iteration no. 257, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5623821267014, Training Error = 0.29296875
Iteration no. 258, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54569024762758, Training Error = 0.28515625
Iteration no. 259, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55491848785754, Training Error = 0.296875
Iteration no. 260, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55452978131902, Training Error = 0.287109375
Iteration no. 261, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58367850476308, Training Error = 0.345703125
Iteration no. 262, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52364968540644, Training Error = 0.2734375
Iteration no. 263, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54498736926417, Training Error = 0.294921875
Iteration no. 264, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.50733545217739, Training Error = 0.251953125
Iteration no. 265, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53067553019089, Training Error = 0.251953125
Iteration no. 266, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55198983057795, Training Error = 0.30078125
Iteration no. 267, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54129361123671, Training Error = 0.29296875
Iteration no. 268, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54878739982651, Training Error = 0.28125
Iteration no. 269, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54944575646706, Training Error = 0.291015625
Iteration no. 270, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55255113492913, Training Error = 0.2734375
Iteration no. 271, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54760680719356, Training Error = 0.26171875
Iteration no. 272, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.58420420586122, Training Error = 0.328125
Iteration no. 273, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53487627408959, Training Error = 0.2734375
Iteration no. 274, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55211824118582, Training Error = 0.279296875
Iteration no. 275, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53293403462947, Training Error = 0.26953125
Iteration no. 276, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53915601010426, Training Error = 0.265625
Iteration no. 277, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54368116201006, Training Error = 0.283203125
Iteration no. 278, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.57060936074743, Training Error = 0.306640625
Iteration no. 279, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.56771856662817, Training Error = 0.310546875
Iteration no. 280, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52244294735046, Training Error = 0.267578125
Iteration no. 281, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53544170711756, Training Error = 0.259765625
Iteration no. 282, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54658616276127, Training Error = 0.287109375
Iteration no. 283, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55052596449022, Training Error = 0.294921875
Iteration no. 284, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53589747334929, Training Error = 0.26953125
Iteration no. 285, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.50531831951227, Training Error = 0.2734375
Iteration no. 286, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54567839740232, Training Error = 0.287109375
Iteration no. 287, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53352958414711, Training Error = 0.26953125
Iteration no. 288, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.53181389248111, Training Error = 0.287109375
Iteration no. 289, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5190352077473, Training Error = 0.251953125
Iteration no. 290, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55071978829334, Training Error = 0.298828125
Iteration no. 291, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.5419054781088, Training Error = 0.279296875
Iteration no. 292, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.51538366342317, Training Error = 0.25
Iteration no. 293, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54257132117627, Training Error = 0.279296875
Iteration no. 294, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52183663706501, Training Error = 0.251953125
Iteration no. 295, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55588521559224, Training Error = 0.2890625
Iteration no. 296, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.50202202080995, Training Error = 0.2265625
Iteration no. 297, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.52796708552012, Training Error = 0.271484375
Iteration no. 298, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.54255677667421, Training Error = 0.28125
Iteration no. 299, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55323952188303, Training Error = 0.283203125
Iteration no. 300, lr = 0.98, attribute lr =0.0098, average batch_loss = 0.55759677597102, Training Error = 0.27734375
Testing... average test_loss = 0.855881005181, average test_pred_err = 0.459
Snapshotting C_model... done
Iteration no. 301, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54165902094859, Training Error = 0.26953125
Iteration no. 302, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55009405699071, Training Error = 0.2734375
Iteration no. 303, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54650698675142, Training Error = 0.283203125
Iteration no. 304, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5122275832385, Training Error = 0.259765625
Iteration no. 305, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54896064609749, Training Error = 0.29296875
Iteration no. 306, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55068796433653, Training Error = 0.306640625
Iteration no. 307, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54801106763926, Training Error = 0.28125
Iteration no. 308, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55578642711017, Training Error = 0.30078125
Iteration no. 309, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52929236853361, Training Error = 0.265625
Iteration no. 310, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55633905040556, Training Error = 0.294921875
Iteration no. 311, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52869797426186, Training Error = 0.267578125
Iteration no. 312, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54943407858376, Training Error = 0.271484375
Iteration no. 313, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51356519392937, Training Error = 0.2734375
Iteration no. 314, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50684649887241, Training Error = 0.26171875
Iteration no. 315, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54987364129445, Training Error = 0.28515625
Iteration no. 316, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54047342675872, Training Error = 0.28515625
Iteration no. 317, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5365485646814, Training Error = 0.28515625
Iteration no. 318, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5225133019838, Training Error = 0.248046875
Iteration no. 319, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52967119916495, Training Error = 0.28125
Iteration no. 320, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52261185673152, Training Error = 0.279296875
Iteration no. 321, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53083785206328, Training Error = 0.279296875
Iteration no. 322, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50438874972103, Training Error = 0.25
Iteration no. 323, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56011749204512, Training Error = 0.3046875
Iteration no. 324, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52931599640037, Training Error = 0.259765625
Iteration no. 325, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53751473652317, Training Error = 0.283203125
Iteration no. 326, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5355009697721, Training Error = 0.26953125
Iteration no. 327, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53119731282702, Training Error = 0.291015625
Iteration no. 328, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51559558014419, Training Error = 0.24609375
Iteration no. 329, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51807561220863, Training Error = 0.263671875
Iteration no. 330, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52657564735239, Training Error = 0.265625
Iteration no. 331, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53014014078781, Training Error = 0.28515625
Iteration no. 332, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55707962001916, Training Error = 0.30859375
Iteration no. 333, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56664896703123, Training Error = 0.296875
Iteration no. 334, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55034974109511, Training Error = 0.29296875
Iteration no. 335, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55320129321588, Training Error = 0.27734375
Iteration no. 336, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.549564240108, Training Error = 0.28515625
Iteration no. 337, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51277908553035, Training Error = 0.25390625
Iteration no. 338, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56355046432688, Training Error = 0.28515625
Iteration no. 339, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53004623788317, Training Error = 0.279296875
Iteration no. 340, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52825473238538, Training Error = 0.275390625
Iteration no. 341, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55270391353294, Training Error = 0.30078125
Iteration no. 342, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.549557958934, Training Error = 0.291015625
Iteration no. 343, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54819338310008, Training Error = 0.271484375
Iteration no. 344, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53883935484493, Training Error = 0.2890625
Iteration no. 345, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50749779783669, Training Error = 0.2578125
Iteration no. 346, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5379882871178, Training Error = 0.283203125
Iteration no. 347, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53228202394794, Training Error = 0.255859375
Iteration no. 348, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.49926901076137, Training Error = 0.263671875
Iteration no. 349, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52219742825474, Training Error = 0.291015625
Iteration no. 350, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53260811577841, Training Error = 0.28125
Testing... average test_loss = 0.89962086803857, average test_pred_err = 0.49
Iteration no. 351, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50069328366803, Training Error = 0.25
Iteration no. 352, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5427751721235, Training Error = 0.291015625
Iteration no. 353, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5465433109201, Training Error = 0.310546875
Iteration no. 354, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56033325457326, Training Error = 0.306640625
Iteration no. 355, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53988302597136, Training Error = 0.28515625
Iteration no. 356, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51893640221904, Training Error = 0.248046875
Iteration no. 357, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52920811780092, Training Error = 0.271484375
Iteration no. 358, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53851431550573, Training Error = 0.30078125
Iteration no. 359, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.57075842061714, Training Error = 0.3125
Iteration no. 360, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5309629472735, Training Error = 0.287109375
Iteration no. 361, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.49139436152141, Training Error = 0.240234375
Iteration no. 362, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.508132954056, Training Error = 0.267578125
Iteration no. 363, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53535356574102, Training Error = 0.291015625
Iteration no. 364, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50224078880874, Training Error = 0.25
Iteration no. 365, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51749648344919, Training Error = 0.283203125
Iteration no. 366, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55407883576739, Training Error = 0.29296875
Iteration no. 367, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53580362068019, Training Error = 0.263671875
Iteration no. 368, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.56785544105525, Training Error = 0.275390625
Iteration no. 369, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50999343808531, Training Error = 0.259765625
Iteration no. 370, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55508993749338, Training Error = 0.28515625
Iteration no. 371, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52788594710141, Training Error = 0.2734375
Iteration no. 372, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5262430471066, Training Error = 0.267578125
Iteration no. 373, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54635678223491, Training Error = 0.28125
Iteration no. 374, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53284854654966, Training Error = 0.267578125
Iteration no. 375, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51197122023858, Training Error = 0.2578125
Iteration no. 376, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50778943969483, Training Error = 0.248046875
Iteration no. 377, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53501711594746, Training Error = 0.267578125
Iteration no. 378, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52041536006999, Training Error = 0.263671875
Iteration no. 379, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54716155333296, Training Error = 0.296875
Iteration no. 380, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50846185406403, Training Error = 0.255859375
Iteration no. 381, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5430230421896, Training Error = 0.291015625
Iteration no. 382, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50470624664872, Training Error = 0.234375
Iteration no. 383, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.522534277814, Training Error = 0.2734375
Iteration no. 384, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.49235193428539, Training Error = 0.232421875
Iteration no. 385, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52608562914534, Training Error = 0.263671875
Iteration no. 386, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5042850626773, Training Error = 0.26171875
Iteration no. 387, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51284337956289, Training Error = 0.251953125
Iteration no. 388, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55811130952104, Training Error = 0.287109375
Iteration no. 389, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.5219636355379, Training Error = 0.259765625
Iteration no. 390, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.52530607250076, Training Error = 0.25390625
Iteration no. 391, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54915954719932, Training Error = 0.28515625
Iteration no. 392, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54159854567497, Training Error = 0.271484375
Iteration no. 393, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.50671006919193, Training Error = 0.267578125
Iteration no. 394, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54100447076801, Training Error = 0.287109375
Iteration no. 395, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.48766898580547, Training Error = 0.240234375
Iteration no. 396, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.53329253473708, Training Error = 0.28125
Iteration no. 397, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.55867397760959, Training Error = 0.314453125
Iteration no. 398, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.51904026094619, Training Error = 0.271484375
Iteration no. 399, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54873895437983, Training Error = 0.29296875
Iteration no. 400, lr = 0.686, attribute lr =0.00686, average batch_loss = 0.54273894477239, Training Error = 0.294921875
Testing... average test_loss = 0.87145995865687, average test_pred_err = 0.482
Snapshotting C_model... done
Iteration no. 401, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48978500272765, Training Error = 0.251953125
Iteration no. 402, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53894177703173, Training Error = 0.2890625
Iteration no. 403, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52520534016966, Training Error = 0.279296875
Iteration no. 404, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55394711766905, Training Error = 0.291015625
Iteration no. 405, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56547014705262, Training Error = 0.27734375
Iteration no. 406, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56329370705005, Training Error = 0.291015625
Iteration no. 407, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48622566391593, Training Error = 0.234375
Iteration no. 408, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53821895227316, Training Error = 0.287109375
Iteration no. 409, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52693941489463, Training Error = 0.271484375
Iteration no. 410, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52521721223032, Training Error = 0.27734375
Iteration no. 411, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50756614343899, Training Error = 0.255859375
Iteration no. 412, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53963319828461, Training Error = 0.296875
Iteration no. 413, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51865016547647, Training Error = 0.251953125
Iteration no. 414, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54753215261592, Training Error = 0.28125
Iteration no. 415, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51933302215871, Training Error = 0.259765625
Iteration no. 416, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54893296421227, Training Error = 0.279296875
Iteration no. 417, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56485068906453, Training Error = 0.3046875
Iteration no. 418, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.4999215140045, Training Error = 0.2421875
Iteration no. 419, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50900537464604, Training Error = 0.265625
Iteration no. 420, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5409115614893, Training Error = 0.2890625
Iteration no. 421, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52374741066525, Training Error = 0.283203125
Iteration no. 422, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51812259967925, Training Error = 0.26171875
Iteration no. 423, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55603460689268, Training Error = 0.291015625
Iteration no. 424, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52011575637362, Training Error = 0.263671875
Iteration no. 425, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51540036383332, Training Error = 0.259765625
Iteration no. 426, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49242775512986, Training Error = 0.240234375
Iteration no. 427, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49566381403222, Training Error = 0.255859375
Iteration no. 428, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56125661257773, Training Error = 0.3046875
Iteration no. 429, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54101282951213, Training Error = 0.294921875
Iteration no. 430, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53771114066853, Training Error = 0.275390625
Iteration no. 431, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54426439133666, Training Error = 0.2734375
Iteration no. 432, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53319488423834, Training Error = 0.279296875
Iteration no. 433, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50415986606435, Training Error = 0.259765625
Iteration no. 434, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51598200071342, Training Error = 0.25
Iteration no. 435, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51099437781885, Training Error = 0.255859375
Iteration no. 436, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48756416266349, Training Error = 0.234375
Iteration no. 437, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55980025602919, Training Error = 0.3046875
Iteration no. 438, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52109654337761, Training Error = 0.259765625
Iteration no. 439, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52033913371857, Training Error = 0.2734375
Iteration no. 440, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50123900163946, Training Error = 0.2265625
Iteration no. 441, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55223052980861, Training Error = 0.294921875
Iteration no. 442, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.56350975855733, Training Error = 0.310546875
Iteration no. 443, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53585923864101, Training Error = 0.279296875
Iteration no. 444, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54509114123619, Training Error = 0.28125
Iteration no. 445, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.57204701358102, Training Error = 0.294921875
Iteration no. 446, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54747528601833, Training Error = 0.294921875
Iteration no. 447, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5203481717381, Training Error = 0.26953125
Iteration no. 448, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50667734440655, Training Error = 0.251953125
Iteration no. 449, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54159580228344, Training Error = 0.287109375
Iteration no. 450, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53474579555507, Training Error = 0.28125
Testing... average test_loss = 0.89829965704142, average test_pred_err = 0.483
Iteration no. 451, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50008367001619, Training Error = 0.24609375
Iteration no. 452, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53623845316013, Training Error = 0.255859375
Iteration no. 453, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.4993047212097, Training Error = 0.22265625
Iteration no. 454, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52705368780385, Training Error = 0.259765625
Iteration no. 455, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51482812389949, Training Error = 0.265625
Iteration no. 456, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.4988379950472, Training Error = 0.248046875
Iteration no. 457, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51590159041837, Training Error = 0.27734375
Iteration no. 458, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51152733537269, Training Error = 0.234375
Iteration no. 459, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5265144014814, Training Error = 0.27734375
Iteration no. 460, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50509016879488, Training Error = 0.244140625
Iteration no. 461, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49906510712008, Training Error = 0.2421875
Iteration no. 462, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52956688267316, Training Error = 0.259765625
Iteration no. 463, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.47771311322175, Training Error = 0.251953125
Iteration no. 464, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49942022889837, Training Error = 0.228515625
Iteration no. 465, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54608408381385, Training Error = 0.29296875
Iteration no. 466, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53753980397775, Training Error = 0.283203125
Iteration no. 467, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49197278502263, Training Error = 0.240234375
Iteration no. 468, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53183000575479, Training Error = 0.294921875
Iteration no. 469, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51616948741932, Training Error = 0.2734375
Iteration no. 470, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52858104166079, Training Error = 0.2578125
Iteration no. 471, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52434199345452, Training Error = 0.259765625
Iteration no. 472, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51811388836333, Training Error = 0.275390625
Iteration no. 473, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53592463250015, Training Error = 0.26171875
Iteration no. 474, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52597496249439, Training Error = 0.267578125
Iteration no. 475, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50971493666395, Training Error = 0.26171875
Iteration no. 476, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.54407813405217, Training Error = 0.2734375
Iteration no. 477, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49668239780168, Training Error = 0.263671875
Iteration no. 478, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53185697997816, Training Error = 0.28125
Iteration no. 479, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53490770186695, Training Error = 0.2734375
Iteration no. 480, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52874569109959, Training Error = 0.267578125
Iteration no. 481, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52779565216417, Training Error = 0.283203125
Iteration no. 482, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53335949908104, Training Error = 0.263671875
Iteration no. 483, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53657945714573, Training Error = 0.29296875
Iteration no. 484, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.5275711412226, Training Error = 0.26171875
Iteration no. 485, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50844687936696, Training Error = 0.255859375
Iteration no. 486, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49289345437871, Training Error = 0.248046875
Iteration no. 487, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48958962987789, Training Error = 0.21875
Iteration no. 488, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55658711789007, Training Error = 0.275390625
Iteration no. 489, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.55930588803528, Training Error = 0.2890625
Iteration no. 490, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50518781745676, Training Error = 0.267578125
Iteration no. 491, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51927024822839, Training Error = 0.25390625
Iteration no. 492, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.50832773257196, Training Error = 0.26171875
Iteration no. 493, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51493901029333, Training Error = 0.267578125
Iteration no. 494, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.48517443863674, Training Error = 0.236328125
Iteration no. 495, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51513123235796, Training Error = 0.271484375
Iteration no. 496, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52728556848401, Training Error = 0.25
Iteration no. 497, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.53555293147449, Training Error = 0.26171875
Iteration no. 498, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.52087186997242, Training Error = 0.271484375
Iteration no. 499, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.49522268526424, Training Error = 0.23828125
Iteration no. 500, lr = 0.4802, attribute lr =0.004802, average batch_loss = 0.51772673085505, Training Error = 0.2578125
Testing... average test_loss = 0.85602895731801, average test_pred_err = 0.474
Snapshotting C_model... done
Iteration no. 501, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50630763724052, Training Error = 0.24609375
Iteration no. 502, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53810452314095, Training Error = 0.263671875
Iteration no. 503, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48535589011036, Training Error = 0.232421875
Iteration no. 504, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51571413509611, Training Error = 0.265625
Iteration no. 505, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49796391999182, Training Error = 0.2578125
Iteration no. 506, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50812985748885, Training Error = 0.265625
Iteration no. 507, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51809027173409, Training Error = 0.263671875
Iteration no. 508, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50875228013837, Training Error = 0.25
Iteration no. 509, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52026828515383, Training Error = 0.28515625
Iteration no. 510, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50562471680668, Training Error = 0.259765625
Iteration no. 511, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54673983484016, Training Error = 0.28515625
Iteration no. 512, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54959229504619, Training Error = 0.2734375
Iteration no. 513, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55227490339225, Training Error = 0.275390625
Iteration no. 514, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52363996027191, Training Error = 0.26953125
Iteration no. 515, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.4826552966347, Training Error = 0.220703125
Iteration no. 516, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48015362932065, Training Error = 0.23046875
Iteration no. 517, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51459499118939, Training Error = 0.2734375
Iteration no. 518, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50025559420547, Training Error = 0.248046875
Iteration no. 519, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50896574496855, Training Error = 0.24609375
Iteration no. 520, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5201137874227, Training Error = 0.27734375
Iteration no. 521, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51907013149436, Training Error = 0.26171875
Iteration no. 522, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52192240357169, Training Error = 0.236328125
Iteration no. 523, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.47340654862765, Training Error = 0.2265625
Iteration no. 524, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50835568952078, Training Error = 0.279296875
Iteration no. 525, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49716502435879, Training Error = 0.228515625
Iteration no. 526, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54192292186221, Training Error = 0.2578125
Iteration no. 527, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55964607936938, Training Error = 0.291015625
Iteration no. 528, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48197430524074, Training Error = 0.224609375
Iteration no. 529, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54898033516166, Training Error = 0.30859375
Iteration no. 530, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48577569303186, Training Error = 0.25
Iteration no. 531, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51325001204542, Training Error = 0.251953125
Iteration no. 532, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49653579683787, Training Error = 0.240234375
Iteration no. 533, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51196085437027, Training Error = 0.26171875
Iteration no. 534, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5090139877064, Training Error = 0.25390625
Iteration no. 535, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51334377530757, Training Error = 0.2578125
Iteration no. 536, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48537457330672, Training Error = 0.234375
Iteration no. 537, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53085858503637, Training Error = 0.279296875
Iteration no. 538, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53101804830601, Training Error = 0.267578125
Iteration no. 539, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50814850021929, Training Error = 0.2578125
Iteration no. 540, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53720205607692, Training Error = 0.2734375
Iteration no. 541, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53863519590501, Training Error = 0.2734375
Iteration no. 542, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55721560526992, Training Error = 0.3046875
Iteration no. 543, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49519668967079, Training Error = 0.251953125
Iteration no. 544, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.47975885768505, Training Error = 0.228515625
Iteration no. 545, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50176664826572, Training Error = 0.251953125
Iteration no. 546, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53606275049859, Training Error = 0.265625
Iteration no. 547, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51701902225413, Training Error = 0.263671875
Iteration no. 548, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49409962890212, Training Error = 0.23046875
Iteration no. 549, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52561448260321, Training Error = 0.263671875
Iteration no. 550, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50498398843575, Training Error = 0.255859375
Testing... average test_loss = 0.91188062336522, average test_pred_err = 0.494
Iteration no. 551, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50516530041429, Training Error = 0.24609375
Iteration no. 552, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53377337279258, Training Error = 0.24609375
Iteration no. 553, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49539996220692, Training Error = 0.26171875
Iteration no. 554, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.48524011908194, Training Error = 0.2421875
Iteration no. 555, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52636871839615, Training Error = 0.251953125
Iteration no. 556, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52759239687861, Training Error = 0.255859375
Iteration no. 557, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52694440886446, Training Error = 0.2578125
Iteration no. 558, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5126692469544, Training Error = 0.265625
Iteration no. 559, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.538024303186, Training Error = 0.28125
Iteration no. 560, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52298819734955, Training Error = 0.291015625
Iteration no. 561, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51467582913471, Training Error = 0.279296875
Iteration no. 562, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51474957250982, Training Error = 0.2578125
Iteration no. 563, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50768972020581, Training Error = 0.275390625
Iteration no. 564, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49756402936089, Training Error = 0.2578125
Iteration no. 565, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51003893602949, Training Error = 0.265625
Iteration no. 566, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49366821226148, Training Error = 0.23828125
Iteration no. 567, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49429473240241, Training Error = 0.24609375
Iteration no. 568, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51706232853366, Training Error = 0.25
Iteration no. 569, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52274135115337, Training Error = 0.259765625
Iteration no. 570, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52507463866244, Training Error = 0.279296875
Iteration no. 571, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49383529528051, Training Error = 0.24609375
Iteration no. 572, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5307577827464, Training Error = 0.275390625
Iteration no. 573, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49860915182397, Training Error = 0.23828125
Iteration no. 574, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50852454600488, Training Error = 0.265625
Iteration no. 575, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53378161782802, Training Error = 0.283203125
Iteration no. 576, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54858861058044, Training Error = 0.291015625
Iteration no. 577, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49637020633372, Training Error = 0.2265625
Iteration no. 578, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49204860816339, Training Error = 0.22265625
Iteration no. 579, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49567445134129, Training Error = 0.265625
Iteration no. 580, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53990562516354, Training Error = 0.279296875
Iteration no. 581, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51037291884526, Training Error = 0.267578125
Iteration no. 582, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51452405401466, Training Error = 0.26171875
Iteration no. 583, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51603849713526, Training Error = 0.27734375
Iteration no. 584, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50225139047586, Training Error = 0.267578125
Iteration no. 585, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.5012310936371, Training Error = 0.248046875
Iteration no. 586, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49278332102008, Training Error = 0.240234375
Iteration no. 587, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49656077785435, Training Error = 0.244140625
Iteration no. 588, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.54392885233889, Training Error = 0.287109375
Iteration no. 589, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.55777064936624, Training Error = 0.287109375
Iteration no. 590, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51423253454941, Training Error = 0.25
Iteration no. 591, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52497190220638, Training Error = 0.271484375
Iteration no. 592, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49233053739595, Training Error = 0.234375
Iteration no. 593, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53590493904142, Training Error = 0.28515625
Iteration no. 594, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.50614897256364, Training Error = 0.23046875
Iteration no. 595, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49457057090364, Training Error = 0.24609375
Iteration no. 596, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51803994295124, Training Error = 0.26171875
Iteration no. 597, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.53842220414857, Training Error = 0.283203125
Iteration no. 598, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.51899614196282, Training Error = 0.259765625
Iteration no. 599, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.52175475659663, Training Error = 0.275390625
Iteration no. 600, lr = 0.33614, attribute lr =0.0033614, average batch_loss = 0.49806753222244, Training Error = 0.25
Testing... average test_loss = 0.8602294983306, average test_pred_err = 0.445
Snapshotting C_model... done
Iteration no. 601, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54369701520627, Training Error = 0.255859375
Iteration no. 602, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49048733263089, Training Error = 0.234375
Iteration no. 603, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49204038277669, Training Error = 0.263671875
Iteration no. 604, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48912779479461, Training Error = 0.236328125
Iteration no. 605, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.56167484631014, Training Error = 0.306640625
Iteration no. 606, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49461877139241, Training Error = 0.2421875
Iteration no. 607, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50267580699353, Training Error = 0.2734375
Iteration no. 608, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52298044391792, Training Error = 0.275390625
Iteration no. 609, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51587146906651, Training Error = 0.27734375
Iteration no. 610, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52390107493865, Training Error = 0.271484375
Iteration no. 611, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54463197437512, Training Error = 0.30078125
Iteration no. 612, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51421283038823, Training Error = 0.248046875
Iteration no. 613, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47217378199824, Training Error = 0.224609375
Iteration no. 614, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49173455944996, Training Error = 0.25390625
Iteration no. 615, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53303715638147, Training Error = 0.263671875
Iteration no. 616, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50863466811677, Training Error = 0.248046875
Iteration no. 617, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53429534768709, Training Error = 0.287109375
Iteration no. 618, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51808891921966, Training Error = 0.26171875
Iteration no. 619, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.4918486908994, Training Error = 0.2578125
Iteration no. 620, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49295045567991, Training Error = 0.26171875
Iteration no. 621, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53022229890852, Training Error = 0.279296875
Iteration no. 622, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5051735590642, Training Error = 0.275390625
Iteration no. 623, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48883629401229, Training Error = 0.224609375
Iteration no. 624, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51618696900227, Training Error = 0.2421875
Iteration no. 625, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51669252742725, Training Error = 0.271484375
Iteration no. 626, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49902927410229, Training Error = 0.236328125
Iteration no. 627, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52038752886932, Training Error = 0.248046875
Iteration no. 628, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50210994610712, Training Error = 0.2421875
Iteration no. 629, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49301364829196, Training Error = 0.263671875
Iteration no. 630, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50494805996923, Training Error = 0.2421875
Iteration no. 631, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5200959196507, Training Error = 0.2734375
Iteration no. 632, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52250645517827, Training Error = 0.2734375
Iteration no. 633, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54536966806834, Training Error = 0.279296875
Iteration no. 634, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52150213007666, Training Error = 0.28125
Iteration no. 635, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49154361122389, Training Error = 0.2421875
Iteration no. 636, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50395809201316, Training Error = 0.240234375
Iteration no. 637, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52220797119367, Training Error = 0.263671875
Iteration no. 638, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52655296542084, Training Error = 0.265625
Iteration no. 639, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52386228429056, Training Error = 0.271484375
Iteration no. 640, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5181742000286, Training Error = 0.26171875
Iteration no. 641, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50019135356597, Training Error = 0.25390625
Iteration no. 642, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.4607194162887, Training Error = 0.21875
Iteration no. 643, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50462079958904, Training Error = 0.244140625
Iteration no. 644, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48651195249887, Training Error = 0.24609375
Iteration no. 645, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53316823651804, Training Error = 0.271484375
Iteration no. 646, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48720227095187, Training Error = 0.248046875
Iteration no. 647, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.46171829001422, Training Error = 0.232421875
Iteration no. 648, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49678922228908, Training Error = 0.2421875
Iteration no. 649, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49151610927661, Training Error = 0.234375
Iteration no. 650, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47755463727164, Training Error = 0.224609375
Testing... average test_loss = 0.87665404353958, average test_pred_err = 0.451
Iteration no. 651, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50008196008761, Training Error = 0.259765625
Iteration no. 652, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50798059678891, Training Error = 0.259765625
Iteration no. 653, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48911095636436, Training Error = 0.25390625
Iteration no. 654, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52261338207637, Training Error = 0.25
Iteration no. 655, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49969431380492, Training Error = 0.24609375
Iteration no. 656, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51379686064275, Training Error = 0.263671875
Iteration no. 657, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52948271253965, Training Error = 0.2734375
Iteration no. 658, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50109814664221, Training Error = 0.255859375
Iteration no. 659, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5102099943349, Training Error = 0.26953125
Iteration no. 660, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52219091737375, Training Error = 0.28125
Iteration no. 661, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50891645224873, Training Error = 0.240234375
Iteration no. 662, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48766316274863, Training Error = 0.23828125
Iteration no. 663, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51968708987573, Training Error = 0.2734375
Iteration no. 664, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.4962623961868, Training Error = 0.234375
Iteration no. 665, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51848104178162, Training Error = 0.275390625
Iteration no. 666, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50244999861426, Training Error = 0.248046875
Iteration no. 667, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53953946660906, Training Error = 0.279296875
Iteration no. 668, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49595507595151, Training Error = 0.25390625
Iteration no. 669, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49664992653611, Training Error = 0.248046875
Iteration no. 670, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48884925959388, Training Error = 0.2421875
Iteration no. 671, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49562678225061, Training Error = 0.23828125
Iteration no. 672, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48946112393162, Training Error = 0.234375
Iteration no. 673, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47925874206438, Training Error = 0.220703125
Iteration no. 674, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50542511908231, Training Error = 0.271484375
Iteration no. 675, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51871629292976, Training Error = 0.244140625
Iteration no. 676, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54410717339037, Training Error = 0.28515625
Iteration no. 677, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51534196281664, Training Error = 0.248046875
Iteration no. 678, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49592227597551, Training Error = 0.2578125
Iteration no. 679, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49984518101052, Training Error = 0.265625
Iteration no. 680, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.48531098516964, Training Error = 0.259765625
Iteration no. 681, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52161594149873, Training Error = 0.271484375
Iteration no. 682, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49382315911419, Training Error = 0.228515625
Iteration no. 683, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53804162971315, Training Error = 0.27734375
Iteration no. 684, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.54467751359696, Training Error = 0.283203125
Iteration no. 685, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53146914603968, Training Error = 0.279296875
Iteration no. 686, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49746648103995, Training Error = 0.234375
Iteration no. 687, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50643103851145, Training Error = 0.232421875
Iteration no. 688, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52087368869524, Training Error = 0.26953125
Iteration no. 689, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49962182401108, Training Error = 0.23046875
Iteration no. 690, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.5038799499389, Training Error = 0.255859375
Iteration no. 691, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49890983950168, Training Error = 0.25
Iteration no. 692, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52190715093748, Training Error = 0.244140625
Iteration no. 693, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49473274785316, Training Error = 0.251953125
Iteration no. 694, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.53872676027152, Training Error = 0.291015625
Iteration no. 695, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50423696947559, Training Error = 0.25390625
Iteration no. 696, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.51101198193454, Training Error = 0.24609375
Iteration no. 697, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.47400100421957, Training Error = 0.228515625
Iteration no. 698, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.50516923394828, Training Error = 0.259765625
Iteration no. 699, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.52068695782794, Training Error = 0.26171875
Iteration no. 700, lr = 0.235298, attribute lr =0.00235298, average batch_loss = 0.49643546799862, Training Error = 0.25
Testing... average test_loss = 0.89229713558973, average test_pred_err = 0.448
Snapshotting C_model... done
Iteration no. 701, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50846255584293, Training Error = 0.244140625
Iteration no. 702, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50294187672966, Training Error = 0.267578125
Iteration no. 703, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51338872176419, Training Error = 0.265625
Iteration no. 704, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47080355370651, Training Error = 0.22265625
Iteration no. 705, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.500966002809, Training Error = 0.2578125
Iteration no. 706, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52557787040636, Training Error = 0.26171875
Iteration no. 707, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53026494431335, Training Error = 0.279296875
Iteration no. 708, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.54486731047903, Training Error = 0.2734375
Iteration no. 709, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50008462280234, Training Error = 0.23828125
Iteration no. 710, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50160920225009, Training Error = 0.26171875
Iteration no. 711, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49755972859106, Training Error = 0.263671875
Iteration no. 712, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53910276819801, Training Error = 0.26953125
Iteration no. 713, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48998193845002, Training Error = 0.251953125
Iteration no. 714, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51295148492793, Training Error = 0.26171875
Iteration no. 715, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49436332033717, Training Error = 0.23046875
Iteration no. 716, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51743608134532, Training Error = 0.26953125
Iteration no. 717, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4856973154676, Training Error = 0.244140625
Iteration no. 718, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52101601297372, Training Error = 0.25390625
Iteration no. 719, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50388434319502, Training Error = 0.2421875
Iteration no. 720, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51493051971481, Training Error = 0.24609375
Iteration no. 721, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50304377521324, Training Error = 0.267578125
Iteration no. 722, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48326369578974, Training Error = 0.23828125
Iteration no. 723, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48667443836084, Training Error = 0.24609375
Iteration no. 724, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.46613521866913, Training Error = 0.2265625
Iteration no. 725, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52922253565689, Training Error = 0.279296875
Iteration no. 726, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.46769458175355, Training Error = 0.21875
Iteration no. 727, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4852258361635, Training Error = 0.23828125
Iteration no. 728, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48900908228836, Training Error = 0.240234375
Iteration no. 729, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50353440782385, Training Error = 0.255859375
Iteration no. 730, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51201369779776, Training Error = 0.244140625
Iteration no. 731, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50407668361553, Training Error = 0.2578125
Iteration no. 732, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50104411360701, Training Error = 0.259765625
Iteration no. 733, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51653124727202, Training Error = 0.251953125
Iteration no. 734, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.55699239352636, Training Error = 0.296875
Iteration no. 735, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48868584883156, Training Error = 0.23046875
Iteration no. 736, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49382012522083, Training Error = 0.236328125
Iteration no. 737, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52676046938826, Training Error = 0.263671875
Iteration no. 738, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51110938531225, Training Error = 0.263671875
Iteration no. 739, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48542285736295, Training Error = 0.255859375
Iteration no. 740, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49802195192857, Training Error = 0.2421875
Iteration no. 741, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48384506933145, Training Error = 0.236328125
Iteration no. 742, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49894543585586, Training Error = 0.236328125
Iteration no. 743, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50464256336683, Training Error = 0.25390625
Iteration no. 744, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.45206561668873, Training Error = 0.208984375
Iteration no. 745, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49613262315897, Training Error = 0.25390625
Iteration no. 746, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.54675160153512, Training Error = 0.28125
Iteration no. 747, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.45945239176787, Training Error = 0.20703125
Iteration no. 748, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48362877416866, Training Error = 0.259765625
Iteration no. 749, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47668719224092, Training Error = 0.251953125
Iteration no. 750, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51600266155721, Training Error = 0.248046875
Testing... average test_loss = 0.92018127409741, average test_pred_err = 0.456
Iteration no. 751, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51131839200627, Training Error = 0.259765625
Iteration no. 752, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51764440231792, Training Error = 0.27734375
Iteration no. 753, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53113898489798, Training Error = 0.283203125
Iteration no. 754, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.5081940576024, Training Error = 0.2578125
Iteration no. 755, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52480633430746, Training Error = 0.271484375
Iteration no. 756, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53359143142905, Training Error = 0.310546875
Iteration no. 757, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52296358666656, Training Error = 0.267578125
Iteration no. 758, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51413958034727, Training Error = 0.283203125
Iteration no. 759, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53489663490033, Training Error = 0.2734375
Iteration no. 760, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53434430724442, Training Error = 0.275390625
Iteration no. 761, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.5192138493858, Training Error = 0.2578125
Iteration no. 762, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.55471300571686, Training Error = 0.27734375
Iteration no. 763, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52175834111962, Training Error = 0.25390625
Iteration no. 764, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52515026334181, Training Error = 0.275390625
Iteration no. 765, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51806271633904, Training Error = 0.271484375
Iteration no. 766, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49840739424573, Training Error = 0.25390625
Iteration no. 767, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.5268752004506, Training Error = 0.2578125
Iteration no. 768, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51941041231681, Training Error = 0.26953125
Iteration no. 769, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50021921768922, Training Error = 0.25390625
Iteration no. 770, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4905992775102, Training Error = 0.248046875
Iteration no. 771, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51864846086796, Training Error = 0.26953125
Iteration no. 772, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50883922413892, Training Error = 0.25390625
Iteration no. 773, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49093181936965, Training Error = 0.25390625
Iteration no. 774, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50736141897333, Training Error = 0.2578125
Iteration no. 775, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50315142202934, Training Error = 0.25
Iteration no. 776, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51185272035589, Training Error = 0.287109375
Iteration no. 777, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47787964848209, Training Error = 0.234375
Iteration no. 778, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51046367427903, Training Error = 0.27734375
Iteration no. 779, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.46334163692868, Training Error = 0.234375
Iteration no. 780, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51900346251395, Training Error = 0.279296875
Iteration no. 781, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49260176134147, Training Error = 0.2421875
Iteration no. 782, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51875180628876, Training Error = 0.24609375
Iteration no. 783, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51814665711445, Training Error = 0.279296875
Iteration no. 784, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51805805041463, Training Error = 0.25390625
Iteration no. 785, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50618518182181, Training Error = 0.251953125
Iteration no. 786, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49491796156397, Training Error = 0.2421875
Iteration no. 787, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50117897240064, Training Error = 0.251953125
Iteration no. 788, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52901826662962, Training Error = 0.267578125
Iteration no. 789, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53380559239016, Training Error = 0.2734375
Iteration no. 790, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.53089921826256, Training Error = 0.279296875
Iteration no. 791, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47409948403792, Training Error = 0.240234375
Iteration no. 792, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49086847243742, Training Error = 0.251953125
Iteration no. 793, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.50426884715161, Training Error = 0.24609375
Iteration no. 794, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.47498912118568, Training Error = 0.232421875
Iteration no. 795, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48592645439359, Training Error = 0.240234375
Iteration no. 796, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.52501919724599, Training Error = 0.283203125
Iteration no. 797, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.49254338359768, Training Error = 0.244140625
Iteration no. 798, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.51790058494296, Training Error = 0.248046875
Iteration no. 799, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.4921577610491, Training Error = 0.23828125
Iteration no. 800, lr = 0.1647086, attribute lr =0.001647086, average batch_loss = 0.48293845936184, Training Error = 0.240234375
Testing... average test_loss = 0.91701670100134, average test_pred_err = 0.462
Snapshotting C_model... done
Iteration no. 801, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48493077752156, Training Error = 0.248046875
Iteration no. 802, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51304112115064, Training Error = 0.251953125
Iteration no. 803, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51583499411243, Training Error = 0.2734375
Iteration no. 804, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53113953617504, Training Error = 0.28125
Iteration no. 805, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48977474263768, Training Error = 0.240234375
Iteration no. 806, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52038963884103, Training Error = 0.2734375
Iteration no. 807, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53362398776675, Training Error = 0.2578125
Iteration no. 808, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.54265381037704, Training Error = 0.28515625
Iteration no. 809, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52980892071795, Training Error = 0.267578125
Iteration no. 810, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50446750321673, Training Error = 0.2578125
Iteration no. 811, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51628358792373, Training Error = 0.255859375
Iteration no. 812, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51389240769483, Training Error = 0.25390625
Iteration no. 813, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49447829489476, Training Error = 0.25
Iteration no. 814, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52952976598157, Training Error = 0.275390625
Iteration no. 815, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4874950823217, Training Error = 0.20703125
Iteration no. 816, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4695293602337, Training Error = 0.220703125
Iteration no. 817, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49072848607561, Training Error = 0.232421875
Iteration no. 818, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52992796213481, Training Error = 0.296875
Iteration no. 819, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5451750681446, Training Error = 0.275390625
Iteration no. 820, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50546704138115, Training Error = 0.24609375
Iteration no. 821, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52921563506978, Training Error = 0.279296875
Iteration no. 822, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51364028982586, Training Error = 0.25
Iteration no. 823, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52308968014251, Training Error = 0.283203125
Iteration no. 824, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51484888628012, Training Error = 0.265625
Iteration no. 825, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49197783547129, Training Error = 0.224609375
Iteration no. 826, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52258872807301, Training Error = 0.267578125
Iteration no. 827, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4762256256513, Training Error = 0.205078125
Iteration no. 828, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46623861078322, Training Error = 0.2265625
Iteration no. 829, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51004859951157, Training Error = 0.275390625
Iteration no. 830, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51736481934497, Training Error = 0.26171875
Iteration no. 831, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46768594112694, Training Error = 0.220703125
Iteration no. 832, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5091413373055, Training Error = 0.2578125
Iteration no. 833, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48237818254912, Training Error = 0.23828125
Iteration no. 834, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52640402906143, Training Error = 0.25
Iteration no. 835, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50265630939911, Training Error = 0.265625
Iteration no. 836, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49579352944567, Training Error = 0.25
Iteration no. 837, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49607922764955, Training Error = 0.251953125
Iteration no. 838, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5106518260663, Training Error = 0.251953125
Iteration no. 839, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51798493908794, Training Error = 0.26953125
Iteration no. 840, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4918382361651, Training Error = 0.234375
Iteration no. 841, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.55035359933998, Training Error = 0.2734375
Iteration no. 842, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53173282634139, Training Error = 0.2578125
Iteration no. 843, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.54371694775121, Training Error = 0.3046875
Iteration no. 844, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52899603202947, Training Error = 0.27734375
Iteration no. 845, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50808201631632, Training Error = 0.25
Iteration no. 846, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52043563960659, Training Error = 0.2734375
Iteration no. 847, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4905882628885, Training Error = 0.224609375
Iteration no. 848, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5203912052344, Training Error = 0.265625
Iteration no. 849, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47564132693, Training Error = 0.25
Iteration no. 850, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52475993816357, Training Error = 0.26953125
Testing... average test_loss = 0.9440602351324, average test_pred_err = 0.483
Iteration no. 851, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49766739317845, Training Error = 0.2421875
Iteration no. 852, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50128010592782, Training Error = 0.265625
Iteration no. 853, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46523476619941, Training Error = 0.21875
Iteration no. 854, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49884996285513, Training Error = 0.25
Iteration no. 855, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51965737864319, Training Error = 0.26171875
Iteration no. 856, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53461227816336, Training Error = 0.2890625
Iteration no. 857, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53288086459182, Training Error = 0.2734375
Iteration no. 858, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51694331625428, Training Error = 0.2578125
Iteration no. 859, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47910575207402, Training Error = 0.224609375
Iteration no. 860, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4816596050343, Training Error = 0.228515625
Iteration no. 861, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48618178229985, Training Error = 0.228515625
Iteration no. 862, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52191026027188, Training Error = 0.2578125
Iteration no. 863, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53007273397861, Training Error = 0.265625
Iteration no. 864, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51653254538383, Training Error = 0.248046875
Iteration no. 865, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47460713892953, Training Error = 0.234375
Iteration no. 866, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48292216181327, Training Error = 0.24609375
Iteration no. 867, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49915790888645, Training Error = 0.244140625
Iteration no. 868, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47903505373591, Training Error = 0.22265625
Iteration no. 869, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50901851836856, Training Error = 0.27734375
Iteration no. 870, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52138479805189, Training Error = 0.2578125
Iteration no. 871, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47283451753732, Training Error = 0.244140625
Iteration no. 872, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47975907732464, Training Error = 0.234375
Iteration no. 873, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5238077767868, Training Error = 0.2734375
Iteration no. 874, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50459121575834, Training Error = 0.244140625
Iteration no. 875, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49741944363054, Training Error = 0.240234375
Iteration no. 876, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5101291249786, Training Error = 0.234375
Iteration no. 877, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.53143720017342, Training Error = 0.255859375
Iteration no. 878, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51047993954124, Training Error = 0.2734375
Iteration no. 879, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49381483437501, Training Error = 0.255859375
Iteration no. 880, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5028888247579, Training Error = 0.240234375
Iteration no. 881, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.4730315932681, Training Error = 0.248046875
Iteration no. 882, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51195010546896, Training Error = 0.2578125
Iteration no. 883, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49021995293677, Training Error = 0.244140625
Iteration no. 884, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50145617501848, Training Error = 0.2578125
Iteration no. 885, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.49977935096001, Training Error = 0.26171875
Iteration no. 886, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48767371213449, Training Error = 0.2421875
Iteration no. 887, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.46808030867021, Training Error = 0.23046875
Iteration no. 888, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47151367014403, Training Error = 0.21875
Iteration no. 889, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50772043257004, Training Error = 0.255859375
Iteration no. 890, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.5126097194092, Training Error = 0.255859375
Iteration no. 891, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47253176392913, Training Error = 0.2109375
Iteration no. 892, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51795669386853, Training Error = 0.2578125
Iteration no. 893, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.52956245112024, Training Error = 0.27734375
Iteration no. 894, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51300811713152, Training Error = 0.25
Iteration no. 895, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50552023366791, Training Error = 0.25390625
Iteration no. 896, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.47212029904265, Training Error = 0.21875
Iteration no. 897, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.51571913321537, Training Error = 0.248046875
Iteration no. 898, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50900804605875, Training Error = 0.2734375
Iteration no. 899, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.48261156073771, Training Error = 0.23828125
Iteration no. 900, lr = 0.11529602, attribute lr =0.0011529602, average batch_loss = 0.50863263235663, Training Error = 0.255859375
Testing... average test_loss = 0.92267680089698, average test_pred_err = 0.438
Snapshotting C_model... done
Iteration no. 901, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54116411204936, Training Error = 0.291015625
Iteration no. 902, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51049780336617, Training Error = 0.263671875
Iteration no. 903, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47446761513274, Training Error = 0.240234375
Iteration no. 904, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4806065467095, Training Error = 0.2265625
Iteration no. 905, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50109708252123, Training Error = 0.26953125
Iteration no. 906, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53248901797007, Training Error = 0.283203125
Iteration no. 907, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49152363197199, Training Error = 0.22265625
Iteration no. 908, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48964755594989, Training Error = 0.255859375
Iteration no. 909, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54530792984422, Training Error = 0.28515625
Iteration no. 910, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47727766457989, Training Error = 0.23046875
Iteration no. 911, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50439134050857, Training Error = 0.2578125
Iteration no. 912, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50003614772561, Training Error = 0.259765625
Iteration no. 913, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51727023623405, Training Error = 0.259765625
Iteration no. 914, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51837138581919, Training Error = 0.2734375
Iteration no. 915, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53930374117871, Training Error = 0.265625
Iteration no. 916, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.46876645144741, Training Error = 0.23046875
Iteration no. 917, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49855552840132, Training Error = 0.267578125
Iteration no. 918, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48505123378969, Training Error = 0.251953125
Iteration no. 919, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5011874174971, Training Error = 0.26171875
Iteration no. 920, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51428515454052, Training Error = 0.26953125
Iteration no. 921, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49958124866684, Training Error = 0.251953125
Iteration no. 922, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53260221519327, Training Error = 0.283203125
Iteration no. 923, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49218734876624, Training Error = 0.248046875
Iteration no. 924, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49322161026614, Training Error = 0.24609375
Iteration no. 925, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5177924427434, Training Error = 0.259765625
Iteration no. 926, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54953812401194, Training Error = 0.287109375
Iteration no. 927, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50771894982061, Training Error = 0.259765625
Iteration no. 928, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47790434916371, Training Error = 0.240234375
Iteration no. 929, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54732125096969, Training Error = 0.28125
Iteration no. 930, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4992789427532, Training Error = 0.26171875
Iteration no. 931, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49174762954971, Training Error = 0.2578125
Iteration no. 932, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51739212940946, Training Error = 0.26171875
Iteration no. 933, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51665116831017, Training Error = 0.259765625
Iteration no. 934, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50622253907823, Training Error = 0.25
Iteration no. 935, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5263432485706, Training Error = 0.26171875
Iteration no. 936, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51835765924698, Training Error = 0.26171875
Iteration no. 937, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.45855219812562, Training Error = 0.208984375
Iteration no. 938, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5185016265249, Training Error = 0.240234375
Iteration no. 939, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51566590252731, Training Error = 0.265625
Iteration no. 940, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49982423619554, Training Error = 0.2578125
Iteration no. 941, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.46471703922308, Training Error = 0.2109375
Iteration no. 942, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51306018149782, Training Error = 0.26171875
Iteration no. 943, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50909081517999, Training Error = 0.255859375
Iteration no. 944, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52690641593004, Training Error = 0.265625
Iteration no. 945, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53405163526727, Training Error = 0.28125
Iteration no. 946, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49079823755984, Training Error = 0.265625
Iteration no. 947, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50715540941229, Training Error = 0.25
Iteration no. 948, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4936444167927, Training Error = 0.2265625
Iteration no. 949, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48492191692137, Training Error = 0.25
Iteration no. 950, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47445184910323, Training Error = 0.251953125
Testing... average test_loss = 0.96785263959461, average test_pred_err = 0.495
Iteration no. 951, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51695175432798, Training Error = 0.244140625
Iteration no. 952, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52362441716383, Training Error = 0.27734375
Iteration no. 953, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53505655346885, Training Error = 0.283203125
Iteration no. 954, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50622486135108, Training Error = 0.25
Iteration no. 955, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51330357620326, Training Error = 0.2265625
Iteration no. 956, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52187351776885, Training Error = 0.265625
Iteration no. 957, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50828380913092, Training Error = 0.2734375
Iteration no. 958, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51810665924826, Training Error = 0.275390625
Iteration no. 959, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47520958049208, Training Error = 0.2265625
Iteration no. 960, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48913108977353, Training Error = 0.2578125
Iteration no. 961, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49458917232163, Training Error = 0.234375
Iteration no. 962, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47968449964639, Training Error = 0.236328125
Iteration no. 963, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52299211548691, Training Error = 0.279296875
Iteration no. 964, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.52452294798458, Training Error = 0.25
Iteration no. 965, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50751645022689, Training Error = 0.25
Iteration no. 966, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51618097119512, Training Error = 0.255859375
Iteration no. 967, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50984310015289, Training Error = 0.271484375
Iteration no. 968, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48954007688285, Training Error = 0.240234375
Iteration no. 969, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5149169706309, Training Error = 0.23828125
Iteration no. 970, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47780637851722, Training Error = 0.234375
Iteration no. 971, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50875208032631, Training Error = 0.275390625
Iteration no. 972, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49940663261979, Training Error = 0.23828125
Iteration no. 973, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51916528090776, Training Error = 0.2578125
Iteration no. 974, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5056033818659, Training Error = 0.265625
Iteration no. 975, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49750301224827, Training Error = 0.25390625
Iteration no. 976, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48391631722885, Training Error = 0.236328125
Iteration no. 977, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50233040409739, Training Error = 0.24609375
Iteration no. 978, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51214512543951, Training Error = 0.27734375
Iteration no. 979, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50481948091825, Training Error = 0.255859375
Iteration no. 980, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.53998200173649, Training Error = 0.27734375
Iteration no. 981, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49738150018118, Training Error = 0.2578125
Iteration no. 982, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.4930979157053, Training Error = 0.2578125
Iteration no. 983, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.5307333992034, Training Error = 0.275390625
Iteration no. 984, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.46607483076226, Training Error = 0.236328125
Iteration no. 985, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49593308062223, Training Error = 0.263671875
Iteration no. 986, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49286448006854, Training Error = 0.2265625
Iteration no. 987, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48652163478306, Training Error = 0.232421875
Iteration no. 988, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49117315403254, Training Error = 0.2421875
Iteration no. 989, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47491708457767, Training Error = 0.2265625
Iteration no. 990, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.54513620865136, Training Error = 0.291015625
Iteration no. 991, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.47266954139684, Training Error = 0.23828125
Iteration no. 992, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51628848662501, Training Error = 0.263671875
Iteration no. 993, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.49204480943138, Training Error = 0.248046875
Iteration no. 994, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50085613163836, Training Error = 0.236328125
Iteration no. 995, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51792155815298, Training Error = 0.26953125
Iteration no. 996, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.50417078842241, Training Error = 0.23828125
Iteration no. 997, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48753354216798, Training Error = 0.23828125
Iteration no. 998, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.51546576438154, Training Error = 0.27734375
Iteration no. 999, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.48463514027865, Training Error = 0.244140625
Iteration no. 1000, lr = 0.080707214, attribute lr =0.00080707214, average batch_loss = 0.45610181848215, Training Error = 0.2109375
Testing... average test_loss = 0.92005245455858, average test_pred_err = 0.45
Snapshotting C_model... done
Iteration no. 1001, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52276137271715, Training Error = 0.28125
Iteration no. 1002, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50166829521614, Training Error = 0.28125
Iteration no. 1003, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.53423042933796, Training Error = 0.2578125
Iteration no. 1004, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48765396875483, Training Error = 0.2265625
Iteration no. 1005, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52237307346627, Training Error = 0.265625
Iteration no. 1006, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49397605499816, Training Error = 0.24609375
Iteration no. 1007, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51640473354979, Training Error = 0.26953125
Iteration no. 1008, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47774721729428, Training Error = 0.228515625
Iteration no. 1009, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49977941814046, Training Error = 0.25
Iteration no. 1010, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46981899648039, Training Error = 0.236328125
Iteration no. 1011, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51195740039882, Training Error = 0.255859375
Iteration no. 1012, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51188580831371, Training Error = 0.259765625
Iteration no. 1013, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46058659460352, Training Error = 0.22265625
Iteration no. 1014, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.55298314035917, Training Error = 0.27734375
Iteration no. 1015, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46362107458413, Training Error = 0.224609375
Iteration no. 1016, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50828673254133, Training Error = 0.267578125
Iteration no. 1017, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51400418439648, Training Error = 0.248046875
Iteration no. 1018, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49628879216829, Training Error = 0.263671875
Iteration no. 1019, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52337393250563, Training Error = 0.279296875
Iteration no. 1020, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52415940509052, Training Error = 0.26171875
Iteration no. 1021, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51206651554103, Training Error = 0.2578125
Iteration no. 1022, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47592746020764, Training Error = 0.236328125
Iteration no. 1023, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47600655991602, Training Error = 0.224609375
Iteration no. 1024, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.54174763230915, Training Error = 0.275390625
Iteration no. 1025, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52588858573926, Training Error = 0.26171875
Iteration no. 1026, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51869548893431, Training Error = 0.259765625
Iteration no. 1027, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51853377598746, Training Error = 0.244140625
Iteration no. 1028, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49996111094315, Training Error = 0.24609375
Iteration no. 1029, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4674121454726, Training Error = 0.228515625
Iteration no. 1030, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51118829322443, Training Error = 0.244140625
Iteration no. 1031, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47464871697007, Training Error = 0.240234375
Iteration no. 1032, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51914363558823, Training Error = 0.2734375
Iteration no. 1033, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47462789354834, Training Error = 0.216796875
Iteration no. 1034, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51863563278498, Training Error = 0.263671875
Iteration no. 1035, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5009994427807, Training Error = 0.248046875
Iteration no. 1036, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49158828873099, Training Error = 0.244140625
Iteration no. 1037, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51196664607699, Training Error = 0.248046875
Iteration no. 1038, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50396578630635, Training Error = 0.2578125
Iteration no. 1039, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48239824162373, Training Error = 0.255859375
Iteration no. 1040, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.53427009572095, Training Error = 0.294921875
Iteration no. 1041, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51425410081255, Training Error = 0.2734375
Iteration no. 1042, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52631771779654, Training Error = 0.291015625
Iteration no. 1043, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49938166063971, Training Error = 0.24609375
Iteration no. 1044, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5134549013102, Training Error = 0.248046875
Iteration no. 1045, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50988845170316, Training Error = 0.2578125
Iteration no. 1046, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52314010022455, Training Error = 0.265625
Iteration no. 1047, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50152959212323, Training Error = 0.248046875
Iteration no. 1048, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51719589539339, Training Error = 0.2578125
Iteration no. 1049, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50102918251301, Training Error = 0.232421875
Iteration no. 1050, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.54685698532454, Training Error = 0.3046875
Testing... average test_loss = 0.947501360425, average test_pred_err = 0.468
Iteration no. 1051, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51731588011861, Training Error = 0.283203125
Iteration no. 1052, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50818701356259, Training Error = 0.263671875
Iteration no. 1053, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49259265155335, Training Error = 0.24609375
Iteration no. 1054, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48812384127494, Training Error = 0.23828125
Iteration no. 1055, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5166797552808, Training Error = 0.26171875
Iteration no. 1056, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49723820998302, Training Error = 0.240234375
Iteration no. 1057, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48984935429127, Training Error = 0.244140625
Iteration no. 1058, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47996392466614, Training Error = 0.224609375
Iteration no. 1059, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50209860688468, Training Error = 0.263671875
Iteration no. 1060, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49911236645007, Training Error = 0.24609375
Iteration no. 1061, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49922765510444, Training Error = 0.236328125
Iteration no. 1062, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4826328572252, Training Error = 0.248046875
Iteration no. 1063, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50316230429426, Training Error = 0.23828125
Iteration no. 1064, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48118692219632, Training Error = 0.23046875
Iteration no. 1065, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49403187168687, Training Error = 0.248046875
Iteration no. 1066, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5030543672141, Training Error = 0.25390625
Iteration no. 1067, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50784729501506, Training Error = 0.25390625
Iteration no. 1068, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50516449980517, Training Error = 0.236328125
Iteration no. 1069, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50297956823792, Training Error = 0.267578125
Iteration no. 1070, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51310512884669, Training Error = 0.2734375
Iteration no. 1071, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.50705311507459, Training Error = 0.263671875
Iteration no. 1072, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48004616091415, Training Error = 0.2265625
Iteration no. 1073, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51221184659796, Training Error = 0.25390625
Iteration no. 1074, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4734301397447, Training Error = 0.216796875
Iteration no. 1075, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51166548245868, Training Error = 0.259765625
Iteration no. 1076, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48362151036351, Training Error = 0.255859375
Iteration no. 1077, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51433286596234, Training Error = 0.26953125
Iteration no. 1078, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.52939820010242, Training Error = 0.2578125
Iteration no. 1079, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46240253793963, Training Error = 0.212890625
Iteration no. 1080, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48577880635671, Training Error = 0.232421875
Iteration no. 1081, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49667391618815, Training Error = 0.26171875
Iteration no. 1082, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51992445207983, Training Error = 0.263671875
Iteration no. 1083, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49001477522628, Training Error = 0.232421875
Iteration no. 1084, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48264805930371, Training Error = 0.23828125
Iteration no. 1085, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51615039129057, Training Error = 0.271484375
Iteration no. 1086, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51454444216023, Training Error = 0.27734375
Iteration no. 1087, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48158347186578, Training Error = 0.2265625
Iteration no. 1088, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48090522711188, Training Error = 0.23046875
Iteration no. 1089, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.54213432601799, Training Error = 0.298828125
Iteration no. 1090, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51800207733852, Training Error = 0.255859375
Iteration no. 1091, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.51971757118808, Training Error = 0.240234375
Iteration no. 1092, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5390021290867, Training Error = 0.306640625
Iteration no. 1093, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.49399202000341, Training Error = 0.234375
Iteration no. 1094, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5014847234888, Training Error = 0.234375
Iteration no. 1095, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.53229204424822, Training Error = 0.2578125
Iteration no. 1096, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.48339685974883, Training Error = 0.236328125
Iteration no. 1097, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.47928756883293, Training Error = 0.244140625
Iteration no. 1098, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.46819491372915, Training Error = 0.21875
Iteration no. 1099, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.5419756899365, Training Error = 0.2734375
Iteration no. 1100, lr = 0.0564950498, attribute lr =0.000564950498, average batch_loss = 0.4856839104002, Training Error = 0.23046875
Testing... average test_loss = 0.98660208325224, average test_pred_err = 0.49
Snapshotting C_model... done
Iteration no. 1101, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48605113109782, Training Error = 0.255859375
Iteration no. 1102, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48535189932549, Training Error = 0.220703125
Iteration no. 1103, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54573923502011, Training Error = 0.267578125
Iteration no. 1104, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48863178410745, Training Error = 0.255859375
Iteration no. 1105, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48659148585155, Training Error = 0.2265625
Iteration no. 1106, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5026891693641, Training Error = 0.248046875
Iteration no. 1107, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51576223139306, Training Error = 0.279296875
Iteration no. 1108, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49589861365072, Training Error = 0.25
Iteration no. 1109, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53711634548405, Training Error = 0.267578125
Iteration no. 1110, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50841092124892, Training Error = 0.255859375
Iteration no. 1111, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47649998449548, Training Error = 0.234375
Iteration no. 1112, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49481697378549, Training Error = 0.236328125
Iteration no. 1113, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47987329804635, Training Error = 0.236328125
Iteration no. 1114, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54288455733767, Training Error = 0.283203125
Iteration no. 1115, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49902594586646, Training Error = 0.2265625
Iteration no. 1116, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47233519108637, Training Error = 0.251953125
Iteration no. 1117, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47332149295666, Training Error = 0.24609375
Iteration no. 1118, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50211776264346, Training Error = 0.259765625
Iteration no. 1119, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50091040981971, Training Error = 0.255859375
Iteration no. 1120, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48624130616454, Training Error = 0.244140625
Iteration no. 1121, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.55138912344086, Training Error = 0.275390625
Iteration no. 1122, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47078095349946, Training Error = 0.21484375
Iteration no. 1123, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48258231294412, Training Error = 0.248046875
Iteration no. 1124, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48874026553339, Training Error = 0.26171875
Iteration no. 1125, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51057756252313, Training Error = 0.23828125
Iteration no. 1126, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48501279459169, Training Error = 0.23828125
Iteration no. 1127, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51044271347902, Training Error = 0.263671875
Iteration no. 1128, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.52514429700279, Training Error = 0.2578125
Iteration no. 1129, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50013570965696, Training Error = 0.240234375
Iteration no. 1130, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48698617906559, Training Error = 0.25
Iteration no. 1131, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49703070151667, Training Error = 0.2578125
Iteration no. 1132, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50951000488877, Training Error = 0.263671875
Iteration no. 1133, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48788884651248, Training Error = 0.244140625
Iteration no. 1134, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50814868846757, Training Error = 0.2578125
Iteration no. 1135, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49852472491009, Training Error = 0.2421875
Iteration no. 1136, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54235311923255, Training Error = 0.30078125
Iteration no. 1137, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48691380493113, Training Error = 0.25390625
Iteration no. 1138, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49653795847936, Training Error = 0.220703125
Iteration no. 1139, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.54801257891104, Training Error = 0.279296875
Iteration no. 1140, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50353655957801, Training Error = 0.263671875
Iteration no. 1141, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50201532766799, Training Error = 0.244140625
Iteration no. 1142, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51514963862588, Training Error = 0.2578125
Iteration no. 1143, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51369469101958, Training Error = 0.265625
Iteration no. 1144, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51229277770525, Training Error = 0.259765625
Iteration no. 1145, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.52231752388038, Training Error = 0.259765625
Iteration no. 1146, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5377118565022, Training Error = 0.28515625
Iteration no. 1147, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48740050246461, Training Error = 0.2421875
Iteration no. 1148, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49972973299165, Training Error = 0.23828125
Iteration no. 1149, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51575256696784, Training Error = 0.265625
Iteration no. 1150, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51340169069388, Training Error = 0.267578125
Testing... average test_loss = 0.92590819707315, average test_pred_err = 0.465
Iteration no. 1151, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49759171498059, Training Error = 0.232421875
Iteration no. 1152, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47758506776223, Training Error = 0.224609375
Iteration no. 1153, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49205037176077, Training Error = 0.2421875
Iteration no. 1154, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47699997377164, Training Error = 0.224609375
Iteration no. 1155, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50508541029691, Training Error = 0.244140625
Iteration no. 1156, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50215213707085, Training Error = 0.248046875
Iteration no. 1157, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51562369647274, Training Error = 0.26171875
Iteration no. 1158, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49153129725613, Training Error = 0.271484375
Iteration no. 1159, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53642162256531, Training Error = 0.279296875
Iteration no. 1160, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.5123886284221, Training Error = 0.24609375
Iteration no. 1161, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53137305258711, Training Error = 0.2578125
Iteration no. 1162, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49081811894184, Training Error = 0.240234375
Iteration no. 1163, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50505695840824, Training Error = 0.251953125
Iteration no. 1164, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47614015493154, Training Error = 0.212890625
Iteration no. 1165, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48170160944491, Training Error = 0.23046875
Iteration no. 1166, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51314491367825, Training Error = 0.248046875
Iteration no. 1167, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50274616045885, Training Error = 0.24609375
Iteration no. 1168, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.48840319030781, Training Error = 0.23046875
Iteration no. 1169, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.53764554935021, Training Error = 0.267578125
Iteration no. 1170, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51606052875406, Training Error = 0.27734375
Iteration no. 1171, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47533910606381, Training Error = 0.228515625
Iteration no. 1172, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.49388534473734, Training Error = 0.2578125
Iteration no. 1173, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.51215366956989, Training Error = 0.271484375
Iteration no. 1174, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.50423200996218, Training Error = 0.25390625
Iteration no. 1175, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.47868182711173, Training Error = 0.25
Iteration no. 1176, lr = 0.03954653486, attribute lr =0.0003954653486, average batch_loss = 0.46772874494761, Training Error = 0.208984375
