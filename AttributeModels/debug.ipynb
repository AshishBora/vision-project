{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nngraph';\n",
    "require 'torch';\n",
    "\n",
    "function getCtrainExample(set, labels)\n",
    "\n",
    "    -- randomly select two images from different classes\n",
    "    torch.manualSeed(3489208)\n",
    "    local y = torch.randperm((#set)[1])\n",
    "\n",
    "    local im_feat = {}\n",
    "    im_feat[1] = set[y[1]]\n",
    "    im_feat[2] = set[y[2]]\n",
    "\n",
    "    -- randomly select one of those to be given to model B\n",
    "    local label = torch.bernoulli() + 1\n",
    "    im_feat[3] = im_feat[label]:clone()\n",
    "\n",
    "    -- randomly select one of the images to ask about its class\n",
    "    local ques = torch.Tensor(42):fill(0)\n",
    "    ques[labels[y[torch.bernoulli() + 1]]] = 1\n",
    "\n",
    "    -- target is probability of label = 2\n",
    "    local target = label - 1\n",
    "    local input = torch.cat({im_feat[1], im_feat[2], im_feat[3], ques})\n",
    "    \n",
    "    return {input, target}\n",
    "end\n",
    "\n",
    "\n",
    "function nextBatch(trainset, train_labels, batchSize)\n",
    "    local inputs = torch.Tensor(batchSize, 12330);\n",
    "    local targets = torch.Tensor(batchSize);\n",
    "    local i = 0;\n",
    "    for i = 1, batchSize do\n",
    "        example = getCtrainExample(trainset, train_labels);\n",
    "        inputs[i] = example[1];\n",
    "        targets[i] = example[2];\n",
    "    end\n",
    "    inputs:double();\n",
    "    targets:double();\n",
    "    return inputs, targets\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- read preprocessed feature vectors and labels\n",
    "feat_vecs = torch.load('feat_vecs.t7')\n",
    "labels = torch.load('labels.t7')\n",
    "\n",
    "-- generate trainset and testset\n",
    "train_perc = 0.80 -- percentage of images in the train set\n",
    "trainset_size = torch.round((#feat_vecs)[1] * train_perc)\n",
    "trainset = feat_vecs[{{1, trainset_size}}]\n",
    "train_labels = labels[{{1, trainset_size}}]\n",
    "testset = feat_vecs[{{trainset_size+1, (#feat_vecs)[1]}}]\n",
    "test_labels = labels[{{trainset_size+1, (#feat_vecs)[1]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_model_old = torch.load('C_model__1500.t7')\n",
    "\n",
    "C_model = torch.load('C_model.t7')\n",
    "C_model.modules[13].modules[1].weight = C_model_old.modules[10].modules[1].weight:clone()\n",
    "C_model.modules[13].modules[3].weight = C_model_old.modules[10].modules[3].weight:clone()\n",
    "C_model.modules[13].modules[1].bias = C_model_old.modules[10].modules[1].bias:clone()\n",
    "C_model.modules[13].modules[3].bias = C_model_old.modules[10].modules[3].bias:clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_model:evaluate()\n",
    "C_model_old:evaluate()\n",
    "\n",
    "C_model:double()\n",
    "C_model_old:double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- confirm that the weights are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0\t\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0\t\n",
       "0\t\n",
       "0\t\n",
       "0\t\n",
       "0\t\n",
       "0\t\n",
       "0\t\n",
       "0\t\n",
       "0\t\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- All 4096x42 modules\n",
    "a1 = C_model_old.modules[2].modules[3]\n",
    "a2 = C_model_old.modules[6].modules[3]\n",
    "a3 = C_model.modules[2].modules[3]\n",
    "a4 = C_model.modules[7].modules[3]\n",
    "print(torch.max(a1.weight - a2.weight))\n",
    "print(torch.max(a2.weight - a3.weight))\n",
    "print(torch.max(a3.weight - a4.weight))\n",
    "print(torch.max(a4.weight - a1.weight))\n",
    "print(torch.max(a1.bias - a2.bias))\n",
    "print(torch.max(a2.bias - a3.bias))\n",
    "print(torch.max(a3.bias - a4.bias))\n",
    "print(torch.max(a4.bias - a1.bias))\n",
    "\n",
    "-- All 3x2 modules\n",
    "b1 = C_model_old.modules[10].modules[1]\n",
    "b2 = C_model.modules[13].modules[1]\n",
    "print(torch.max(b1.weight - b2.weight))\n",
    "print(torch.max(b1.bias - b2.bias))\n",
    "\n",
    "-- All 2x1 modules\n",
    "c1 = C_model_old.modules[10].modules[3]\n",
    "c2 = C_model.modules[13].modules[3]\n",
    "print(torch.max(c1.weight - c2.weight))\n",
    "print(torch.max(c1.bias - c2.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-16 *\n",
       "  2.4476\n",
       "[torch.DoubleTensor of size 1x1]\n",
       "\n",
       "1e-16 *\n",
       " 2.4476\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "inputs, targets = nextBatch(trainset, train_labels, batch_size);\n",
    "input = inputs[1]\n",
    "\n",
    "image_feat1 = input:narrow(1, 1, 4096)\n",
    "image_feat2 = input:narrow(1, 4097, 4096)\n",
    "image_feat3 = input:narrow(1, 8193, 4096)\n",
    "question = input:narrow(1, 12289, 42)\n",
    "confidence = torch.Tensor{0.8}\n",
    "\n",
    "print(C_model:forward({image_feat1, image_feat2, question, confidence}))\n",
    "print(C_model_old:forward({image_feat1, image_feat2, question, confidence}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
